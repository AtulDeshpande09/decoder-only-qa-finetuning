Step 1 | loss: 4.088625907897949
Step 1 | grad_norm: 0.6822727918624878
Step 1 | learning_rate: 0.0001
Step 1 | epoch: 0.0009633911368015414
Step 2 | loss: 4.473577976226807
Step 2 | grad_norm: 0.6480823755264282
Step 2 | learning_rate: 9.996788696210661e-05
Step 2 | epoch: 0.0019267822736030828
Step 3 | loss: 3.9802310466766357
Step 3 | grad_norm: 0.778215229511261
Step 3 | learning_rate: 9.993577392421323e-05
Step 3 | epoch: 0.002890173410404624
Step 4 | loss: 4.229485034942627
Step 4 | grad_norm: 1.1202521324157715
Step 4 | learning_rate: 9.990366088631985e-05
Step 4 | epoch: 0.0038535645472061657
Step 5 | loss: 4.954882621765137
Step 5 | grad_norm: 0.7381162643432617
Step 5 | learning_rate: 9.987154784842646e-05
Step 5 | epoch: 0.004816955684007707
Step 6 | loss: 4.571215629577637
Step 6 | grad_norm: 0.7987918257713318
Step 6 | learning_rate: 9.983943481053308e-05
Step 6 | epoch: 0.005780346820809248
Step 7 | loss: 4.527415752410889
Step 7 | grad_norm: 0.7923184037208557
Step 7 | learning_rate: 9.980732177263969e-05
Step 7 | epoch: 0.00674373795761079
Step 8 | loss: 3.85671329498291
Step 8 | grad_norm: 0.6859886050224304
Step 8 | learning_rate: 9.977520873474631e-05
Step 8 | epoch: 0.007707129094412331
Step 9 | loss: 3.7919273376464844
Step 9 | grad_norm: 0.7153675556182861
Step 9 | learning_rate: 9.974309569685293e-05
Step 9 | epoch: 0.008670520231213872
Step 10 | loss: 4.33343505859375
Step 10 | grad_norm: 0.6899328827857971
Step 10 | learning_rate: 9.971098265895954e-05
Step 10 | epoch: 0.009633911368015413
Step 11 | loss: 3.6303341388702393
Step 11 | grad_norm: 0.7346368432044983
Step 11 | learning_rate: 9.967886962106616e-05
Step 11 | epoch: 0.010597302504816955
Step 12 | loss: 4.633975505828857
Step 12 | grad_norm: 0.7491808533668518
Step 12 | learning_rate: 9.964675658317277e-05
Step 12 | epoch: 0.011560693641618497
Step 13 | loss: 4.138704776763916
Step 13 | grad_norm: 0.6344655752182007
Step 13 | learning_rate: 9.961464354527939e-05
Step 13 | epoch: 0.012524084778420038
Step 14 | loss: 4.182367324829102
Step 14 | grad_norm: 0.6339722871780396
Step 14 | learning_rate: 9.958253050738601e-05
Step 14 | epoch: 0.01348747591522158
Step 15 | loss: 3.905498743057251
Step 15 | grad_norm: 0.6345131397247314
Step 15 | learning_rate: 9.955041746949262e-05
Step 15 | epoch: 0.014450867052023121
Step 16 | loss: 5.260451316833496
Step 16 | grad_norm: 0.816376268863678
Step 16 | learning_rate: 9.951830443159922e-05
Step 16 | epoch: 0.015414258188824663
Step 17 | loss: 5.111060619354248
Step 17 | grad_norm: 1.1604979038238525
Step 17 | learning_rate: 9.948619139370585e-05
Step 17 | epoch: 0.016377649325626204
Step 18 | loss: 4.013152599334717
Step 18 | grad_norm: 0.6846078634262085
Step 18 | learning_rate: 9.945407835581247e-05
Step 18 | epoch: 0.017341040462427744
Step 19 | loss: 2.7868449687957764
Step 19 | grad_norm: 0.6786662936210632
Step 19 | learning_rate: 9.942196531791907e-05
Step 19 | epoch: 0.018304431599229287
Step 20 | loss: 4.383124828338623
Step 20 | grad_norm: 1.231497883796692
Step 20 | learning_rate: 9.93898522800257e-05
Step 20 | epoch: 0.019267822736030827
Step 21 | loss: 4.255809307098389
Step 21 | grad_norm: 1.2949062585830688
Step 21 | learning_rate: 9.93577392421323e-05
Step 21 | epoch: 0.02023121387283237
Step 22 | loss: 4.596851348876953
Step 22 | grad_norm: 0.9173665642738342
Step 22 | learning_rate: 9.932562620423894e-05
Step 22 | epoch: 0.02119460500963391
Step 23 | loss: 4.318512439727783
Step 23 | grad_norm: 1.0708271265029907
Step 23 | learning_rate: 9.929351316634554e-05
Step 23 | epoch: 0.022157996146435453
Step 24 | loss: 4.537840366363525
Step 24 | grad_norm: 0.9201270341873169
Step 24 | learning_rate: 9.926140012845215e-05
Step 24 | epoch: 0.023121387283236993
Step 25 | loss: 3.8841552734375
Step 25 | grad_norm: 0.8986729383468628
Step 25 | learning_rate: 9.922928709055877e-05
Step 25 | epoch: 0.024084778420038536
Step 26 | loss: 4.465268611907959
Step 26 | grad_norm: 0.9235057234764099
Step 26 | learning_rate: 9.91971740526654e-05
Step 26 | epoch: 0.025048169556840076
Step 27 | loss: 4.267496585845947
Step 27 | grad_norm: 0.9016082286834717
Step 27 | learning_rate: 9.9165061014772e-05
Step 27 | epoch: 0.02601156069364162
Step 28 | loss: 4.590764045715332
Step 28 | grad_norm: 0.8248746395111084
Step 28 | learning_rate: 9.913294797687862e-05
Step 28 | epoch: 0.02697495183044316
Step 29 | loss: 4.014322757720947
Step 29 | grad_norm: 0.8630393147468567
Step 29 | learning_rate: 9.910083493898523e-05
Step 29 | epoch: 0.027938342967244702
Step 30 | loss: 3.9799084663391113
Step 30 | grad_norm: 0.8331925868988037
Step 30 | learning_rate: 9.906872190109184e-05
Step 30 | epoch: 0.028901734104046242
Step 31 | loss: 3.794959306716919
Step 31 | grad_norm: 1.0283387899398804
Step 31 | learning_rate: 9.903660886319847e-05
Step 31 | epoch: 0.029865125240847785
Step 32 | loss: 3.699645757675171
Step 32 | grad_norm: 0.7763090133666992
Step 32 | learning_rate: 9.900449582530508e-05
Step 32 | epoch: 0.030828516377649325
Step 33 | loss: 4.505204677581787
Step 33 | grad_norm: 0.9737361669540405
Step 33 | learning_rate: 9.89723827874117e-05
Step 33 | epoch: 0.031791907514450865
Step 34 | loss: 4.1474289894104
Step 34 | grad_norm: 0.9424571990966797
Step 34 | learning_rate: 9.894026974951831e-05
Step 34 | epoch: 0.03275529865125241
Step 35 | loss: 5.138873100280762
Step 35 | grad_norm: 1.0334709882736206
Step 35 | learning_rate: 9.890815671162493e-05
Step 35 | epoch: 0.03371868978805395
Step 36 | loss: 4.5636186599731445
Step 36 | grad_norm: 1.4239693880081177
Step 36 | learning_rate: 9.887604367373155e-05
Step 36 | epoch: 0.03468208092485549
Step 37 | loss: 3.5910420417785645
Step 37 | grad_norm: 0.8560277223587036
Step 37 | learning_rate: 9.884393063583816e-05
Step 37 | epoch: 0.03564547206165703
Step 38 | loss: 4.084400653839111
Step 38 | grad_norm: 0.9946047067642212
Step 38 | learning_rate: 9.881181759794476e-05
Step 38 | epoch: 0.036608863198458574
Step 39 | loss: 3.519956588745117
Step 39 | grad_norm: 0.9948849678039551
Step 39 | learning_rate: 9.877970456005138e-05
Step 39 | epoch: 0.03757225433526012
Step 40 | loss: 4.627908229827881
Step 40 | grad_norm: 0.9446666240692139
Step 40 | learning_rate: 9.8747591522158e-05
Step 40 | epoch: 0.038535645472061654
Step 41 | loss: 4.084989070892334
Step 41 | grad_norm: 1.0866144895553589
Step 41 | learning_rate: 9.871547848426461e-05
Step 41 | epoch: 0.0394990366088632
Step 42 | loss: 4.448331832885742
Step 42 | grad_norm: 1.1490586996078491
Step 42 | learning_rate: 9.868336544637123e-05
Step 42 | epoch: 0.04046242774566474
Step 43 | loss: 4.046576023101807
Step 43 | grad_norm: 1.408279538154602
Step 43 | learning_rate: 9.865125240847784e-05
Step 43 | epoch: 0.041425818882466284
Step 44 | loss: 3.369450569152832
Step 44 | grad_norm: 1.1015260219573975
Step 44 | learning_rate: 9.861913937058446e-05
Step 44 | epoch: 0.04238921001926782
Step 45 | loss: 2.9872586727142334
Step 45 | grad_norm: 0.8980904221534729
Step 45 | learning_rate: 9.858702633269108e-05
Step 45 | epoch: 0.04335260115606936
Step 46 | loss: 4.532236099243164
Step 46 | grad_norm: 1.3260046243667603
Step 46 | learning_rate: 9.855491329479769e-05
Step 46 | epoch: 0.04431599229287091
Step 47 | loss: 3.5859994888305664
Step 47 | grad_norm: 1.0204280614852905
Step 47 | learning_rate: 9.852280025690431e-05
Step 47 | epoch: 0.04527938342967245
Step 48 | loss: 4.937209129333496
Step 48 | grad_norm: 1.2539352178573608
Step 48 | learning_rate: 9.849068721901092e-05
Step 48 | epoch: 0.046242774566473986
Step 49 | loss: 4.170693397521973
Step 49 | grad_norm: 1.8688265085220337
Step 49 | learning_rate: 9.845857418111754e-05
Step 49 | epoch: 0.04720616570327553
Step 50 | loss: 4.6423821449279785
Step 50 | grad_norm: 1.2451937198638916
Step 50 | learning_rate: 9.842646114322416e-05
Step 50 | epoch: 0.04816955684007707
Step 51 | loss: 5.490020751953125
Step 51 | grad_norm: 1.7354766130447388
Step 51 | learning_rate: 9.839434810533077e-05
Step 51 | epoch: 0.049132947976878616
Step 52 | loss: 4.486059665679932
Step 52 | grad_norm: 1.536616325378418
Step 52 | learning_rate: 9.836223506743738e-05
Step 52 | epoch: 0.05009633911368015
Step 53 | loss: 4.4089884757995605
Step 53 | grad_norm: 2.0940682888031006
Step 53 | learning_rate: 9.8330122029544e-05
Step 53 | epoch: 0.051059730250481696
Step 54 | loss: 3.9045536518096924
Step 54 | grad_norm: 1.323662519454956
Step 54 | learning_rate: 9.829800899165062e-05
Step 54 | epoch: 0.05202312138728324
Step 55 | loss: 3.69702410697937
Step 55 | grad_norm: 1.1199284791946411
Step 55 | learning_rate: 9.826589595375723e-05
Step 55 | epoch: 0.052986512524084775
Step 56 | loss: 4.802529811859131
Step 56 | grad_norm: 1.5987639427185059
Step 56 | learning_rate: 9.823378291586385e-05
Step 56 | epoch: 0.05394990366088632
Step 57 | loss: 4.489511489868164
Step 57 | grad_norm: 1.3062255382537842
Step 57 | learning_rate: 9.820166987797045e-05
Step 57 | epoch: 0.05491329479768786
Step 58 | loss: 3.858673095703125
Step 58 | grad_norm: 1.193135380744934
Step 58 | learning_rate: 9.816955684007709e-05
Step 58 | epoch: 0.055876685934489405
Step 59 | loss: 3.870673179626465
Step 59 | grad_norm: 1.2182735204696655
Step 59 | learning_rate: 9.81374438021837e-05
Step 59 | epoch: 0.05684007707129094
Step 60 | loss: 4.463705062866211
Step 60 | grad_norm: 1.2258212566375732
Step 60 | learning_rate: 9.81053307642903e-05
Step 60 | epoch: 0.057803468208092484
Step 61 | loss: 3.6854019165039062
Step 61 | grad_norm: 1.118889331817627
Step 61 | learning_rate: 9.807321772639692e-05
Step 61 | epoch: 0.05876685934489403
Step 62 | loss: 3.6994266510009766
Step 62 | grad_norm: 1.5314290523529053
Step 62 | learning_rate: 9.804110468850353e-05
Step 62 | epoch: 0.05973025048169557
Step 63 | loss: 4.319828033447266
Step 63 | grad_norm: 2.128373384475708
Step 63 | learning_rate: 9.800899165061015e-05
Step 63 | epoch: 0.06069364161849711
Step 64 | loss: 3.619615077972412
Step 64 | grad_norm: 1.3915674686431885
Step 64 | learning_rate: 9.797687861271677e-05
Step 64 | epoch: 0.06165703275529865
Step 65 | loss: 4.182648658752441
Step 65 | grad_norm: 1.5383317470550537
Step 65 | learning_rate: 9.794476557482338e-05
Step 65 | epoch: 0.0626204238921002
Step 66 | loss: 4.21198844909668
Step 66 | grad_norm: 1.4216971397399902
Step 66 | learning_rate: 9.791265253692999e-05
Step 66 | epoch: 0.06358381502890173
Step 67 | loss: 4.1439714431762695
Step 67 | grad_norm: 1.863998293876648
Step 67 | learning_rate: 9.788053949903662e-05
Step 67 | epoch: 0.06454720616570328
Step 68 | loss: 3.945197343826294
Step 68 | grad_norm: 1.3063251972198486
Step 68 | learning_rate: 9.784842646114323e-05
Step 68 | epoch: 0.06551059730250482
Step 69 | loss: 3.619845390319824
Step 69 | grad_norm: 2.251833915710449
Step 69 | learning_rate: 9.781631342324984e-05
Step 69 | epoch: 0.06647398843930635
Step 70 | loss: 3.582871675491333
Step 70 | grad_norm: 2.0365209579467773
Step 70 | learning_rate: 9.778420038535646e-05
Step 70 | epoch: 0.0674373795761079
Step 71 | loss: 4.024065017700195
Step 71 | grad_norm: 1.3723173141479492
Step 71 | learning_rate: 9.775208734746308e-05
Step 71 | epoch: 0.06840077071290944
Step 72 | loss: 3.942469358444214
Step 72 | grad_norm: 1.0580682754516602
Step 72 | learning_rate: 9.77199743095697e-05
Step 72 | epoch: 0.06936416184971098
Step 73 | loss: 3.0439181327819824
Step 73 | grad_norm: 1.2639964818954468
Step 73 | learning_rate: 9.768786127167631e-05
Step 73 | epoch: 0.07032755298651253
Step 74 | loss: 3.479452133178711
Step 74 | grad_norm: 1.256111741065979
Step 74 | learning_rate: 9.765574823378292e-05
Step 74 | epoch: 0.07129094412331406
Step 75 | loss: 4.056365966796875
Step 75 | grad_norm: 1.3049681186676025
Step 75 | learning_rate: 9.762363519588954e-05
Step 75 | epoch: 0.07225433526011561
Step 76 | loss: 3.3312461376190186
Step 76 | grad_norm: 1.4170624017715454
Step 76 | learning_rate: 9.759152215799616e-05
Step 76 | epoch: 0.07321772639691715
Step 77 | loss: 4.915544033050537
Step 77 | grad_norm: 1.478461742401123
Step 77 | learning_rate: 9.755940912010276e-05
Step 77 | epoch: 0.07418111753371869
Step 78 | loss: 4.435762405395508
Step 78 | grad_norm: 1.9397492408752441
Step 78 | learning_rate: 9.752729608220939e-05
Step 78 | epoch: 0.07514450867052024
Step 79 | loss: 4.220626354217529
Step 79 | grad_norm: 2.1897811889648438
Step 79 | learning_rate: 9.749518304431599e-05
Step 79 | epoch: 0.07610789980732177
Step 80 | loss: 3.9159209728240967
Step 80 | grad_norm: 2.517360210418701
Step 80 | learning_rate: 9.746307000642261e-05
Step 80 | epoch: 0.07707129094412331
Step 81 | loss: 3.918314218521118
Step 81 | grad_norm: 1.552034616470337
Step 81 | learning_rate: 9.743095696852923e-05
Step 81 | epoch: 0.07803468208092486
Step 82 | loss: 3.7183268070220947
Step 82 | grad_norm: 1.8357970714569092
Step 82 | learning_rate: 9.739884393063584e-05
Step 82 | epoch: 0.0789980732177264
Step 83 | loss: 3.7397751808166504
Step 83 | grad_norm: 1.2301907539367676
Step 83 | learning_rate: 9.736673089274245e-05
Step 83 | epoch: 0.07996146435452794
Step 84 | loss: 4.626834869384766
Step 84 | grad_norm: 1.5539261102676392
Step 84 | learning_rate: 9.733461785484907e-05
Step 84 | epoch: 0.08092485549132948
Step 85 | loss: 4.471016883850098
Step 85 | grad_norm: 1.7001453638076782
Step 85 | learning_rate: 9.730250481695569e-05
Step 85 | epoch: 0.08188824662813102
Step 86 | loss: 2.8285131454467773
Step 86 | grad_norm: 1.3295305967330933
Step 86 | learning_rate: 9.727039177906231e-05
Step 86 | epoch: 0.08285163776493257
Step 87 | loss: 3.5480124950408936
Step 87 | grad_norm: 1.489764928817749
Step 87 | learning_rate: 9.723827874116892e-05
Step 87 | epoch: 0.0838150289017341
Step 88 | loss: 3.986990451812744
Step 88 | grad_norm: 1.5801944732666016
Step 88 | learning_rate: 9.720616570327553e-05
Step 88 | epoch: 0.08477842003853564
Step 89 | loss: 2.832744598388672
Step 89 | grad_norm: 1.8646453619003296
Step 89 | learning_rate: 9.717405266538215e-05
Step 89 | epoch: 0.08574181117533719
Step 90 | loss: 2.808558702468872
Step 90 | grad_norm: 1.3650542497634888
Step 90 | learning_rate: 9.714193962748877e-05
Step 90 | epoch: 0.08670520231213873
Step 91 | loss: 3.205550193786621
Step 91 | grad_norm: 1.4611966609954834
Step 91 | learning_rate: 9.710982658959538e-05
Step 91 | epoch: 0.08766859344894026
Step 92 | loss: 4.815492630004883
Step 92 | grad_norm: 1.7363351583480835
Step 92 | learning_rate: 9.7077713551702e-05
Step 92 | epoch: 0.08863198458574181
Step 93 | loss: 3.896446466445923
Step 93 | grad_norm: 1.5467294454574585
Step 93 | learning_rate: 9.70456005138086e-05
Step 93 | epoch: 0.08959537572254335
Step 94 | loss: 3.619889259338379
Step 94 | grad_norm: 1.3363840579986572
Step 94 | learning_rate: 9.701348747591523e-05
Step 94 | epoch: 0.0905587668593449
Step 95 | loss: 3.968043565750122
Step 95 | grad_norm: 2.822246789932251
Step 95 | learning_rate: 9.698137443802185e-05
Step 95 | epoch: 0.09152215799614644
Step 96 | loss: 3.9297661781311035
Step 96 | grad_norm: 3.378926992416382
Step 96 | learning_rate: 9.694926140012845e-05
Step 96 | epoch: 0.09248554913294797
Step 97 | loss: 3.458261013031006
Step 97 | grad_norm: 1.7325836420059204
Step 97 | learning_rate: 9.691714836223508e-05
Step 97 | epoch: 0.09344894026974952
Step 98 | loss: 2.96692156791687
Step 98 | grad_norm: 1.4396908283233643
Step 98 | learning_rate: 9.688503532434168e-05
Step 98 | epoch: 0.09441233140655106
Step 99 | loss: 4.078801155090332
Step 99 | grad_norm: 1.8661707639694214
Step 99 | learning_rate: 9.68529222864483e-05
Step 99 | epoch: 0.0953757225433526
Step 100 | loss: 4.223275661468506
Step 100 | grad_norm: 1.987103819847107
Step 100 | learning_rate: 9.682080924855492e-05
Step 100 | epoch: 0.09633911368015415
Step 101 | loss: 3.3148577213287354
Step 101 | grad_norm: 1.3717212677001953
Step 101 | learning_rate: 9.678869621066153e-05
Step 101 | epoch: 0.09730250481695568
Step 102 | loss: 3.9097366333007812
Step 102 | grad_norm: 1.6201084852218628
Step 102 | learning_rate: 9.675658317276814e-05
Step 102 | epoch: 0.09826589595375723
Step 103 | loss: 3.1638569831848145
Step 103 | grad_norm: 3.3732099533081055
Step 103 | learning_rate: 9.672447013487477e-05
Step 103 | epoch: 0.09922928709055877
Step 104 | loss: 4.096516132354736
Step 104 | grad_norm: 1.7508642673492432
Step 104 | learning_rate: 9.669235709698138e-05
Step 104 | epoch: 0.1001926782273603
Step 105 | loss: 4.0588274002075195
Step 105 | grad_norm: 1.7580269575119019
Step 105 | learning_rate: 9.666024405908799e-05
Step 105 | epoch: 0.10115606936416185
Step 106 | loss: 4.381237983703613
Step 106 | grad_norm: 1.650486707687378
Step 106 | learning_rate: 9.662813102119461e-05
Step 106 | epoch: 0.10211946050096339
Step 107 | loss: 3.8691422939300537
Step 107 | grad_norm: 1.7476392984390259
Step 107 | learning_rate: 9.659601798330122e-05
Step 107 | epoch: 0.10308285163776493
Step 108 | loss: 3.6807761192321777
Step 108 | grad_norm: 1.4493763446807861
Step 108 | learning_rate: 9.656390494540784e-05
Step 108 | epoch: 0.10404624277456648
Step 109 | loss: 3.579644203186035
Step 109 | grad_norm: 1.719897747039795
Step 109 | learning_rate: 9.653179190751446e-05
Step 109 | epoch: 0.10500963391136801
Step 110 | loss: 3.7094736099243164
Step 110 | grad_norm: 2.18418025970459
Step 110 | learning_rate: 9.649967886962107e-05
Step 110 | epoch: 0.10597302504816955
Step 111 | loss: 3.2704174518585205
Step 111 | grad_norm: 1.5703036785125732
Step 111 | learning_rate: 9.646756583172769e-05
Step 111 | epoch: 0.1069364161849711
Step 112 | loss: 4.011006832122803
Step 112 | grad_norm: 2.4986040592193604
Step 112 | learning_rate: 9.643545279383431e-05
Step 112 | epoch: 0.10789980732177264
Step 113 | loss: 3.6245572566986084
Step 113 | grad_norm: 1.5650094747543335
Step 113 | learning_rate: 9.640333975594092e-05
Step 113 | epoch: 0.10886319845857419
Step 114 | loss: 3.2619426250457764
Step 114 | grad_norm: 1.6650257110595703
Step 114 | learning_rate: 9.637122671804754e-05
Step 114 | epoch: 0.10982658959537572
Step 115 | loss: 3.5598576068878174
Step 115 | grad_norm: 1.6539738178253174
Step 115 | learning_rate: 9.633911368015414e-05
Step 115 | epoch: 0.11078998073217726
Step 116 | loss: 4.116985321044922
Step 116 | grad_norm: 2.714245080947876
Step 116 | learning_rate: 9.630700064226077e-05
Step 116 | epoch: 0.11175337186897881
Step 117 | loss: 3.529545307159424
Step 117 | grad_norm: 2.0161139965057373
Step 117 | learning_rate: 9.627488760436739e-05
Step 117 | epoch: 0.11271676300578035
Step 118 | loss: 3.9180660247802734
Step 118 | grad_norm: 1.7898871898651123
Step 118 | learning_rate: 9.6242774566474e-05
Step 118 | epoch: 0.11368015414258188
Step 119 | loss: 4.023714065551758
Step 119 | grad_norm: 2.0804800987243652
Step 119 | learning_rate: 9.62106615285806e-05
Step 119 | epoch: 0.11464354527938343
Step 120 | loss: 3.2334516048431396
Step 120 | grad_norm: 1.8461666107177734
Step 120 | learning_rate: 9.617854849068722e-05
Step 120 | epoch: 0.11560693641618497
Step 121 | loss: 3.495215654373169
Step 121 | grad_norm: 1.8611526489257812
Step 121 | learning_rate: 9.614643545279384e-05
Step 121 | epoch: 0.11657032755298652
Step 122 | loss: 3.2576234340667725
Step 122 | grad_norm: 1.705010175704956
Step 122 | learning_rate: 9.611432241490046e-05
Step 122 | epoch: 0.11753371868978806
Step 123 | loss: 2.8761565685272217
Step 123 | grad_norm: 1.6619247198104858
Step 123 | learning_rate: 9.608220937700707e-05
Step 123 | epoch: 0.11849710982658959
Step 124 | loss: 3.800307512283325
Step 124 | grad_norm: 3.6969950199127197
Step 124 | learning_rate: 9.605009633911368e-05
Step 124 | epoch: 0.11946050096339114
Step 125 | loss: 3.4281833171844482
Step 125 | grad_norm: 1.4154256582260132
Step 125 | learning_rate: 9.60179833012203e-05
Step 125 | epoch: 0.12042389210019268
Step 126 | loss: 3.662705659866333
Step 126 | grad_norm: 1.8240845203399658
Step 126 | learning_rate: 9.598587026332692e-05
Step 126 | epoch: 0.12138728323699421
Step 127 | loss: 2.8110103607177734
Step 127 | grad_norm: 1.9252208471298218
Step 127 | learning_rate: 9.595375722543353e-05
Step 127 | epoch: 0.12235067437379576
Step 128 | loss: 3.926215410232544
Step 128 | grad_norm: 1.7961132526397705
Step 128 | learning_rate: 9.592164418754015e-05
Step 128 | epoch: 0.1233140655105973
Step 129 | loss: 3.125446081161499
Step 129 | grad_norm: 1.2939708232879639
Step 129 | learning_rate: 9.588953114964676e-05
Step 129 | epoch: 0.12427745664739884
Step 130 | loss: 3.3993611335754395
Step 130 | grad_norm: 1.9629298448562622
Step 130 | learning_rate: 9.585741811175338e-05
Step 130 | epoch: 0.1252408477842004
Step 131 | loss: 4.407268524169922
Step 131 | grad_norm: 2.0727756023406982
Step 131 | learning_rate: 9.582530507386e-05
Step 131 | epoch: 0.12620423892100194
Step 132 | loss: 3.669767141342163
Step 132 | grad_norm: 2.2483630180358887
Step 132 | learning_rate: 9.57931920359666e-05
Step 132 | epoch: 0.12716763005780346
Step 133 | loss: 3.332069158554077
Step 133 | grad_norm: 2.067077159881592
Step 133 | learning_rate: 9.576107899807321e-05
Step 133 | epoch: 0.128131021194605
Step 134 | loss: 2.6174910068511963
Step 134 | grad_norm: 2.3247475624084473
Step 134 | learning_rate: 9.572896596017983e-05
Step 134 | epoch: 0.12909441233140656
Step 135 | loss: 3.5914881229400635
Step 135 | grad_norm: 1.495746374130249
Step 135 | learning_rate: 9.569685292228645e-05
Step 135 | epoch: 0.13005780346820808
Step 136 | loss: 3.3869571685791016
Step 136 | grad_norm: 1.770923376083374
Step 136 | learning_rate: 9.566473988439308e-05
Step 136 | epoch: 0.13102119460500963
Step 137 | loss: 2.866408109664917
Step 137 | grad_norm: 1.3984034061431885
Step 137 | learning_rate: 9.563262684649968e-05
Step 137 | epoch: 0.13198458574181118
Step 138 | loss: 3.065420627593994
Step 138 | grad_norm: 1.575279951095581
Step 138 | learning_rate: 9.560051380860629e-05
Step 138 | epoch: 0.1329479768786127
Step 139 | loss: 3.6350364685058594
Step 139 | grad_norm: 3.137094736099243
Step 139 | learning_rate: 9.556840077071291e-05
Step 139 | epoch: 0.13391136801541426
Step 140 | loss: 3.9367012977600098
Step 140 | grad_norm: 2.378523349761963
Step 140 | learning_rate: 9.553628773281953e-05
Step 140 | epoch: 0.1348747591522158
Step 141 | loss: 3.8294544219970703
Step 141 | grad_norm: 1.9095635414123535
Step 141 | learning_rate: 9.550417469492614e-05
Step 141 | epoch: 0.13583815028901733
Step 142 | loss: 3.583679676055908
Step 142 | grad_norm: 3.0036263465881348
Step 142 | learning_rate: 9.547206165703276e-05
Step 142 | epoch: 0.13680154142581888
Step 143 | loss: 3.7910773754119873
Step 143 | grad_norm: 1.7719123363494873
Step 143 | learning_rate: 9.543994861913937e-05
Step 143 | epoch: 0.13776493256262043
Step 144 | loss: 3.1722404956817627
Step 144 | grad_norm: 1.5605722665786743
Step 144 | learning_rate: 9.540783558124599e-05
Step 144 | epoch: 0.13872832369942195
Step 145 | loss: 3.2755324840545654
Step 145 | grad_norm: 1.8456908464431763
Step 145 | learning_rate: 9.537572254335261e-05
Step 145 | epoch: 0.1396917148362235
Step 146 | loss: 2.9594125747680664
Step 146 | grad_norm: 1.9629004001617432
Step 146 | learning_rate: 9.534360950545922e-05
Step 146 | epoch: 0.14065510597302505
Step 147 | loss: 3.6537935733795166
Step 147 | grad_norm: 2.9640188217163086
Step 147 | learning_rate: 9.531149646756584e-05
Step 147 | epoch: 0.1416184971098266
Step 148 | loss: 3.572220802307129
Step 148 | grad_norm: 1.508028507232666
Step 148 | learning_rate: 9.527938342967246e-05
Step 148 | epoch: 0.14258188824662812
Step 149 | loss: 3.377655029296875
Step 149 | grad_norm: 2.318812847137451
Step 149 | learning_rate: 9.524727039177907e-05
Step 149 | epoch: 0.14354527938342967
Step 150 | loss: 3.355180501937866
Step 150 | grad_norm: 1.6670769453048706
Step 150 | learning_rate: 9.521515735388569e-05
Step 150 | epoch: 0.14450867052023122
Step 151 | loss: 3.2938730716705322
Step 151 | grad_norm: 1.5727322101593018
Step 151 | learning_rate: 9.51830443159923e-05
Step 151 | epoch: 0.14547206165703275
Step 152 | loss: 3.730616569519043
Step 152 | grad_norm: 2.329498529434204
Step 152 | learning_rate: 9.51509312780989e-05
Step 152 | epoch: 0.1464354527938343
Step 153 | loss: 2.919137716293335
Step 153 | grad_norm: 2.539069652557373
Step 153 | learning_rate: 9.511881824020554e-05
Step 153 | epoch: 0.14739884393063585
Step 154 | loss: 4.248438835144043
Step 154 | grad_norm: 2.652087926864624
Step 154 | learning_rate: 9.508670520231214e-05
Step 154 | epoch: 0.14836223506743737
Step 155 | loss: 3.7153923511505127
Step 155 | grad_norm: 2.214317798614502
Step 155 | learning_rate: 9.505459216441875e-05
Step 155 | epoch: 0.14932562620423892
Step 156 | loss: 3.68418550491333
Step 156 | grad_norm: 2.0480384826660156
Step 156 | learning_rate: 9.502247912652537e-05
Step 156 | epoch: 0.15028901734104047
Step 157 | loss: 3.5568995475769043
Step 157 | grad_norm: 2.044804096221924
Step 157 | learning_rate: 9.4990366088632e-05
Step 157 | epoch: 0.151252408477842
Step 158 | loss: 4.133194446563721
Step 158 | grad_norm: 2.133549690246582
Step 158 | learning_rate: 9.49582530507386e-05
Step 158 | epoch: 0.15221579961464354
Step 159 | loss: 3.70680832862854
Step 159 | grad_norm: 2.353846788406372
Step 159 | learning_rate: 9.492614001284522e-05
Step 159 | epoch: 0.1531791907514451
Step 160 | loss: 3.384829044342041
Step 160 | grad_norm: 1.615228295326233
Step 160 | learning_rate: 9.489402697495183e-05
Step 160 | epoch: 0.15414258188824662
Step 161 | loss: 3.565896987915039
Step 161 | grad_norm: 2.358107805252075
Step 161 | learning_rate: 9.486191393705845e-05
Step 161 | epoch: 0.15510597302504817
Step 162 | loss: 3.7877371311187744
Step 162 | grad_norm: 2.5984723567962646
Step 162 | learning_rate: 9.482980089916507e-05
Step 162 | epoch: 0.15606936416184972
Step 163 | loss: 3.4997246265411377
Step 163 | grad_norm: 1.92267644405365
Step 163 | learning_rate: 9.479768786127168e-05
Step 163 | epoch: 0.15703275529865124
Step 164 | loss: 3.3526947498321533
Step 164 | grad_norm: 1.757219672203064
Step 164 | learning_rate: 9.47655748233783e-05
Step 164 | epoch: 0.1579961464354528
Step 165 | loss: 3.0049331188201904
Step 165 | grad_norm: 1.8466490507125854
Step 165 | learning_rate: 9.473346178548491e-05
Step 165 | epoch: 0.15895953757225434
Step 166 | loss: 4.122214317321777
Step 166 | grad_norm: 2.550025701522827
Step 166 | learning_rate: 9.470134874759153e-05
Step 166 | epoch: 0.1599229287090559
Step 167 | loss: 3.1155946254730225
Step 167 | grad_norm: 2.030874013900757
Step 167 | learning_rate: 9.466923570969815e-05
Step 167 | epoch: 0.1608863198458574
Step 168 | loss: 3.343571662902832
Step 168 | grad_norm: 2.3453938961029053
Step 168 | learning_rate: 9.463712267180476e-05
Step 168 | epoch: 0.16184971098265896
Step 169 | loss: 3.250415802001953
Step 169 | grad_norm: 1.9281326532363892
Step 169 | learning_rate: 9.460500963391136e-05
Step 169 | epoch: 0.1628131021194605
Step 170 | loss: 3.967402458190918
Step 170 | grad_norm: 2.8439645767211914
Step 170 | learning_rate: 9.457289659601799e-05
Step 170 | epoch: 0.16377649325626203
Step 171 | loss: 3.7714197635650635
Step 171 | grad_norm: 1.8690532445907593
Step 171 | learning_rate: 9.45407835581246e-05
Step 171 | epoch: 0.16473988439306358
Step 172 | loss: 3.973710298538208
Step 172 | grad_norm: 1.9279438257217407
Step 172 | learning_rate: 9.450867052023123e-05
Step 172 | epoch: 0.16570327552986513
Step 173 | loss: 3.326120376586914
Step 173 | grad_norm: 2.7002973556518555
Step 173 | learning_rate: 9.447655748233783e-05
Step 173 | epoch: 0.16666666666666666
Step 174 | loss: 3.642597198486328
Step 174 | grad_norm: 2.098998785018921
Step 174 | learning_rate: 9.444444444444444e-05
Step 174 | epoch: 0.1676300578034682
Step 175 | loss: 3.3023245334625244
Step 175 | grad_norm: 2.018671751022339
Step 175 | learning_rate: 9.441233140655106e-05
Step 175 | epoch: 0.16859344894026976
Step 176 | loss: 3.142224073410034
Step 176 | grad_norm: 1.8629889488220215
Step 176 | learning_rate: 9.438021836865768e-05
Step 176 | epoch: 0.16955684007707128
Step 177 | loss: 3.7542529106140137
Step 177 | grad_norm: 2.3304483890533447
Step 177 | learning_rate: 9.434810533076429e-05
Step 177 | epoch: 0.17052023121387283
Step 178 | loss: 3.465104818344116
Step 178 | grad_norm: 2.0320663452148438
Step 178 | learning_rate: 9.431599229287091e-05
Step 178 | epoch: 0.17148362235067438
Step 179 | loss: 3.615858793258667
Step 179 | grad_norm: 2.20084810256958
Step 179 | learning_rate: 9.428387925497752e-05
Step 179 | epoch: 0.1724470134874759
Step 180 | loss: 3.790435552597046
Step 180 | grad_norm: 2.1227564811706543
Step 180 | learning_rate: 9.425176621708414e-05
Step 180 | epoch: 0.17341040462427745
Step 181 | loss: 4.785163402557373
Step 181 | grad_norm: 2.7894251346588135
Step 181 | learning_rate: 9.421965317919076e-05
Step 181 | epoch: 0.174373795761079
Step 182 | loss: 3.781574249267578
Step 182 | grad_norm: 3.0534911155700684
Step 182 | learning_rate: 9.418754014129737e-05
Step 182 | epoch: 0.17533718689788053
Step 183 | loss: 3.477534294128418
Step 183 | grad_norm: 1.9628864526748657
Step 183 | learning_rate: 9.415542710340398e-05
Step 183 | epoch: 0.17630057803468208
Step 184 | loss: 3.3368093967437744
Step 184 | grad_norm: 2.140673875808716
Step 184 | learning_rate: 9.41233140655106e-05
Step 184 | epoch: 0.17726396917148363
Step 185 | loss: 3.8628714084625244
Step 185 | grad_norm: 2.0527567863464355
Step 185 | learning_rate: 9.409120102761722e-05
Step 185 | epoch: 0.17822736030828518
Step 186 | loss: 3.133491039276123
Step 186 | grad_norm: 1.9663395881652832
Step 186 | learning_rate: 9.405908798972384e-05
Step 186 | epoch: 0.1791907514450867
Step 187 | loss: 4.292286396026611
Step 187 | grad_norm: 1.9605237245559692
Step 187 | learning_rate: 9.402697495183045e-05
Step 187 | epoch: 0.18015414258188825
Step 188 | loss: 3.714770793914795
Step 188 | grad_norm: 1.9953738451004028
Step 188 | learning_rate: 9.399486191393705e-05
Step 188 | epoch: 0.1811175337186898
Step 189 | loss: 3.69706130027771
Step 189 | grad_norm: 1.890579104423523
Step 189 | learning_rate: 9.396274887604369e-05
Step 189 | epoch: 0.18208092485549132
Step 190 | loss: 3.364079713821411
Step 190 | grad_norm: 1.9783573150634766
Step 190 | learning_rate: 9.39306358381503e-05
Step 190 | epoch: 0.18304431599229287
Step 191 | loss: 2.9576516151428223
Step 191 | grad_norm: 2.4186782836914062
Step 191 | learning_rate: 9.38985228002569e-05
Step 191 | epoch: 0.18400770712909442
Step 192 | loss: 4.347466945648193
Step 192 | grad_norm: 2.241760492324829
Step 192 | learning_rate: 9.386640976236352e-05
Step 192 | epoch: 0.18497109826589594
Step 193 | loss: 3.789721965789795
Step 193 | grad_norm: 1.6980396509170532
Step 193 | learning_rate: 9.383429672447013e-05
Step 193 | epoch: 0.1859344894026975
Step 194 | loss: 3.724360227584839
Step 194 | grad_norm: 2.1590981483459473
Step 194 | learning_rate: 9.380218368657675e-05
Step 194 | epoch: 0.18689788053949905
Step 195 | loss: 3.8255913257598877
Step 195 | grad_norm: 2.0883195400238037
Step 195 | learning_rate: 9.377007064868337e-05
Step 195 | epoch: 0.18786127167630057
Step 196 | loss: 3.57680082321167
Step 196 | grad_norm: 2.244306802749634
Step 196 | learning_rate: 9.373795761078998e-05
Step 196 | epoch: 0.18882466281310212
Step 197 | loss: 3.2081665992736816
Step 197 | grad_norm: 1.8886796236038208
Step 197 | learning_rate: 9.370584457289659e-05
Step 197 | epoch: 0.18978805394990367
Step 198 | loss: 3.487622022628784
Step 198 | grad_norm: 2.1653473377227783
Step 198 | learning_rate: 9.367373153500322e-05
Step 198 | epoch: 0.1907514450867052
Step 199 | loss: 3.2127785682678223
Step 199 | grad_norm: 2.224954843521118
Step 199 | learning_rate: 9.364161849710983e-05
Step 199 | epoch: 0.19171483622350674
Step 200 | loss: 3.393490791320801
Step 200 | grad_norm: 2.019099473953247
Step 200 | learning_rate: 9.360950545921645e-05
Step 200 | epoch: 0.1926782273603083
Step 201 | loss: 3.72636342048645
Step 201 | grad_norm: 2.5124237537384033
Step 201 | learning_rate: 9.357739242132306e-05
Step 201 | epoch: 0.1936416184971098
Step 202 | loss: 4.044535160064697
Step 202 | grad_norm: 3.17081356048584
Step 202 | learning_rate: 9.354527938342968e-05
Step 202 | epoch: 0.19460500963391136
Step 203 | loss: 3.492748737335205
Step 203 | grad_norm: 2.4339773654937744
Step 203 | learning_rate: 9.35131663455363e-05
Step 203 | epoch: 0.1955684007707129
Step 204 | loss: 3.4766807556152344
Step 204 | grad_norm: 1.9700641632080078
Step 204 | learning_rate: 9.348105330764291e-05
Step 204 | epoch: 0.19653179190751446
Step 205 | loss: 3.116335153579712
Step 205 | grad_norm: 1.669944405555725
Step 205 | learning_rate: 9.344894026974952e-05
Step 205 | epoch: 0.197495183044316
Step 206 | loss: 3.3752825260162354
Step 206 | grad_norm: 2.195081949234009
Step 206 | learning_rate: 9.341682723185614e-05
Step 206 | epoch: 0.19845857418111754
Step 207 | loss: 3.260502576828003
Step 207 | grad_norm: 1.6931555271148682
Step 207 | learning_rate: 9.338471419396276e-05
Step 207 | epoch: 0.1994219653179191
Step 208 | loss: 3.117253541946411
Step 208 | grad_norm: 1.9324145317077637
Step 208 | learning_rate: 9.335260115606937e-05
Step 208 | epoch: 0.2003853564547206
Step 209 | loss: 3.3951334953308105
Step 209 | grad_norm: 2.09136700630188
Step 209 | learning_rate: 9.332048811817599e-05
Step 209 | epoch: 0.20134874759152216
Step 210 | loss: 3.7076191902160645
Step 210 | grad_norm: 2.6177940368652344
Step 210 | learning_rate: 9.32883750802826e-05
Step 210 | epoch: 0.2023121387283237
Step 211 | loss: 3.9191133975982666
Step 211 | grad_norm: 2.448115110397339
Step 211 | learning_rate: 9.325626204238921e-05
Step 211 | epoch: 0.20327552986512523
Step 212 | loss: 3.8147759437561035
Step 212 | grad_norm: 2.593393087387085
Step 212 | learning_rate: 9.322414900449584e-05
Step 212 | epoch: 0.20423892100192678
Step 213 | loss: 2.921462059020996
Step 213 | grad_norm: 1.6811597347259521
Step 213 | learning_rate: 9.319203596660244e-05
Step 213 | epoch: 0.20520231213872833
Step 214 | loss: 3.2735490798950195
Step 214 | grad_norm: 1.9286075830459595
Step 214 | learning_rate: 9.315992292870906e-05
Step 214 | epoch: 0.20616570327552985
Step 215 | loss: 3.036116600036621
Step 215 | grad_norm: 2.1353421211242676
Step 215 | learning_rate: 9.312780989081567e-05
Step 215 | epoch: 0.2071290944123314
Step 216 | loss: 3.406691551208496
Step 216 | grad_norm: 1.867750644683838
Step 216 | learning_rate: 9.309569685292229e-05
Step 216 | epoch: 0.20809248554913296
Step 217 | loss: 4.033539772033691
Step 217 | grad_norm: 2.8827085494995117
Step 217 | learning_rate: 9.306358381502891e-05
Step 217 | epoch: 0.20905587668593448
Step 218 | loss: 2.8769519329071045
Step 218 | grad_norm: 1.954507827758789
Step 218 | learning_rate: 9.303147077713552e-05
Step 218 | epoch: 0.21001926782273603
Step 219 | loss: 2.4780967235565186
Step 219 | grad_norm: 1.2981613874435425
Step 219 | learning_rate: 9.299935773924213e-05
Step 219 | epoch: 0.21098265895953758
Step 220 | loss: 3.4470784664154053
Step 220 | grad_norm: 1.7732151746749878
Step 220 | learning_rate: 9.296724470134875e-05
Step 220 | epoch: 0.2119460500963391
Step 221 | loss: 3.132840156555176
Step 221 | grad_norm: 1.9639918804168701
Step 221 | learning_rate: 9.293513166345537e-05
Step 221 | epoch: 0.21290944123314065
Step 222 | loss: 3.494650363922119
Step 222 | grad_norm: 1.8918077945709229
Step 222 | learning_rate: 9.290301862556198e-05
Step 222 | epoch: 0.2138728323699422
Step 223 | loss: 4.180150032043457
Step 223 | grad_norm: 2.1826860904693604
Step 223 | learning_rate: 9.28709055876686e-05
Step 223 | epoch: 0.21483622350674375
Step 224 | loss: 3.5936310291290283
Step 224 | grad_norm: 2.806131601333618
Step 224 | learning_rate: 9.28387925497752e-05
Step 224 | epoch: 0.21579961464354527
Step 225 | loss: 2.955331802368164
Step 225 | grad_norm: 2.0054843425750732
Step 225 | learning_rate: 9.280667951188183e-05
Step 225 | epoch: 0.21676300578034682
Step 226 | loss: 2.8032498359680176
Step 226 | grad_norm: 1.8765157461166382
Step 226 | learning_rate: 9.277456647398845e-05
Step 226 | epoch: 0.21772639691714837
Step 227 | loss: 3.0340960025787354
Step 227 | grad_norm: 1.9258452653884888
Step 227 | learning_rate: 9.274245343609506e-05
Step 227 | epoch: 0.2186897880539499
Step 228 | loss: 4.110362529754639
Step 228 | grad_norm: 2.296473979949951
Step 228 | learning_rate: 9.271034039820168e-05
Step 228 | epoch: 0.21965317919075145
Step 229 | loss: 2.9096198081970215
Step 229 | grad_norm: 1.7621198892593384
Step 229 | learning_rate: 9.267822736030828e-05
Step 229 | epoch: 0.220616570327553
Step 230 | loss: 4.257779598236084
Step 230 | grad_norm: 2.482492446899414
Step 230 | learning_rate: 9.26461143224149e-05
Step 230 | epoch: 0.22157996146435452
Step 231 | loss: 3.448019504547119
Step 231 | grad_norm: 2.1570262908935547
Step 231 | learning_rate: 9.261400128452153e-05
Step 231 | epoch: 0.22254335260115607
Step 232 | loss: 3.0255329608917236
Step 232 | grad_norm: 1.5221508741378784
Step 232 | learning_rate: 9.258188824662813e-05
Step 232 | epoch: 0.22350674373795762
Step 233 | loss: 3.929126024246216
Step 233 | grad_norm: 2.7584424018859863
Step 233 | learning_rate: 9.254977520873474e-05
Step 233 | epoch: 0.22447013487475914
Step 234 | loss: 3.444124937057495
Step 234 | grad_norm: 2.149542808532715
Step 234 | learning_rate: 9.251766217084137e-05
Step 234 | epoch: 0.2254335260115607
Step 235 | loss: 3.870159149169922
Step 235 | grad_norm: 5.546450614929199
Step 235 | learning_rate: 9.248554913294798e-05
Step 235 | epoch: 0.22639691714836224
Step 236 | loss: 3.388381004333496
Step 236 | grad_norm: 2.0176634788513184
Step 236 | learning_rate: 9.24534360950546e-05
Step 236 | epoch: 0.22736030828516376
Step 237 | loss: 3.7274341583251953
Step 237 | grad_norm: 2.0163211822509766
Step 237 | learning_rate: 9.242132305716121e-05
Step 237 | epoch: 0.22832369942196531
Step 238 | loss: 3.384119987487793
Step 238 | grad_norm: 1.9988725185394287
Step 238 | learning_rate: 9.238921001926782e-05
Step 238 | epoch: 0.22928709055876687
Step 239 | loss: 3.3855061531066895
Step 239 | grad_norm: 2.322690963745117
Step 239 | learning_rate: 9.235709698137445e-05
Step 239 | epoch: 0.2302504816955684
Step 240 | loss: 2.837419271469116
Step 240 | grad_norm: 2.2007458209991455
Step 240 | learning_rate: 9.232498394348106e-05
Step 240 | epoch: 0.23121387283236994
Step 241 | loss: 3.5164144039154053
Step 241 | grad_norm: 2.84993839263916
Step 241 | learning_rate: 9.229287090558767e-05
Step 241 | epoch: 0.2321772639691715
Step 242 | loss: 3.9473700523376465
Step 242 | grad_norm: 2.7316479682922363
Step 242 | learning_rate: 9.226075786769429e-05
Step 242 | epoch: 0.23314065510597304
Step 243 | loss: 3.3474512100219727
Step 243 | grad_norm: 2.287349224090576
Step 243 | learning_rate: 9.222864482980091e-05
Step 243 | epoch: 0.23410404624277456
Step 244 | loss: 3.6493959426879883
Step 244 | grad_norm: 2.2861218452453613
Step 244 | learning_rate: 9.219653179190752e-05
Step 244 | epoch: 0.2350674373795761
Step 245 | loss: 3.8220551013946533
Step 245 | grad_norm: 1.9208054542541504
Step 245 | learning_rate: 9.216441875401414e-05
Step 245 | epoch: 0.23603082851637766
Step 246 | loss: 4.255275249481201
Step 246 | grad_norm: 2.191152334213257
Step 246 | learning_rate: 9.213230571612075e-05
Step 246 | epoch: 0.23699421965317918
Step 247 | loss: 3.198625326156616
Step 247 | grad_norm: 1.8033576011657715
Step 247 | learning_rate: 9.210019267822737e-05
Step 247 | epoch: 0.23795761078998073
Step 248 | loss: 3.4928667545318604
Step 248 | grad_norm: 2.0613667964935303
Step 248 | learning_rate: 9.206807964033399e-05
Step 248 | epoch: 0.23892100192678228
Step 249 | loss: 3.7446467876434326
Step 249 | grad_norm: 2.207382917404175
Step 249 | learning_rate: 9.20359666024406e-05
Step 249 | epoch: 0.2398843930635838
Step 250 | loss: 3.636969804763794
Step 250 | grad_norm: 3.3606696128845215
Step 250 | learning_rate: 9.200385356454722e-05
Step 250 | epoch: 0.24084778420038536
Step 251 | loss: 3.6870555877685547
Step 251 | grad_norm: 1.7673918008804321
Step 251 | learning_rate: 9.197174052665382e-05
Step 251 | epoch: 0.2418111753371869
Step 252 | loss: 3.4339332580566406
Step 252 | grad_norm: 1.9712172746658325
Step 252 | learning_rate: 9.193962748876044e-05
Step 252 | epoch: 0.24277456647398843
Step 253 | loss: 3.3641154766082764
Step 253 | grad_norm: 1.9349805116653442
Step 253 | learning_rate: 9.190751445086706e-05
Step 253 | epoch: 0.24373795761078998
Step 254 | loss: 4.047788619995117
Step 254 | grad_norm: 2.0764944553375244
Step 254 | learning_rate: 9.187540141297367e-05
Step 254 | epoch: 0.24470134874759153
Step 255 | loss: 3.111388683319092
Step 255 | grad_norm: 1.7799415588378906
Step 255 | learning_rate: 9.184328837508028e-05
Step 255 | epoch: 0.24566473988439305
Step 256 | loss: 3.4401559829711914
Step 256 | grad_norm: 1.967657446861267
Step 256 | learning_rate: 9.18111753371869e-05
Step 256 | epoch: 0.2466281310211946
Step 257 | loss: 3.528566598892212
Step 257 | grad_norm: 1.8559516668319702
Step 257 | learning_rate: 9.177906229929352e-05
Step 257 | epoch: 0.24759152215799615
Step 258 | loss: 3.056720018386841
Step 258 | grad_norm: 1.7461214065551758
Step 258 | learning_rate: 9.174694926140013e-05
Step 258 | epoch: 0.24855491329479767
Step 259 | loss: 3.5810794830322266
Step 259 | grad_norm: 1.7425682544708252
Step 259 | learning_rate: 9.171483622350675e-05
Step 259 | epoch: 0.24951830443159922
Step 260 | loss: 3.010922431945801
Step 260 | grad_norm: 1.8846638202667236
Step 260 | learning_rate: 9.168272318561336e-05
Step 260 | epoch: 0.2504816955684008
Step 261 | loss: 3.647477865219116
Step 261 | grad_norm: 2.424564838409424
Step 261 | learning_rate: 9.165061014771998e-05
Step 261 | epoch: 0.2514450867052023
Step 262 | loss: 3.7074103355407715
Step 262 | grad_norm: 2.305131196975708
Step 262 | learning_rate: 9.16184971098266e-05
Step 262 | epoch: 0.2524084778420039
Step 263 | loss: 3.7528598308563232
Step 263 | grad_norm: 2.0030765533447266
Step 263 | learning_rate: 9.15863840719332e-05
Step 263 | epoch: 0.25337186897880537
Step 264 | loss: 2.9466476440429688
Step 264 | grad_norm: 1.6665297746658325
Step 264 | learning_rate: 9.155427103403983e-05
Step 264 | epoch: 0.2543352601156069
Step 265 | loss: 4.371106147766113
Step 265 | grad_norm: 2.370023727416992
Step 265 | learning_rate: 9.152215799614643e-05
Step 265 | epoch: 0.25529865125240847
Step 266 | loss: 4.382811546325684
Step 266 | grad_norm: 2.4699454307556152
Step 266 | learning_rate: 9.149004495825306e-05
Step 266 | epoch: 0.25626204238921
Step 267 | loss: 2.984369993209839
Step 267 | grad_norm: 1.6829842329025269
Step 267 | learning_rate: 9.145793192035968e-05
Step 267 | epoch: 0.25722543352601157
Step 268 | loss: 3.6593194007873535
Step 268 | grad_norm: 1.6934293508529663
Step 268 | learning_rate: 9.142581888246628e-05
Step 268 | epoch: 0.2581888246628131
Step 269 | loss: 2.9037322998046875
Step 269 | grad_norm: 2.239839553833008
Step 269 | learning_rate: 9.139370584457289e-05
Step 269 | epoch: 0.2591522157996146
Step 270 | loss: 3.108689546585083
Step 270 | grad_norm: 1.7694482803344727
Step 270 | learning_rate: 9.136159280667951e-05
Step 270 | epoch: 0.26011560693641617
Step 271 | loss: 3.56477427482605
Step 271 | grad_norm: 2.021667718887329
Step 271 | learning_rate: 9.132947976878613e-05
Step 271 | epoch: 0.2610789980732177
Step 272 | loss: 3.6510074138641357
Step 272 | grad_norm: 2.0361366271972656
Step 272 | learning_rate: 9.129736673089274e-05
Step 272 | epoch: 0.26204238921001927
Step 273 | loss: 2.899883985519409
Step 273 | grad_norm: 2.5141167640686035
Step 273 | learning_rate: 9.126525369299936e-05
Step 273 | epoch: 0.2630057803468208
Step 274 | loss: 3.2577366828918457
Step 274 | grad_norm: 1.8080682754516602
Step 274 | learning_rate: 9.123314065510597e-05
Step 274 | epoch: 0.26396917148362237
Step 275 | loss: 2.7720556259155273
Step 275 | grad_norm: 2.57820463180542
Step 275 | learning_rate: 9.12010276172126e-05
Step 275 | epoch: 0.2649325626204239
Step 276 | loss: 4.577566146850586
Step 276 | grad_norm: 2.7717628479003906
Step 276 | learning_rate: 9.116891457931921e-05
Step 276 | epoch: 0.2658959537572254
Step 277 | loss: 3.2295007705688477
Step 277 | grad_norm: 1.6158963441848755
Step 277 | learning_rate: 9.113680154142582e-05
Step 277 | epoch: 0.26685934489402696
Step 278 | loss: 3.2308566570281982
Step 278 | grad_norm: 2.040696382522583
Step 278 | learning_rate: 9.110468850353244e-05
Step 278 | epoch: 0.2678227360308285
Step 279 | loss: 3.0415844917297363
Step 279 | grad_norm: 1.870710015296936
Step 279 | learning_rate: 9.107257546563906e-05
Step 279 | epoch: 0.26878612716763006
Step 280 | loss: 4.020501136779785
Step 280 | grad_norm: 1.9724591970443726
Step 280 | learning_rate: 9.104046242774567e-05
Step 280 | epoch: 0.2697495183044316
Step 281 | loss: 3.6058919429779053
Step 281 | grad_norm: 1.7398518323898315
Step 281 | learning_rate: 9.100834938985229e-05
Step 281 | epoch: 0.27071290944123316
Step 282 | loss: 3.4260270595550537
Step 282 | grad_norm: 1.8515572547912598
Step 282 | learning_rate: 9.09762363519589e-05
Step 282 | epoch: 0.27167630057803466
Step 283 | loss: 3.2425105571746826
Step 283 | grad_norm: 2.061647415161133
Step 283 | learning_rate: 9.09441233140655e-05
Step 283 | epoch: 0.2726396917148362
Step 284 | loss: 3.598242998123169
Step 284 | grad_norm: 1.8630893230438232
Step 284 | learning_rate: 9.091201027617214e-05
Step 284 | epoch: 0.27360308285163776
Step 285 | loss: 3.451542854309082
Step 285 | grad_norm: 1.7714282274246216
Step 285 | learning_rate: 9.087989723827875e-05
Step 285 | epoch: 0.2745664739884393
Step 286 | loss: 3.629965305328369
Step 286 | grad_norm: 2.4760825634002686
Step 286 | learning_rate: 9.084778420038537e-05
Step 286 | epoch: 0.27552986512524086
Step 287 | loss: 3.992436647415161
Step 287 | grad_norm: 2.85862398147583
Step 287 | learning_rate: 9.081567116249197e-05
Step 287 | epoch: 0.2764932562620424
Step 288 | loss: 3.744920253753662
Step 288 | grad_norm: 2.902620792388916
Step 288 | learning_rate: 9.07835581245986e-05
Step 288 | epoch: 0.2774566473988439
Step 289 | loss: 3.2502708435058594
Step 289 | grad_norm: 2.372952461242676
Step 289 | learning_rate: 9.075144508670522e-05
Step 289 | epoch: 0.27842003853564545
Step 290 | loss: 4.146472930908203
Step 290 | grad_norm: 2.4082329273223877
Step 290 | learning_rate: 9.071933204881182e-05
Step 290 | epoch: 0.279383429672447
Step 291 | loss: 3.79687762260437
Step 291 | grad_norm: 2.2366888523101807
Step 291 | learning_rate: 9.068721901091843e-05
Step 291 | epoch: 0.28034682080924855
Step 292 | loss: 3.2597827911376953
Step 292 | grad_norm: 2.4843695163726807
Step 292 | learning_rate: 9.065510597302505e-05
Step 292 | epoch: 0.2813102119460501
Step 293 | loss: 3.309809923171997
Step 293 | grad_norm: 2.169903516769409
Step 293 | learning_rate: 9.062299293513167e-05
Step 293 | epoch: 0.28227360308285165
Step 294 | loss: 3.1864449977874756
Step 294 | grad_norm: 2.966730833053589
Step 294 | learning_rate: 9.059087989723828e-05
Step 294 | epoch: 0.2832369942196532
Step 295 | loss: 3.3691742420196533
Step 295 | grad_norm: 1.7457820177078247
Step 295 | learning_rate: 9.05587668593449e-05
Step 295 | epoch: 0.2842003853564547
Step 296 | loss: 3.3882381916046143
Step 296 | grad_norm: 2.129758834838867
Step 296 | learning_rate: 9.052665382145151e-05
Step 296 | epoch: 0.28516377649325625
Step 297 | loss: 3.061765193939209
Step 297 | grad_norm: 2.0863099098205566
Step 297 | learning_rate: 9.049454078355813e-05
Step 297 | epoch: 0.2861271676300578
Step 298 | loss: 3.0132808685302734
Step 298 | grad_norm: 5.167612075805664
Step 298 | learning_rate: 9.046242774566475e-05
Step 298 | epoch: 0.28709055876685935
Step 299 | loss: 3.545565366744995
Step 299 | grad_norm: 2.2081284523010254
Step 299 | learning_rate: 9.043031470777136e-05
Step 299 | epoch: 0.2880539499036609
Step 300 | loss: 3.524074077606201
Step 300 | grad_norm: 2.837690591812134
Step 300 | learning_rate: 9.039820166987798e-05
Step 300 | epoch: 0.28901734104046245
Step 301 | loss: 3.2066397666931152
Step 301 | grad_norm: 1.8225709199905396
Step 301 | learning_rate: 9.036608863198459e-05
Step 301 | epoch: 0.28998073217726394
Step 302 | loss: 3.3246617317199707
Step 302 | grad_norm: 1.8524836301803589
Step 302 | learning_rate: 9.033397559409121e-05
Step 302 | epoch: 0.2909441233140655
Step 303 | loss: 3.521768808364868
Step 303 | grad_norm: 2.230565071105957
Step 303 | learning_rate: 9.030186255619783e-05
Step 303 | epoch: 0.29190751445086704
Step 304 | loss: 3.2434935569763184
Step 304 | grad_norm: 2.099425792694092
Step 304 | learning_rate: 9.026974951830444e-05
Step 304 | epoch: 0.2928709055876686
Step 305 | loss: 4.080508708953857
Step 305 | grad_norm: 3.1824934482574463
Step 305 | learning_rate: 9.023763648041104e-05
Step 305 | epoch: 0.29383429672447015
Step 306 | loss: 2.8977599143981934
Step 306 | grad_norm: 2.07303524017334
Step 306 | learning_rate: 9.020552344251766e-05
Step 306 | epoch: 0.2947976878612717
Step 307 | loss: 4.211945056915283
Step 307 | grad_norm: 2.272268056869507
Step 307 | learning_rate: 9.017341040462428e-05
Step 307 | epoch: 0.2957610789980732
Step 308 | loss: 3.3779115676879883
Step 308 | grad_norm: 2.010340452194214
Step 308 | learning_rate: 9.014129736673089e-05
Step 308 | epoch: 0.29672447013487474
Step 309 | loss: 3.101116418838501
Step 309 | grad_norm: 2.3601343631744385
Step 309 | learning_rate: 9.010918432883751e-05
Step 309 | epoch: 0.2976878612716763
Step 310 | loss: 2.66074800491333
Step 310 | grad_norm: 1.7494676113128662
Step 310 | learning_rate: 9.007707129094412e-05
Step 310 | epoch: 0.29865125240847784
Step 311 | loss: 3.6804771423339844
Step 311 | grad_norm: 2.3136680126190186
Step 311 | learning_rate: 9.004495825305074e-05
Step 311 | epoch: 0.2996146435452794
Step 312 | loss: 2.7881276607513428
Step 312 | grad_norm: 2.1041204929351807
Step 312 | learning_rate: 9.001284521515736e-05
Step 312 | epoch: 0.30057803468208094
Step 313 | loss: 3.1978776454925537
Step 313 | grad_norm: 2.3724939823150635
Step 313 | learning_rate: 8.998073217726397e-05
Step 313 | epoch: 0.3015414258188825
Step 314 | loss: 3.7971279621124268
Step 314 | grad_norm: 2.327314853668213
Step 314 | learning_rate: 8.994861913937059e-05
Step 314 | epoch: 0.302504816955684
Step 315 | loss: 4.685607433319092
Step 315 | grad_norm: 2.7745859622955322
Step 315 | learning_rate: 8.99165061014772e-05
Step 315 | epoch: 0.30346820809248554
Step 316 | loss: 3.6485238075256348
Step 316 | grad_norm: 2.073322057723999
Step 316 | learning_rate: 8.988439306358382e-05
Step 316 | epoch: 0.3044315992292871
Step 317 | loss: 3.389921188354492
Step 317 | grad_norm: 2.134357213973999
Step 317 | learning_rate: 8.985228002569044e-05
Step 317 | epoch: 0.30539499036608864
Step 318 | loss: 3.2372169494628906
Step 318 | grad_norm: 1.8275202512741089
Step 318 | learning_rate: 8.982016698779705e-05
Step 318 | epoch: 0.3063583815028902
Step 319 | loss: 3.974834442138672
Step 319 | grad_norm: 2.689204216003418
Step 319 | learning_rate: 8.978805394990366e-05
Step 319 | epoch: 0.30732177263969174
Step 320 | loss: 3.7568469047546387
Step 320 | grad_norm: 3.467327117919922
Step 320 | learning_rate: 8.975594091201029e-05
Step 320 | epoch: 0.30828516377649323
Step 321 | loss: 3.602992057800293
Step 321 | grad_norm: 2.3497116565704346
Step 321 | learning_rate: 8.97238278741169e-05
Step 321 | epoch: 0.3092485549132948
Step 322 | loss: 2.7265772819519043
Step 322 | grad_norm: 1.666122555732727
Step 322 | learning_rate: 8.96917148362235e-05
Step 322 | epoch: 0.31021194605009633
Step 323 | loss: 3.7539138793945312
Step 323 | grad_norm: 2.451153039932251
Step 323 | learning_rate: 8.965960179833013e-05
Step 323 | epoch: 0.3111753371868979
Step 324 | loss: 3.3867850303649902
Step 324 | grad_norm: 1.8665578365325928
Step 324 | learning_rate: 8.962748876043673e-05
Step 324 | epoch: 0.31213872832369943
Step 325 | loss: 2.514850378036499
Step 325 | grad_norm: 2.260312080383301
Step 325 | learning_rate: 8.959537572254337e-05
Step 325 | epoch: 0.313102119460501
Step 326 | loss: 3.2336585521698
Step 326 | grad_norm: 1.8395020961761475
Step 326 | learning_rate: 8.956326268464997e-05
Step 326 | epoch: 0.3140655105973025
Step 327 | loss: 3.7595767974853516
Step 327 | grad_norm: 1.9227335453033447
Step 327 | learning_rate: 8.953114964675658e-05
Step 327 | epoch: 0.315028901734104
Step 328 | loss: 3.5371410846710205
Step 328 | grad_norm: 2.2108099460601807
Step 328 | learning_rate: 8.94990366088632e-05
Step 328 | epoch: 0.3159922928709056
Step 329 | loss: 2.9260051250457764
Step 329 | grad_norm: 1.905502438545227
Step 329 | learning_rate: 8.946692357096982e-05
Step 329 | epoch: 0.31695568400770713
Step 330 | loss: 2.8911633491516113
Step 330 | grad_norm: 2.154111385345459
Step 330 | learning_rate: 8.943481053307643e-05
Step 330 | epoch: 0.3179190751445087
Step 331 | loss: 2.723632574081421
Step 331 | grad_norm: 2.2246856689453125
Step 331 | learning_rate: 8.940269749518305e-05
Step 331 | epoch: 0.31888246628131023
Step 332 | loss: 3.2180373668670654
Step 332 | grad_norm: 2.0026309490203857
Step 332 | learning_rate: 8.937058445728966e-05
Step 332 | epoch: 0.3198458574181118
Step 333 | loss: 3.5134358406066895
Step 333 | grad_norm: 2.2451353073120117
Step 333 | learning_rate: 8.933847141939628e-05
Step 333 | epoch: 0.3208092485549133
Step 334 | loss: 3.9259374141693115
Step 334 | grad_norm: 2.555209159851074
Step 334 | learning_rate: 8.93063583815029e-05
Step 334 | epoch: 0.3217726396917148
Step 335 | loss: 3.249521493911743
Step 335 | grad_norm: 1.9763051271438599
Step 335 | learning_rate: 8.927424534360951e-05
Step 335 | epoch: 0.3227360308285164
Step 336 | loss: 4.047640800476074
Step 336 | grad_norm: 2.269346237182617
Step 336 | learning_rate: 8.924213230571612e-05
Step 336 | epoch: 0.3236994219653179
Step 337 | loss: 3.6867551803588867
Step 337 | grad_norm: 2.07485032081604
Step 337 | learning_rate: 8.921001926782274e-05
Step 337 | epoch: 0.3246628131021195
Step 338 | loss: 3.137354612350464
Step 338 | grad_norm: 1.6824991703033447
Step 338 | learning_rate: 8.917790622992936e-05
Step 338 | epoch: 0.325626204238921
Step 339 | loss: 3.063230037689209
Step 339 | grad_norm: 1.8668975830078125
Step 339 | learning_rate: 8.914579319203598e-05
Step 339 | epoch: 0.3265895953757225
Step 340 | loss: 3.421396017074585
Step 340 | grad_norm: 2.084484815597534
Step 340 | learning_rate: 8.911368015414259e-05
Step 340 | epoch: 0.32755298651252407
Step 341 | loss: 3.5384161472320557
Step 341 | grad_norm: 2.170713186264038
Step 341 | learning_rate: 8.90815671162492e-05
Step 341 | epoch: 0.3285163776493256
Step 342 | loss: 3.577180862426758
Step 342 | grad_norm: 1.940015435218811
Step 342 | learning_rate: 8.904945407835582e-05
Step 342 | epoch: 0.32947976878612717
Step 343 | loss: 2.6896581649780273
Step 343 | grad_norm: 2.3109946250915527
Step 343 | learning_rate: 8.901734104046244e-05
Step 343 | epoch: 0.3304431599229287
Step 344 | loss: 3.4201927185058594
Step 344 | grad_norm: 2.026228904724121
Step 344 | learning_rate: 8.898522800256904e-05
Step 344 | epoch: 0.33140655105973027
Step 345 | loss: 3.323725461959839
Step 345 | grad_norm: 2.2287302017211914
Step 345 | learning_rate: 8.895311496467566e-05
Step 345 | epoch: 0.33236994219653176
Step 346 | loss: 3.9373185634613037
Step 346 | grad_norm: 2.8104114532470703
Step 346 | learning_rate: 8.892100192678227e-05
Step 346 | epoch: 0.3333333333333333
Step 347 | loss: 3.9355506896972656
Step 347 | grad_norm: 2.714615821838379
Step 347 | learning_rate: 8.888888888888889e-05
Step 347 | epoch: 0.33429672447013487
Step 348 | loss: 3.7546770572662354
Step 348 | grad_norm: 2.3540220260620117
Step 348 | learning_rate: 8.885677585099551e-05
Step 348 | epoch: 0.3352601156069364
Step 349 | loss: 3.6022255420684814
Step 349 | grad_norm: 1.8258548974990845
Step 349 | learning_rate: 8.882466281310212e-05
Step 349 | epoch: 0.33622350674373797
Step 350 | loss: 3.462995767593384
Step 350 | grad_norm: 2.4589643478393555
Step 350 | learning_rate: 8.879254977520874e-05
Step 350 | epoch: 0.3371868978805395
Step 351 | loss: 4.938734531402588
Step 351 | grad_norm: 3.008488655090332
Step 351 | learning_rate: 8.876043673731535e-05
Step 351 | epoch: 0.33815028901734107
Step 352 | loss: 3.690324068069458
Step 352 | grad_norm: 1.9277867078781128
Step 352 | learning_rate: 8.872832369942197e-05
Step 352 | epoch: 0.33911368015414256
Step 353 | loss: 3.6985840797424316
Step 353 | grad_norm: 2.6111843585968018
Step 353 | learning_rate: 8.869621066152859e-05
Step 353 | epoch: 0.3400770712909441
Step 354 | loss: 3.9023027420043945
Step 354 | grad_norm: 1.9457751512527466
Step 354 | learning_rate: 8.86640976236352e-05
Step 354 | epoch: 0.34104046242774566
Step 355 | loss: 3.85835862159729
Step 355 | grad_norm: 2.501840591430664
Step 355 | learning_rate: 8.86319845857418e-05
Step 355 | epoch: 0.3420038535645472
Step 356 | loss: 3.0870959758758545
Step 356 | grad_norm: 2.1046483516693115
Step 356 | learning_rate: 8.859987154784843e-05
Step 356 | epoch: 0.34296724470134876
Step 357 | loss: 3.287567138671875
Step 357 | grad_norm: 2.1969316005706787
Step 357 | learning_rate: 8.856775850995505e-05
Step 357 | epoch: 0.3439306358381503
Step 358 | loss: 3.206679105758667
Step 358 | grad_norm: 2.098424196243286
Step 358 | learning_rate: 8.853564547206166e-05
Step 358 | epoch: 0.3448940269749518
Step 359 | loss: 4.338985443115234
Step 359 | grad_norm: 3.0896830558776855
Step 359 | learning_rate: 8.850353243416828e-05
Step 359 | epoch: 0.34585741811175336
Step 360 | loss: 3.8362534046173096
Step 360 | grad_norm: 2.3339056968688965
Step 360 | learning_rate: 8.847141939627488e-05
Step 360 | epoch: 0.3468208092485549
Step 361 | loss: 3.3177413940429688
Step 361 | grad_norm: 2.6414129734039307
Step 361 | learning_rate: 8.84393063583815e-05
Step 361 | epoch: 0.34778420038535646
Step 362 | loss: 3.403512716293335
Step 362 | grad_norm: 1.6414183378219604
Step 362 | learning_rate: 8.840719332048813e-05
Step 362 | epoch: 0.348747591522158
Step 363 | loss: 3.498511791229248
Step 363 | grad_norm: 2.672879934310913
Step 363 | learning_rate: 8.837508028259473e-05
Step 363 | epoch: 0.34971098265895956
Step 364 | loss: 3.582183837890625
Step 364 | grad_norm: 4.727872848510742
Step 364 | learning_rate: 8.834296724470135e-05
Step 364 | epoch: 0.35067437379576105
Step 365 | loss: 4.0537614822387695
Step 365 | grad_norm: 2.3239052295684814
Step 365 | learning_rate: 8.831085420680798e-05
Step 365 | epoch: 0.3516377649325626
Step 366 | loss: 3.7171387672424316
Step 366 | grad_norm: 2.1401419639587402
Step 366 | learning_rate: 8.827874116891458e-05
Step 366 | epoch: 0.35260115606936415
Step 367 | loss: 3.241042137145996
Step 367 | grad_norm: 2.3086462020874023
Step 367 | learning_rate: 8.82466281310212e-05
Step 367 | epoch: 0.3535645472061657
Step 368 | loss: 3.132087469100952
Step 368 | grad_norm: 1.8330519199371338
Step 368 | learning_rate: 8.821451509312781e-05
Step 368 | epoch: 0.35452793834296725
Step 369 | loss: 3.6024672985076904
Step 369 | grad_norm: 2.3294312953948975
Step 369 | learning_rate: 8.818240205523442e-05
Step 369 | epoch: 0.3554913294797688
Step 370 | loss: 3.5271124839782715
Step 370 | grad_norm: 2.4524753093719482
Step 370 | learning_rate: 8.815028901734105e-05
Step 370 | epoch: 0.35645472061657035
Step 371 | loss: 3.6923909187316895
Step 371 | grad_norm: 2.096583127975464
Step 371 | learning_rate: 8.811817597944766e-05
Step 371 | epoch: 0.35741811175337185
Step 372 | loss: 3.9984984397888184
Step 372 | grad_norm: 2.1750643253326416
Step 372 | learning_rate: 8.808606294155427e-05
Step 372 | epoch: 0.3583815028901734
Step 373 | loss: 2.7705881595611572
Step 373 | grad_norm: 1.7940175533294678
Step 373 | learning_rate: 8.805394990366089e-05
Step 373 | epoch: 0.35934489402697495
Step 374 | loss: 3.723116159439087
Step 374 | grad_norm: 2.2250635623931885
Step 374 | learning_rate: 8.802183686576751e-05
Step 374 | epoch: 0.3603082851637765
Step 375 | loss: 3.1474087238311768
Step 375 | grad_norm: 2.3619306087493896
Step 375 | learning_rate: 8.798972382787413e-05
Step 375 | epoch: 0.36127167630057805
Step 376 | loss: 3.523251533508301
Step 376 | grad_norm: 2.356177806854248
Step 376 | learning_rate: 8.795761078998074e-05
Step 376 | epoch: 0.3622350674373796
Step 377 | loss: 3.2124831676483154
Step 377 | grad_norm: 2.397920608520508
Step 377 | learning_rate: 8.792549775208735e-05
Step 377 | epoch: 0.3631984585741811
Step 378 | loss: 2.9843764305114746
Step 378 | grad_norm: 2.4406352043151855
Step 378 | learning_rate: 8.789338471419397e-05
Step 378 | epoch: 0.36416184971098264
Step 379 | loss: 3.2076897621154785
Step 379 | grad_norm: 2.045071840286255
Step 379 | learning_rate: 8.786127167630059e-05
Step 379 | epoch: 0.3651252408477842
Step 380 | loss: 3.3167600631713867
Step 380 | grad_norm: 2.1400387287139893
Step 380 | learning_rate: 8.78291586384072e-05
Step 380 | epoch: 0.36608863198458574
Step 381 | loss: 3.4205899238586426
Step 381 | grad_norm: 2.36511492729187
Step 381 | learning_rate: 8.779704560051382e-05
Step 381 | epoch: 0.3670520231213873
Step 382 | loss: 3.076568603515625
Step 382 | grad_norm: 2.408823013305664
Step 382 | learning_rate: 8.776493256262042e-05
Step 382 | epoch: 0.36801541425818884
Step 383 | loss: 3.126340866088867
Step 383 | grad_norm: 1.9145797491073608
Step 383 | learning_rate: 8.773281952472704e-05
Step 383 | epoch: 0.36897880539499034
Step 384 | loss: 3.5049479007720947
Step 384 | grad_norm: 2.0650839805603027
Step 384 | learning_rate: 8.770070648683367e-05
Step 384 | epoch: 0.3699421965317919
Step 385 | loss: 2.7659294605255127
Step 385 | grad_norm: 2.0903069972991943
Step 385 | learning_rate: 8.766859344894027e-05
Step 385 | epoch: 0.37090558766859344
Step 386 | loss: 2.91168212890625
Step 386 | grad_norm: 2.5240612030029297
Step 386 | learning_rate: 8.763648041104688e-05
Step 386 | epoch: 0.371868978805395
Step 387 | loss: 3.08122181892395
Step 387 | grad_norm: 1.7951699495315552
Step 387 | learning_rate: 8.76043673731535e-05
Step 387 | epoch: 0.37283236994219654
Step 388 | loss: 3.912147283554077
Step 388 | grad_norm: 2.118304967880249
Step 388 | learning_rate: 8.757225433526012e-05
Step 388 | epoch: 0.3737957610789981
Step 389 | loss: 3.2946937084198
Step 389 | grad_norm: 2.031332492828369
Step 389 | learning_rate: 8.754014129736674e-05
Step 389 | epoch: 0.37475915221579964
Step 390 | loss: 3.5937931537628174
Step 390 | grad_norm: 2.2034952640533447
Step 390 | learning_rate: 8.750802825947335e-05
Step 390 | epoch: 0.37572254335260113
Step 391 | loss: 3.2793917655944824
Step 391 | grad_norm: 2.3947885036468506
Step 391 | learning_rate: 8.747591522157996e-05
Step 391 | epoch: 0.3766859344894027
Step 392 | loss: 2.7836809158325195
Step 392 | grad_norm: 2.470130205154419
Step 392 | learning_rate: 8.744380218368658e-05
Step 392 | epoch: 0.37764932562620424
Step 393 | loss: 3.3034965991973877
Step 393 | grad_norm: 2.5667285919189453
Step 393 | learning_rate: 8.74116891457932e-05
Step 393 | epoch: 0.3786127167630058
Step 394 | loss: 3.476297378540039
Step 394 | grad_norm: 2.8222997188568115
Step 394 | learning_rate: 8.737957610789981e-05
Step 394 | epoch: 0.37957610789980734
Step 395 | loss: 2.5119099617004395
Step 395 | grad_norm: 1.8554939031600952
Step 395 | learning_rate: 8.734746307000643e-05
Step 395 | epoch: 0.3805394990366089
Step 396 | loss: 3.341444253921509
Step 396 | grad_norm: 2.1549417972564697
Step 396 | learning_rate: 8.731535003211304e-05
Step 396 | epoch: 0.3815028901734104
Step 397 | loss: 3.3524253368377686
Step 397 | grad_norm: 2.1655654907226562
Step 397 | learning_rate: 8.728323699421966e-05
Step 397 | epoch: 0.38246628131021193
Step 398 | loss: 3.6965091228485107
Step 398 | grad_norm: 2.039259672164917
Step 398 | learning_rate: 8.725112395632628e-05
Step 398 | epoch: 0.3834296724470135
Step 399 | loss: 4.43502140045166
Step 399 | grad_norm: 2.5755727291107178
Step 399 | learning_rate: 8.721901091843289e-05
Step 399 | epoch: 0.38439306358381503
Step 400 | loss: 3.0474538803100586
Step 400 | grad_norm: 2.481865167617798
Step 400 | learning_rate: 8.71868978805395e-05
Step 400 | epoch: 0.3853564547206166
Step 401 | loss: 3.632239580154419
Step 401 | grad_norm: 2.2001852989196777
Step 401 | learning_rate: 8.715478484264611e-05
Step 401 | epoch: 0.38631984585741813
Step 402 | loss: 3.682433605194092
Step 402 | grad_norm: 2.711878538131714
Step 402 | learning_rate: 8.712267180475273e-05
Step 402 | epoch: 0.3872832369942196
Step 403 | loss: 3.4386935234069824
Step 403 | grad_norm: 1.928322434425354
Step 403 | learning_rate: 8.709055876685936e-05
Step 403 | epoch: 0.3882466281310212
Step 404 | loss: 3.9926936626434326
Step 404 | grad_norm: 2.3965089321136475
Step 404 | learning_rate: 8.705844572896596e-05
Step 404 | epoch: 0.3892100192678227
Step 405 | loss: 3.6845498085021973
Step 405 | grad_norm: 2.6205151081085205
Step 405 | learning_rate: 8.702633269107257e-05
Step 405 | epoch: 0.3901734104046243
Step 406 | loss: 3.68066143989563
Step 406 | grad_norm: 3.9889321327209473
Step 406 | learning_rate: 8.69942196531792e-05
Step 406 | epoch: 0.3911368015414258
Step 407 | loss: 3.8206965923309326
Step 407 | grad_norm: 2.3373005390167236
Step 407 | learning_rate: 8.696210661528581e-05
Step 407 | epoch: 0.3921001926782274
Step 408 | loss: 3.7191572189331055
Step 408 | grad_norm: 2.1051509380340576
Step 408 | learning_rate: 8.692999357739242e-05
Step 408 | epoch: 0.3930635838150289
Step 409 | loss: 3.155045509338379
Step 409 | grad_norm: 2.2160415649414062
Step 409 | learning_rate: 8.689788053949904e-05
Step 409 | epoch: 0.3940269749518304
Step 410 | loss: 3.4488439559936523
Step 410 | grad_norm: 2.0185604095458984
Step 410 | learning_rate: 8.686576750160566e-05
Step 410 | epoch: 0.394990366088632
Step 411 | loss: 4.0307793617248535
Step 411 | grad_norm: 3.7727651596069336
Step 411 | learning_rate: 8.683365446371227e-05
Step 411 | epoch: 0.3959537572254335
Step 412 | loss: 3.770106792449951
Step 412 | grad_norm: 2.798888921737671
Step 412 | learning_rate: 8.680154142581889e-05
Step 412 | epoch: 0.3969171483622351
Step 413 | loss: 3.4108455181121826
Step 413 | grad_norm: 2.538753032684326
Step 413 | learning_rate: 8.67694283879255e-05
Step 413 | epoch: 0.3978805394990366
Step 414 | loss: 2.766676902770996
Step 414 | grad_norm: 2.0081887245178223
Step 414 | learning_rate: 8.673731535003212e-05
Step 414 | epoch: 0.3988439306358382
Step 415 | loss: 2.702005624771118
Step 415 | grad_norm: 2.0324809551239014
Step 415 | learning_rate: 8.670520231213874e-05
Step 415 | epoch: 0.39980732177263967
Step 416 | loss: 3.222907543182373
Step 416 | grad_norm: 2.361483573913574
Step 416 | learning_rate: 8.667308927424535e-05
Step 416 | epoch: 0.4007707129094412
Step 417 | loss: 3.2079832553863525
Step 417 | grad_norm: 1.961571455001831
Step 417 | learning_rate: 8.664097623635197e-05
Step 417 | epoch: 0.40173410404624277
Step 418 | loss: 3.5173275470733643
Step 418 | grad_norm: 2.2503879070281982
Step 418 | learning_rate: 8.660886319845858e-05
Step 418 | epoch: 0.4026974951830443
Step 419 | loss: 4.059643745422363
Step 419 | grad_norm: 2.505871057510376
Step 419 | learning_rate: 8.65767501605652e-05
Step 419 | epoch: 0.40366088631984587
Step 420 | loss: 3.241363525390625
Step 420 | grad_norm: 1.7315715551376343
Step 420 | learning_rate: 8.654463712267182e-05
Step 420 | epoch: 0.4046242774566474
Step 421 | loss: 2.389894485473633
Step 421 | grad_norm: 1.705661654472351
Step 421 | learning_rate: 8.651252408477842e-05
Step 421 | epoch: 0.4055876685934489
Step 422 | loss: 4.628812789916992
Step 422 | grad_norm: 2.7448151111602783
Step 422 | learning_rate: 8.648041104688503e-05
Step 422 | epoch: 0.40655105973025046
Step 423 | loss: 2.7908568382263184
Step 423 | grad_norm: 2.8619792461395264
Step 423 | learning_rate: 8.644829800899165e-05
Step 423 | epoch: 0.407514450867052
Step 424 | loss: 3.1337387561798096
Step 424 | grad_norm: 2.1495070457458496
Step 424 | learning_rate: 8.641618497109827e-05
Step 424 | epoch: 0.40847784200385356
Step 425 | loss: 3.1975908279418945
Step 425 | grad_norm: 2.3704187870025635
Step 425 | learning_rate: 8.638407193320488e-05
Step 425 | epoch: 0.4094412331406551
Step 426 | loss: 3.7346692085266113
Step 426 | grad_norm: 2.091843843460083
Step 426 | learning_rate: 8.63519588953115e-05
Step 426 | epoch: 0.41040462427745666
Step 427 | loss: 3.554790735244751
Step 427 | grad_norm: 2.360847234725952
Step 427 | learning_rate: 8.631984585741811e-05
Step 427 | epoch: 0.4113680154142582
Step 428 | loss: 3.437352180480957
Step 428 | grad_norm: 2.051316738128662
Step 428 | learning_rate: 8.628773281952473e-05
Step 428 | epoch: 0.4123314065510597
Step 429 | loss: 2.7507288455963135
Step 429 | grad_norm: 2.317797899246216
Step 429 | learning_rate: 8.625561978163135e-05
Step 429 | epoch: 0.41329479768786126
Step 430 | loss: 2.876089334487915
Step 430 | grad_norm: 1.8032832145690918
Step 430 | learning_rate: 8.622350674373796e-05
Step 430 | epoch: 0.4142581888246628
Step 431 | loss: 4.0166096687316895
Step 431 | grad_norm: 2.5003957748413086
Step 431 | learning_rate: 8.619139370584458e-05
Step 431 | epoch: 0.41522157996146436
Step 432 | loss: 3.3791940212249756
Step 432 | grad_norm: 2.315445899963379
Step 432 | learning_rate: 8.615928066795119e-05
Step 432 | epoch: 0.4161849710982659
Step 433 | loss: 3.7822649478912354
Step 433 | grad_norm: 2.1031875610351562
Step 433 | learning_rate: 8.612716763005781e-05
Step 433 | epoch: 0.41714836223506746
Step 434 | loss: 3.908372163772583
Step 434 | grad_norm: 2.0115926265716553
Step 434 | learning_rate: 8.609505459216443e-05
Step 434 | epoch: 0.41811175337186895
Step 435 | loss: 3.367774486541748
Step 435 | grad_norm: 2.016616106033325
Step 435 | learning_rate: 8.606294155427104e-05
Step 435 | epoch: 0.4190751445086705
Step 436 | loss: 3.817058801651001
Step 436 | grad_norm: 2.2295761108398438
Step 436 | learning_rate: 8.603082851637764e-05
Step 436 | epoch: 0.42003853564547206
Step 437 | loss: 3.7208590507507324
Step 437 | grad_norm: 2.7076027393341064
Step 437 | learning_rate: 8.599871547848426e-05
Step 437 | epoch: 0.4210019267822736
Step 438 | loss: 3.4841151237487793
Step 438 | grad_norm: 2.2554190158843994
Step 438 | learning_rate: 8.596660244059089e-05
Step 438 | epoch: 0.42196531791907516
Step 439 | loss: 3.1073904037475586
Step 439 | grad_norm: 1.9733123779296875
Step 439 | learning_rate: 8.593448940269751e-05
Step 439 | epoch: 0.4229287090558767
Step 440 | loss: 3.6969363689422607
Step 440 | grad_norm: 2.4433674812316895
Step 440 | learning_rate: 8.590237636480411e-05
Step 440 | epoch: 0.4238921001926782
Step 441 | loss: 3.26153302192688
Step 441 | grad_norm: 1.9815013408660889
Step 441 | learning_rate: 8.587026332691072e-05
Step 441 | epoch: 0.42485549132947975
Step 442 | loss: 3.5812456607818604
Step 442 | grad_norm: 3.2816848754882812
Step 442 | learning_rate: 8.583815028901736e-05
Step 442 | epoch: 0.4258188824662813
Step 443 | loss: 4.149617671966553
Step 443 | grad_norm: 2.8253016471862793
Step 443 | learning_rate: 8.580603725112396e-05
Step 443 | epoch: 0.42678227360308285
Step 444 | loss: 3.975680351257324
Step 444 | grad_norm: 2.438920021057129
Step 444 | learning_rate: 8.577392421323057e-05
Step 444 | epoch: 0.4277456647398844
Step 445 | loss: 3.360466480255127
Step 445 | grad_norm: 3.255246639251709
Step 445 | learning_rate: 8.574181117533719e-05
Step 445 | epoch: 0.42870905587668595
Step 446 | loss: 3.696239471435547
Step 446 | grad_norm: 2.897648811340332
Step 446 | learning_rate: 8.57096981374438e-05
Step 446 | epoch: 0.4296724470134875
Step 447 | loss: 3.643383741378784
Step 447 | grad_norm: 2.3852267265319824
Step 447 | learning_rate: 8.567758509955042e-05
Step 447 | epoch: 0.430635838150289
Step 448 | loss: 3.267240047454834
Step 448 | grad_norm: 1.8182018995285034
Step 448 | learning_rate: 8.564547206165704e-05
Step 448 | epoch: 0.43159922928709055
Step 449 | loss: 3.695974111557007
Step 449 | grad_norm: 2.1080338954925537
Step 449 | learning_rate: 8.561335902376365e-05
Step 449 | epoch: 0.4325626204238921
Step 450 | loss: 2.8932015895843506
Step 450 | grad_norm: 2.049530506134033
Step 450 | learning_rate: 8.558124598587026e-05
Step 450 | epoch: 0.43352601156069365
Step 451 | loss: 3.777585506439209
Step 451 | grad_norm: 2.2061312198638916
Step 451 | learning_rate: 8.554913294797689e-05
Step 451 | epoch: 0.4344894026974952
Step 452 | loss: 3.458345651626587
Step 452 | grad_norm: 2.3985469341278076
Step 452 | learning_rate: 8.55170199100835e-05
Step 452 | epoch: 0.43545279383429675
Step 453 | loss: 3.626277446746826
Step 453 | grad_norm: 1.9804227352142334
Step 453 | learning_rate: 8.548490687219012e-05
Step 453 | epoch: 0.43641618497109824
Step 454 | loss: 3.259465217590332
Step 454 | grad_norm: 2.139916181564331
Step 454 | learning_rate: 8.545279383429673e-05
Step 454 | epoch: 0.4373795761078998
Step 455 | loss: 4.478196620941162
Step 455 | grad_norm: 2.230217456817627
Step 455 | learning_rate: 8.542068079640335e-05
Step 455 | epoch: 0.43834296724470134
Step 456 | loss: 3.1393377780914307
Step 456 | grad_norm: 2.207850933074951
Step 456 | learning_rate: 8.538856775850997e-05
Step 456 | epoch: 0.4393063583815029
Step 457 | loss: 4.015768051147461
Step 457 | grad_norm: 2.1282601356506348
Step 457 | learning_rate: 8.535645472061658e-05
Step 457 | epoch: 0.44026974951830444
Step 458 | loss: 3.086393356323242
Step 458 | grad_norm: 2.092862367630005
Step 458 | learning_rate: 8.532434168272318e-05
Step 458 | epoch: 0.441233140655106
Step 459 | loss: 3.454284906387329
Step 459 | grad_norm: 2.169603109359741
Step 459 | learning_rate: 8.52922286448298e-05
Step 459 | epoch: 0.4421965317919075
Step 460 | loss: 3.231045722961426
Step 460 | grad_norm: 2.1826961040496826
Step 460 | learning_rate: 8.526011560693643e-05
Step 460 | epoch: 0.44315992292870904
Step 461 | loss: 3.6478841304779053
Step 461 | grad_norm: 2.200826644897461
Step 461 | learning_rate: 8.522800256904303e-05
Step 461 | epoch: 0.4441233140655106
Step 462 | loss: 3.717472553253174
Step 462 | grad_norm: 2.1218924522399902
Step 462 | learning_rate: 8.519588953114965e-05
Step 462 | epoch: 0.44508670520231214
Step 463 | loss: 3.0323784351348877
Step 463 | grad_norm: 2.211217164993286
Step 463 | learning_rate: 8.516377649325626e-05
Step 463 | epoch: 0.4460500963391137
Step 464 | loss: 3.5761497020721436
Step 464 | grad_norm: 2.10127329826355
Step 464 | learning_rate: 8.513166345536288e-05
Step 464 | epoch: 0.44701348747591524
Step 465 | loss: 4.333013534545898
Step 465 | grad_norm: 2.0144708156585693
Step 465 | learning_rate: 8.50995504174695e-05
Step 465 | epoch: 0.4479768786127168
Step 466 | loss: 3.2299985885620117
Step 466 | grad_norm: 2.4653971195220947
Step 466 | learning_rate: 8.506743737957611e-05
Step 466 | epoch: 0.4489402697495183
Step 467 | loss: 3.2177934646606445
Step 467 | grad_norm: 2.595710515975952
Step 467 | learning_rate: 8.503532434168273e-05
Step 467 | epoch: 0.44990366088631983
Step 468 | loss: 4.255712032318115
Step 468 | grad_norm: 2.868525981903076
Step 468 | learning_rate: 8.500321130378934e-05
Step 468 | epoch: 0.4508670520231214
Step 469 | loss: 2.932426691055298
Step 469 | grad_norm: 1.8041681051254272
Step 469 | learning_rate: 8.497109826589596e-05
Step 469 | epoch: 0.45183044315992293
Step 470 | loss: 3.2403507232666016
Step 470 | grad_norm: 2.856400966644287
Step 470 | learning_rate: 8.493898522800258e-05
Step 470 | epoch: 0.4527938342967245
Step 471 | loss: 3.503655195236206
Step 471 | grad_norm: 2.4203531742095947
Step 471 | learning_rate: 8.490687219010919e-05
Step 471 | epoch: 0.45375722543352603
Step 472 | loss: 3.9362125396728516
Step 472 | grad_norm: 2.0910091400146484
Step 472 | learning_rate: 8.48747591522158e-05
Step 472 | epoch: 0.45472061657032753
Step 473 | loss: 4.094515323638916
Step 473 | grad_norm: 2.4323577880859375
Step 473 | learning_rate: 8.484264611432242e-05
Step 473 | epoch: 0.4556840077071291
Step 474 | loss: 3.685614585876465
Step 474 | grad_norm: 2.2347631454467773
Step 474 | learning_rate: 8.481053307642904e-05
Step 474 | epoch: 0.45664739884393063
Step 475 | loss: 3.8533847332000732
Step 475 | grad_norm: 2.469088077545166
Step 475 | learning_rate: 8.477842003853564e-05
Step 475 | epoch: 0.4576107899807322
Step 476 | loss: 2.871185064315796
Step 476 | grad_norm: 2.243060827255249
Step 476 | learning_rate: 8.474630700064227e-05
Step 476 | epoch: 0.45857418111753373
Step 477 | loss: 3.1894562244415283
Step 477 | grad_norm: 3.8581981658935547
Step 477 | learning_rate: 8.471419396274887e-05
Step 477 | epoch: 0.4595375722543353
Step 478 | loss: 3.1957802772521973
Step 478 | grad_norm: 2.1597976684570312
Step 478 | learning_rate: 8.46820809248555e-05
Step 478 | epoch: 0.4605009633911368
Step 479 | loss: 3.1187686920166016
Step 479 | grad_norm: 2.7297000885009766
Step 479 | learning_rate: 8.464996788696211e-05
Step 479 | epoch: 0.4614643545279383
Step 480 | loss: 3.4693820476531982
Step 480 | grad_norm: 2.300945281982422
Step 480 | learning_rate: 8.461785484906872e-05
Step 480 | epoch: 0.4624277456647399
Step 481 | loss: 3.8367371559143066
Step 481 | grad_norm: 2.4925272464752197
Step 481 | learning_rate: 8.458574181117534e-05
Step 481 | epoch: 0.4633911368015414
Step 482 | loss: 3.3966801166534424
Step 482 | grad_norm: 2.360100507736206
Step 482 | learning_rate: 8.455362877328195e-05
Step 482 | epoch: 0.464354527938343
Step 483 | loss: 3.8687820434570312
Step 483 | grad_norm: 2.176140785217285
Step 483 | learning_rate: 8.452151573538857e-05
Step 483 | epoch: 0.4653179190751445
Step 484 | loss: 3.750060796737671
Step 484 | grad_norm: 2.196437120437622
Step 484 | learning_rate: 8.448940269749519e-05
Step 484 | epoch: 0.4662813102119461
Step 485 | loss: 3.8610429763793945
Step 485 | grad_norm: 2.57267689704895
Step 485 | learning_rate: 8.44572896596018e-05
Step 485 | epoch: 0.46724470134874757
Step 486 | loss: 3.426175117492676
Step 486 | grad_norm: 2.7948763370513916
Step 486 | learning_rate: 8.442517662170841e-05
Step 486 | epoch: 0.4682080924855491
Step 487 | loss: 2.6024668216705322
Step 487 | grad_norm: 2.412893772125244
Step 487 | learning_rate: 8.439306358381503e-05
Step 487 | epoch: 0.46917148362235067
Step 488 | loss: 3.0697288513183594
Step 488 | grad_norm: 1.740028977394104
Step 488 | learning_rate: 8.436095054592165e-05
Step 488 | epoch: 0.4701348747591522
Step 489 | loss: 3.239603042602539
Step 489 | grad_norm: 2.5462610721588135
Step 489 | learning_rate: 8.432883750802827e-05
Step 489 | epoch: 0.47109826589595377
Step 490 | loss: 4.0057830810546875
Step 490 | grad_norm: 2.4207375049591064
Step 490 | learning_rate: 8.429672447013488e-05
Step 490 | epoch: 0.4720616570327553
Step 491 | loss: 3.8078396320343018
Step 491 | grad_norm: 2.1231908798217773
Step 491 | learning_rate: 8.426461143224149e-05
Step 491 | epoch: 0.4730250481695568
Step 492 | loss: 3.713068723678589
Step 492 | grad_norm: 2.3441717624664307
Step 492 | learning_rate: 8.423249839434812e-05
Step 492 | epoch: 0.47398843930635837
Step 493 | loss: 2.9174301624298096
Step 493 | grad_norm: 3.5550191402435303
Step 493 | learning_rate: 8.420038535645473e-05
Step 493 | epoch: 0.4749518304431599
Step 494 | loss: 3.468010187149048
Step 494 | grad_norm: 1.9067929983139038
Step 494 | learning_rate: 8.416827231856133e-05
Step 494 | epoch: 0.47591522157996147
Step 495 | loss: 3.280393123626709
Step 495 | grad_norm: 3.0249531269073486
Step 495 | learning_rate: 8.413615928066796e-05
Step 495 | epoch: 0.476878612716763
Step 496 | loss: 3.2030222415924072
Step 496 | grad_norm: 2.112658739089966
Step 496 | learning_rate: 8.410404624277458e-05
Step 496 | epoch: 0.47784200385356457
Step 497 | loss: 2.9723002910614014
Step 497 | grad_norm: 1.8890966176986694
Step 497 | learning_rate: 8.407193320488118e-05
Step 497 | epoch: 0.47880539499036606
Step 498 | loss: 4.949097633361816
Step 498 | grad_norm: 2.4243412017822266
Step 498 | learning_rate: 8.40398201669878e-05
Step 498 | epoch: 0.4797687861271676
Step 499 | loss: 4.040735244750977
Step 499 | grad_norm: 2.7731316089630127
Step 499 | learning_rate: 8.400770712909441e-05
Step 499 | epoch: 0.48073217726396916
Step 500 | loss: 3.828460216522217
Step 500 | grad_norm: 2.5801427364349365
Step 500 | learning_rate: 8.397559409120102e-05
Step 500 | epoch: 0.4816955684007707
Step 501 | loss: 3.420274019241333
Step 501 | grad_norm: 1.9104596376419067
Step 501 | learning_rate: 8.394348105330765e-05
Step 501 | epoch: 0.48265895953757226
Step 502 | loss: 3.188692808151245
Step 502 | grad_norm: 2.569195032119751
Step 502 | learning_rate: 8.391136801541426e-05
Step 502 | epoch: 0.4836223506743738
Step 503 | loss: 3.4233827590942383
Step 503 | grad_norm: 2.3832569122314453
Step 503 | learning_rate: 8.387925497752088e-05
Step 503 | epoch: 0.48458574181117536
Step 504 | loss: 3.375124216079712
Step 504 | grad_norm: 2.319406270980835
Step 504 | learning_rate: 8.384714193962749e-05
Step 504 | epoch: 0.48554913294797686
Step 505 | loss: 3.588564395904541
Step 505 | grad_norm: 2.8833391666412354
Step 505 | learning_rate: 8.381502890173411e-05
Step 505 | epoch: 0.4865125240847784
Step 506 | loss: 3.9471466541290283
Step 506 | grad_norm: 2.183825969696045
Step 506 | learning_rate: 8.378291586384073e-05
Step 506 | epoch: 0.48747591522157996
Step 507 | loss: 4.186059951782227
Step 507 | grad_norm: 2.242138385772705
Step 507 | learning_rate: 8.375080282594734e-05
Step 507 | epoch: 0.4884393063583815
Step 508 | loss: 3.2318403720855713
Step 508 | grad_norm: 2.7404367923736572
Step 508 | learning_rate: 8.371868978805395e-05
Step 508 | epoch: 0.48940269749518306
Step 509 | loss: 3.0403032302856445
Step 509 | grad_norm: 2.95354962348938
Step 509 | learning_rate: 8.368657675016057e-05
Step 509 | epoch: 0.4903660886319846
Step 510 | loss: 3.7989742755889893
Step 510 | grad_norm: 2.35872483253479
Step 510 | learning_rate: 8.365446371226719e-05
Step 510 | epoch: 0.4913294797687861
Step 511 | loss: 3.052366018295288
Step 511 | grad_norm: 2.334865093231201
Step 511 | learning_rate: 8.36223506743738e-05
Step 511 | epoch: 0.49229287090558765
Step 512 | loss: 3.5965664386749268
Step 512 | grad_norm: 1.9755266904830933
Step 512 | learning_rate: 8.359023763648042e-05
Step 512 | epoch: 0.4932562620423892
Step 513 | loss: 2.50016188621521
Step 513 | grad_norm: 1.7738252878189087
Step 513 | learning_rate: 8.355812459858702e-05
Step 513 | epoch: 0.49421965317919075
Step 514 | loss: 3.8043081760406494
Step 514 | grad_norm: 2.1921682357788086
Step 514 | learning_rate: 8.352601156069365e-05
Step 514 | epoch: 0.4951830443159923
Step 515 | loss: 3.1879162788391113
Step 515 | grad_norm: 2.1974239349365234
Step 515 | learning_rate: 8.349389852280027e-05
Step 515 | epoch: 0.49614643545279385
Step 516 | loss: 3.3892476558685303
Step 516 | grad_norm: 2.1347413063049316
Step 516 | learning_rate: 8.346178548490687e-05
Step 516 | epoch: 0.49710982658959535
Step 517 | loss: 3.2265326976776123
Step 517 | grad_norm: 2.4213154315948486
Step 517 | learning_rate: 8.34296724470135e-05
Step 517 | epoch: 0.4980732177263969
Step 518 | loss: 3.2766497135162354
Step 518 | grad_norm: 2.1097280979156494
Step 518 | learning_rate: 8.33975594091201e-05
Step 518 | epoch: 0.49903660886319845
Step 519 | loss: 3.5024795532226562
Step 519 | grad_norm: 1.9977641105651855
Step 519 | learning_rate: 8.336544637122672e-05
Step 519 | epoch: 0.5
Step 520 | loss: 4.071037292480469
Step 520 | grad_norm: 4.039975643157959
Step 520 | learning_rate: 8.333333333333334e-05
Step 520 | epoch: 0.5009633911368016
Step 521 | loss: 3.4076895713806152
Step 521 | grad_norm: 2.304398536682129
Step 521 | learning_rate: 8.330122029543995e-05
Step 521 | epoch: 0.5019267822736031
Step 522 | loss: 3.130722999572754
Step 522 | grad_norm: 1.792194128036499
Step 522 | learning_rate: 8.326910725754656e-05
Step 522 | epoch: 0.5028901734104047
Step 523 | loss: 3.974970579147339
Step 523 | grad_norm: 2.4106462001800537
Step 523 | learning_rate: 8.323699421965318e-05
Step 523 | epoch: 0.5038535645472062
Step 524 | loss: 3.4333114624023438
Step 524 | grad_norm: 2.3211264610290527
Step 524 | learning_rate: 8.32048811817598e-05
Step 524 | epoch: 0.5048169556840078
Step 525 | loss: 3.9551563262939453
Step 525 | grad_norm: 3.0646767616271973
Step 525 | learning_rate: 8.317276814386641e-05
Step 525 | epoch: 0.5057803468208093
Step 526 | loss: 3.4722914695739746
Step 526 | grad_norm: 2.5256595611572266
Step 526 | learning_rate: 8.314065510597303e-05
Step 526 | epoch: 0.5067437379576107
Step 527 | loss: 3.8165268898010254
Step 527 | grad_norm: 9.037434577941895
Step 527 | learning_rate: 8.310854206807964e-05
Step 527 | epoch: 0.5077071290944123
Step 528 | loss: 4.686666488647461
Step 528 | grad_norm: 2.9145379066467285
Step 528 | learning_rate: 8.307642903018627e-05
Step 528 | epoch: 0.5086705202312138
Step 529 | loss: 4.0485358238220215
Step 529 | grad_norm: 2.057394027709961
Step 529 | learning_rate: 8.304431599229288e-05
Step 529 | epoch: 0.5096339113680154
Step 530 | loss: 3.323032855987549
Step 530 | grad_norm: 2.289515256881714
Step 530 | learning_rate: 8.301220295439949e-05
Step 530 | epoch: 0.5105973025048169
Step 531 | loss: 3.210651397705078
Step 531 | grad_norm: 2.589961528778076
Step 531 | learning_rate: 8.298008991650611e-05
Step 531 | epoch: 0.5115606936416185
Step 532 | loss: 2.881105899810791
Step 532 | grad_norm: 2.16111159324646
Step 532 | learning_rate: 8.294797687861271e-05
Step 532 | epoch: 0.51252408477842
Step 533 | loss: 3.362698793411255
Step 533 | grad_norm: 3.2323598861694336
Step 533 | learning_rate: 8.291586384071934e-05
Step 533 | epoch: 0.5134874759152216
Step 534 | loss: 3.372746229171753
Step 534 | grad_norm: 1.7460891008377075
Step 534 | learning_rate: 8.288375080282596e-05
Step 534 | epoch: 0.5144508670520231
Step 535 | loss: 2.924747943878174
Step 535 | grad_norm: 2.154047727584839
Step 535 | learning_rate: 8.285163776493256e-05
Step 535 | epoch: 0.5154142581888247
Step 536 | loss: 4.047191143035889
Step 536 | grad_norm: 2.943044424057007
Step 536 | learning_rate: 8.281952472703917e-05
Step 536 | epoch: 0.5163776493256262
Step 537 | loss: 2.8377645015716553
Step 537 | grad_norm: 1.8829320669174194
Step 537 | learning_rate: 8.27874116891458e-05
Step 537 | epoch: 0.5173410404624278
Step 538 | loss: 3.3261139392852783
Step 538 | grad_norm: 2.66752290725708
Step 538 | learning_rate: 8.275529865125241e-05
Step 538 | epoch: 0.5183044315992292
Step 539 | loss: 3.737626552581787
Step 539 | grad_norm: 2.2349565029144287
Step 539 | learning_rate: 8.272318561335903e-05
Step 539 | epoch: 0.5192678227360308
Step 540 | loss: 3.975857973098755
Step 540 | grad_norm: 2.1602535247802734
Step 540 | learning_rate: 8.269107257546564e-05
Step 540 | epoch: 0.5202312138728323
Step 541 | loss: 2.820262908935547
Step 541 | grad_norm: 1.647567629814148
Step 541 | learning_rate: 8.265895953757226e-05
Step 541 | epoch: 0.5211946050096339
Step 542 | loss: 4.2148051261901855
Step 542 | grad_norm: 2.118917465209961
Step 542 | learning_rate: 8.262684649967888e-05
Step 542 | epoch: 0.5221579961464354
Step 543 | loss: 3.532097578048706
Step 543 | grad_norm: 2.0177884101867676
Step 543 | learning_rate: 8.259473346178549e-05
Step 543 | epoch: 0.523121387283237
Step 544 | loss: 3.3872833251953125
Step 544 | grad_norm: 3.027728796005249
Step 544 | learning_rate: 8.25626204238921e-05
Step 544 | epoch: 0.5240847784200385
Step 545 | loss: 2.7268965244293213
Step 545 | grad_norm: 2.239053964614868
Step 545 | learning_rate: 8.253050738599872e-05
Step 545 | epoch: 0.5250481695568401
Step 546 | loss: 3.834559202194214
Step 546 | grad_norm: 2.532334566116333
Step 546 | learning_rate: 8.249839434810534e-05
Step 546 | epoch: 0.5260115606936416
Step 547 | loss: 4.399279594421387
Step 547 | grad_norm: 2.953808307647705
Step 547 | learning_rate: 8.246628131021195e-05
Step 547 | epoch: 0.5269749518304432
Step 548 | loss: 3.524696111679077
Step 548 | grad_norm: 2.2691831588745117
Step 548 | learning_rate: 8.243416827231857e-05
Step 548 | epoch: 0.5279383429672447
Step 549 | loss: 2.5257532596588135
Step 549 | grad_norm: 1.7987964153289795
Step 549 | learning_rate: 8.240205523442518e-05
Step 549 | epoch: 0.5289017341040463
Step 550 | loss: 3.5626113414764404
Step 550 | grad_norm: 2.654752731323242
Step 550 | learning_rate: 8.23699421965318e-05
Step 550 | epoch: 0.5298651252408478
Step 551 | loss: 2.9719626903533936
Step 551 | grad_norm: 2.1584250926971436
Step 551 | learning_rate: 8.233782915863842e-05
Step 551 | epoch: 0.5308285163776493
Step 552 | loss: 3.9693684577941895
Step 552 | grad_norm: 2.1815905570983887
Step 552 | learning_rate: 8.230571612074503e-05
Step 552 | epoch: 0.5317919075144508
Step 553 | loss: 3.0816104412078857
Step 553 | grad_norm: 2.0382907390594482
Step 553 | learning_rate: 8.227360308285165e-05
Step 553 | epoch: 0.5327552986512524
Step 554 | loss: 3.2543747425079346
Step 554 | grad_norm: 2.05430269241333
Step 554 | learning_rate: 8.224149004495825e-05
Step 554 | epoch: 0.5337186897880539
Step 555 | loss: 3.647563934326172
Step 555 | grad_norm: 2.4768733978271484
Step 555 | learning_rate: 8.220937700706487e-05
Step 555 | epoch: 0.5346820809248555
Step 556 | loss: 3.7228195667266846
Step 556 | grad_norm: 2.3578364849090576
Step 556 | learning_rate: 8.21772639691715e-05
Step 556 | epoch: 0.535645472061657
Step 557 | loss: 3.1943788528442383
Step 557 | grad_norm: 2.037264108657837
Step 557 | learning_rate: 8.21451509312781e-05
Step 557 | epoch: 0.5366088631984586
Step 558 | loss: 3.3024823665618896
Step 558 | grad_norm: 1.8296258449554443
Step 558 | learning_rate: 8.211303789338471e-05
Step 558 | epoch: 0.5375722543352601
Step 559 | loss: 3.275202989578247
Step 559 | grad_norm: 2.1679575443267822
Step 559 | learning_rate: 8.208092485549133e-05
Step 559 | epoch: 0.5385356454720617
Step 560 | loss: 3.8786232471466064
Step 560 | grad_norm: 2.91050386428833
Step 560 | learning_rate: 8.204881181759795e-05
Step 560 | epoch: 0.5394990366088632
Step 561 | loss: 3.6873624324798584
Step 561 | grad_norm: 2.8815014362335205
Step 561 | learning_rate: 8.201669877970456e-05
Step 561 | epoch: 0.5404624277456648
Step 562 | loss: 3.8284614086151123
Step 562 | grad_norm: 2.1025257110595703
Step 562 | learning_rate: 8.198458574181118e-05
Step 562 | epoch: 0.5414258188824663
Step 563 | loss: 3.93527889251709
Step 563 | grad_norm: 2.711202383041382
Step 563 | learning_rate: 8.195247270391779e-05
Step 563 | epoch: 0.5423892100192679
Step 564 | loss: 3.399090528488159
Step 564 | grad_norm: 1.9139708280563354
Step 564 | learning_rate: 8.192035966602441e-05
Step 564 | epoch: 0.5433526011560693
Step 565 | loss: 2.949284076690674
Step 565 | grad_norm: 1.953359603881836
Step 565 | learning_rate: 8.188824662813103e-05
Step 565 | epoch: 0.5443159922928709
Step 566 | loss: 2.903066873550415
Step 566 | grad_norm: 2.455996036529541
Step 566 | learning_rate: 8.185613359023764e-05
Step 566 | epoch: 0.5452793834296724
Step 567 | loss: 3.1676111221313477
Step 567 | grad_norm: 2.2594010829925537
Step 567 | learning_rate: 8.182402055234426e-05
Step 567 | epoch: 0.546242774566474
Step 568 | loss: 4.221534729003906
Step 568 | grad_norm: 2.7284038066864014
Step 568 | learning_rate: 8.179190751445087e-05
Step 568 | epoch: 0.5472061657032755
Step 569 | loss: 4.029384613037109
Step 569 | grad_norm: 2.515059232711792
Step 569 | learning_rate: 8.175979447655749e-05
Step 569 | epoch: 0.5481695568400771
Step 570 | loss: 2.6404857635498047
Step 570 | grad_norm: 2.4418861865997314
Step 570 | learning_rate: 8.172768143866411e-05
Step 570 | epoch: 0.5491329479768786
Step 571 | loss: 3.1192126274108887
Step 571 | grad_norm: 2.355642557144165
Step 571 | learning_rate: 8.169556840077072e-05
Step 571 | epoch: 0.5500963391136802
Step 572 | loss: 3.15633487701416
Step 572 | grad_norm: 1.8627511262893677
Step 572 | learning_rate: 8.166345536287732e-05
Step 572 | epoch: 0.5510597302504817
Step 573 | loss: 3.423152446746826
Step 573 | grad_norm: 2.249501943588257
Step 573 | learning_rate: 8.163134232498396e-05
Step 573 | epoch: 0.5520231213872833
Step 574 | loss: 3.1341392993927
Step 574 | grad_norm: 1.9095598459243774
Step 574 | learning_rate: 8.159922928709056e-05
Step 574 | epoch: 0.5529865125240848
Step 575 | loss: 4.068305969238281
Step 575 | grad_norm: 3.3545734882354736
Step 575 | learning_rate: 8.156711624919717e-05
Step 575 | epoch: 0.5539499036608864
Step 576 | loss: 3.7702386379241943
Step 576 | grad_norm: 1.8325001001358032
Step 576 | learning_rate: 8.153500321130379e-05
Step 576 | epoch: 0.5549132947976878
Step 577 | loss: 2.979123592376709
Step 577 | grad_norm: 2.097080707550049
Step 577 | learning_rate: 8.15028901734104e-05
Step 577 | epoch: 0.5558766859344894
Step 578 | loss: 4.747774600982666
Step 578 | grad_norm: 4.022038459777832
Step 578 | learning_rate: 8.147077713551703e-05
Step 578 | epoch: 0.5568400770712909
Step 579 | loss: 3.663562059402466
Step 579 | grad_norm: 5.740058898925781
Step 579 | learning_rate: 8.143866409762364e-05
Step 579 | epoch: 0.5578034682080925
Step 580 | loss: 3.4996867179870605
Step 580 | grad_norm: 2.2283875942230225
Step 580 | learning_rate: 8.140655105973025e-05
Step 580 | epoch: 0.558766859344894
Step 581 | loss: 3.392239570617676
Step 581 | grad_norm: 1.8020524978637695
Step 581 | learning_rate: 8.137443802183687e-05
Step 581 | epoch: 0.5597302504816956
Step 582 | loss: 3.0478732585906982
Step 582 | grad_norm: 1.943964958190918
Step 582 | learning_rate: 8.134232498394349e-05
Step 582 | epoch: 0.5606936416184971
Step 583 | loss: 3.8272855281829834
Step 583 | grad_norm: 2.9798030853271484
Step 583 | learning_rate: 8.13102119460501e-05
Step 583 | epoch: 0.5616570327552987
Step 584 | loss: 2.7752346992492676
Step 584 | grad_norm: 1.9930187463760376
Step 584 | learning_rate: 8.127809890815672e-05
Step 584 | epoch: 0.5626204238921002
Step 585 | loss: 2.8714661598205566
Step 585 | grad_norm: 2.0303637981414795
Step 585 | learning_rate: 8.124598587026333e-05
Step 585 | epoch: 0.5635838150289018
Step 586 | loss: 3.541835308074951
Step 586 | grad_norm: 2.459324836730957
Step 586 | learning_rate: 8.121387283236995e-05
Step 586 | epoch: 0.5645472061657033
Step 587 | loss: 3.249250888824463
Step 587 | grad_norm: 1.992752194404602
Step 587 | learning_rate: 8.118175979447657e-05
Step 587 | epoch: 0.5655105973025049
Step 588 | loss: 2.86605167388916
Step 588 | grad_norm: 2.4595720767974854
Step 588 | learning_rate: 8.114964675658318e-05
Step 588 | epoch: 0.5664739884393064
Step 589 | loss: 3.035179853439331
Step 589 | grad_norm: 2.2466001510620117
Step 589 | learning_rate: 8.111753371868978e-05
Step 589 | epoch: 0.5674373795761078
Step 590 | loss: 3.346266269683838
Step 590 | grad_norm: 2.0239241123199463
Step 590 | learning_rate: 8.10854206807964e-05
Step 590 | epoch: 0.5684007707129094
Step 591 | loss: 2.819758176803589
Step 591 | grad_norm: 2.4374237060546875
Step 591 | learning_rate: 8.105330764290303e-05
Step 591 | epoch: 0.569364161849711
Step 592 | loss: 3.145493984222412
Step 592 | grad_norm: 2.004068374633789
Step 592 | learning_rate: 8.102119460500965e-05
Step 592 | epoch: 0.5703275529865125
Step 593 | loss: 3.4914026260375977
Step 593 | grad_norm: 2.45993971824646
Step 593 | learning_rate: 8.098908156711625e-05
Step 593 | epoch: 0.571290944123314
Step 594 | loss: 3.5777478218078613
Step 594 | grad_norm: 2.00038480758667
Step 594 | learning_rate: 8.095696852922286e-05
Step 594 | epoch: 0.5722543352601156
Step 595 | loss: 3.065295934677124
Step 595 | grad_norm: 2.0649755001068115
Step 595 | learning_rate: 8.092485549132948e-05
Step 595 | epoch: 0.5732177263969171
Step 596 | loss: 3.118366003036499
Step 596 | grad_norm: 2.2414181232452393
Step 596 | learning_rate: 8.08927424534361e-05
Step 596 | epoch: 0.5741811175337187
Step 597 | loss: 2.7965474128723145
Step 597 | grad_norm: 2.0278074741363525
Step 597 | learning_rate: 8.086062941554271e-05
Step 597 | epoch: 0.5751445086705202
Step 598 | loss: 3.678910732269287
Step 598 | grad_norm: 2.6040854454040527
Step 598 | learning_rate: 8.082851637764933e-05
Step 598 | epoch: 0.5761078998073218
Step 599 | loss: 3.23287296295166
Step 599 | grad_norm: 2.2860333919525146
Step 599 | learning_rate: 8.079640333975594e-05
Step 599 | epoch: 0.5770712909441233
Step 600 | loss: 3.3922367095947266
Step 600 | grad_norm: 2.1037869453430176
Step 600 | learning_rate: 8.076429030186256e-05
Step 600 | epoch: 0.5780346820809249
Step 601 | loss: 3.7569141387939453
Step 601 | grad_norm: 2.838366746902466
Step 601 | learning_rate: 8.073217726396918e-05
Step 601 | epoch: 0.5789980732177264
Step 602 | loss: 3.3646607398986816
Step 602 | grad_norm: 1.8474496603012085
Step 602 | learning_rate: 8.070006422607579e-05
Step 602 | epoch: 0.5799614643545279
Step 603 | loss: 2.866569757461548
Step 603 | grad_norm: 1.882157325744629
Step 603 | learning_rate: 8.066795118818241e-05
Step 603 | epoch: 0.5809248554913294
Step 604 | loss: 2.7847015857696533
Step 604 | grad_norm: 2.150162935256958
Step 604 | learning_rate: 8.063583815028902e-05
Step 604 | epoch: 0.581888246628131
Step 605 | loss: 4.280918598175049
Step 605 | grad_norm: 2.583789110183716
Step 605 | learning_rate: 8.060372511239564e-05
Step 605 | epoch: 0.5828516377649325
Step 606 | loss: 2.2374792098999023
Step 606 | grad_norm: 1.567591905593872
Step 606 | learning_rate: 8.057161207450226e-05
Step 606 | epoch: 0.5838150289017341
Step 607 | loss: 3.742938756942749
Step 607 | grad_norm: 2.396700859069824
Step 607 | learning_rate: 8.053949903660887e-05
Step 607 | epoch: 0.5847784200385356
Step 608 | loss: 3.0929453372955322
Step 608 | grad_norm: 2.0296080112457275
Step 608 | learning_rate: 8.050738599871547e-05
Step 608 | epoch: 0.5857418111753372
Step 609 | loss: 2.9993317127227783
Step 609 | grad_norm: 1.9340380430221558
Step 609 | learning_rate: 8.04752729608221e-05
Step 609 | epoch: 0.5867052023121387
Step 610 | loss: 3.448531150817871
Step 610 | grad_norm: 2.2116129398345947
Step 610 | learning_rate: 8.044315992292872e-05
Step 610 | epoch: 0.5876685934489403
Step 611 | loss: 3.1137516498565674
Step 611 | grad_norm: 2.418184995651245
Step 611 | learning_rate: 8.041104688503532e-05
Step 611 | epoch: 0.5886319845857418
Step 612 | loss: 3.7902722358703613
Step 612 | grad_norm: 3.0576276779174805
Step 612 | learning_rate: 8.037893384714194e-05
Step 612 | epoch: 0.5895953757225434
Step 613 | loss: 3.3238472938537598
Step 613 | grad_norm: 2.5126070976257324
Step 613 | learning_rate: 8.034682080924855e-05
Step 613 | epoch: 0.5905587668593449
Step 614 | loss: 2.8636224269866943
Step 614 | grad_norm: 1.8902734518051147
Step 614 | learning_rate: 8.031470777135517e-05
Step 614 | epoch: 0.5915221579961464
Step 615 | loss: 4.044913291931152
Step 615 | grad_norm: 2.945122480392456
Step 615 | learning_rate: 8.02825947334618e-05
Step 615 | epoch: 0.5924855491329479
Step 616 | loss: 3.282010316848755
Step 616 | grad_norm: 2.459792375564575
Step 616 | learning_rate: 8.02504816955684e-05
Step 616 | epoch: 0.5934489402697495
Step 617 | loss: 3.7033770084381104
Step 617 | grad_norm: 2.120757818222046
Step 617 | learning_rate: 8.021836865767502e-05
Step 617 | epoch: 0.594412331406551
Step 618 | loss: 3.826961040496826
Step 618 | grad_norm: 2.3087284564971924
Step 618 | learning_rate: 8.018625561978164e-05
Step 618 | epoch: 0.5953757225433526
Step 619 | loss: 3.565539598464966
Step 619 | grad_norm: 2.819828987121582
Step 619 | learning_rate: 8.015414258188825e-05
Step 619 | epoch: 0.5963391136801541
Step 620 | loss: 2.606213092803955
Step 620 | grad_norm: 2.6209163665771484
Step 620 | learning_rate: 8.012202954399487e-05
Step 620 | epoch: 0.5973025048169557
Step 621 | loss: 3.613156318664551
Step 621 | grad_norm: 2.1196823120117188
Step 621 | learning_rate: 8.008991650610148e-05
Step 621 | epoch: 0.5982658959537572
Step 622 | loss: 3.5718448162078857
Step 622 | grad_norm: 1.8247053623199463
Step 622 | learning_rate: 8.005780346820809e-05
Step 622 | epoch: 0.5992292870905588
Step 623 | loss: 4.342235565185547
Step 623 | grad_norm: 2.6362500190734863
Step 623 | learning_rate: 8.002569043031472e-05
Step 623 | epoch: 0.6001926782273603
Step 624 | loss: 3.2587273120880127
Step 624 | grad_norm: 2.2315540313720703
Step 624 | learning_rate: 7.999357739242133e-05
Step 624 | epoch: 0.6011560693641619
Step 625 | loss: 3.3982274532318115
Step 625 | grad_norm: 2.335998058319092
Step 625 | learning_rate: 7.996146435452794e-05
Step 625 | epoch: 0.6021194605009634
Step 626 | loss: 3.125452995300293
Step 626 | grad_norm: 2.5149428844451904
Step 626 | learning_rate: 7.992935131663456e-05
Step 626 | epoch: 0.603082851637765
Step 627 | loss: 3.326432228088379
Step 627 | grad_norm: 2.196650266647339
Step 627 | learning_rate: 7.989723827874118e-05
Step 627 | epoch: 0.6040462427745664
Step 628 | loss: 3.7641263008117676
Step 628 | grad_norm: 2.580660343170166
Step 628 | learning_rate: 7.98651252408478e-05
Step 628 | epoch: 0.605009633911368
Step 629 | loss: 3.2737503051757812
Step 629 | grad_norm: 2.8936500549316406
Step 629 | learning_rate: 7.98330122029544e-05
Step 629 | epoch: 0.6059730250481695
Step 630 | loss: 3.5570850372314453
Step 630 | grad_norm: 2.195131540298462
Step 630 | learning_rate: 7.980089916506101e-05
Step 630 | epoch: 0.6069364161849711
Step 631 | loss: 3.3752458095550537
Step 631 | grad_norm: 2.4966540336608887
Step 631 | learning_rate: 7.976878612716763e-05
Step 631 | epoch: 0.6078998073217726
Step 632 | loss: 3.4038467407226562
Step 632 | grad_norm: 2.310009717941284
Step 632 | learning_rate: 7.973667308927426e-05
Step 632 | epoch: 0.6088631984585742
Step 633 | loss: 3.419356107711792
Step 633 | grad_norm: 2.6313304901123047
Step 633 | learning_rate: 7.970456005138086e-05
Step 633 | epoch: 0.6098265895953757
Step 634 | loss: 3.396179437637329
Step 634 | grad_norm: 2.357125997543335
Step 634 | learning_rate: 7.967244701348748e-05
Step 634 | epoch: 0.6107899807321773
Step 635 | loss: 3.0953972339630127
Step 635 | grad_norm: 2.2720236778259277
Step 635 | learning_rate: 7.964033397559409e-05
Step 635 | epoch: 0.6117533718689788
Step 636 | loss: 3.277975559234619
Step 636 | grad_norm: 2.865204095840454
Step 636 | learning_rate: 7.960822093770071e-05
Step 636 | epoch: 0.6127167630057804
Step 637 | loss: 3.960911750793457
Step 637 | grad_norm: 2.4921298027038574
Step 637 | learning_rate: 7.957610789980733e-05
Step 637 | epoch: 0.6136801541425819
Step 638 | loss: 3.0569279193878174
Step 638 | grad_norm: 2.8880832195281982
Step 638 | learning_rate: 7.954399486191394e-05
Step 638 | epoch: 0.6146435452793835
Step 639 | loss: 3.3754727840423584
Step 639 | grad_norm: 2.446963310241699
Step 639 | learning_rate: 7.951188182402055e-05
Step 639 | epoch: 0.615606936416185
Step 640 | loss: 3.326666831970215
Step 640 | grad_norm: 2.625434398651123
Step 640 | learning_rate: 7.947976878612717e-05
Step 640 | epoch: 0.6165703275529865
Step 641 | loss: 3.399034023284912
Step 641 | grad_norm: 1.8290787935256958
Step 641 | learning_rate: 7.944765574823379e-05
Step 641 | epoch: 0.617533718689788
Step 642 | loss: 3.1618947982788086
Step 642 | grad_norm: 2.4120829105377197
Step 642 | learning_rate: 7.941554271034041e-05
Step 642 | epoch: 0.6184971098265896
Step 643 | loss: 3.539965867996216
Step 643 | grad_norm: 2.4629130363464355
Step 643 | learning_rate: 7.938342967244702e-05
Step 643 | epoch: 0.6194605009633911
Step 644 | loss: 3.57584285736084
Step 644 | grad_norm: 2.2458293437957764
Step 644 | learning_rate: 7.935131663455363e-05
Step 644 | epoch: 0.6204238921001927
Step 645 | loss: 3.9112207889556885
Step 645 | grad_norm: 2.0114285945892334
Step 645 | learning_rate: 7.931920359666025e-05
Step 645 | epoch: 0.6213872832369942
Step 646 | loss: 3.1143555641174316
Step 646 | grad_norm: 2.2500391006469727
Step 646 | learning_rate: 7.928709055876687e-05
Step 646 | epoch: 0.6223506743737958
Step 647 | loss: 2.7884750366210938
Step 647 | grad_norm: 2.123943567276001
Step 647 | learning_rate: 7.925497752087347e-05
Step 647 | epoch: 0.6233140655105973
Step 648 | loss: 4.5594000816345215
Step 648 | grad_norm: 2.458266258239746
Step 648 | learning_rate: 7.92228644829801e-05
Step 648 | epoch: 0.6242774566473989
Step 649 | loss: 3.6409692764282227
Step 649 | grad_norm: 2.770806074142456
Step 649 | learning_rate: 7.91907514450867e-05
Step 649 | epoch: 0.6252408477842004
Step 650 | loss: 3.281264543533325
Step 650 | grad_norm: 2.7995247840881348
Step 650 | learning_rate: 7.915863840719332e-05
Step 650 | epoch: 0.626204238921002
Step 651 | loss: 2.91321063041687
Step 651 | grad_norm: 1.9373670816421509
Step 651 | learning_rate: 7.912652536929994e-05
Step 651 | epoch: 0.6271676300578035
Step 652 | loss: 4.407800674438477
Step 652 | grad_norm: 2.978421211242676
Step 652 | learning_rate: 7.909441233140655e-05
Step 652 | epoch: 0.628131021194605
Step 653 | loss: 3.551470994949341
Step 653 | grad_norm: 2.0623459815979004
Step 653 | learning_rate: 7.906229929351317e-05
Step 653 | epoch: 0.6290944123314065
Step 654 | loss: 4.304990291595459
Step 654 | grad_norm: 2.8856449127197266
Step 654 | learning_rate: 7.903018625561978e-05
Step 654 | epoch: 0.630057803468208
Step 655 | loss: 4.391533851623535
Step 655 | grad_norm: 2.957430601119995
Step 655 | learning_rate: 7.89980732177264e-05
Step 655 | epoch: 0.6310211946050096
Step 656 | loss: 3.262800693511963
Step 656 | grad_norm: 1.9468339681625366
Step 656 | learning_rate: 7.896596017983302e-05
Step 656 | epoch: 0.6319845857418112
Step 657 | loss: 3.437495470046997
Step 657 | grad_norm: 2.595916271209717
Step 657 | learning_rate: 7.893384714193963e-05
Step 657 | epoch: 0.6329479768786127
Step 658 | loss: 3.47109317779541
Step 658 | grad_norm: 2.2734029293060303
Step 658 | learning_rate: 7.890173410404624e-05
Step 658 | epoch: 0.6339113680154143
Step 659 | loss: 2.5917768478393555
Step 659 | grad_norm: 2.6029295921325684
Step 659 | learning_rate: 7.886962106615287e-05
Step 659 | epoch: 0.6348747591522158
Step 660 | loss: 2.650559425354004
Step 660 | grad_norm: 4.861682415008545
Step 660 | learning_rate: 7.883750802825948e-05
Step 660 | epoch: 0.6358381502890174
Step 661 | loss: 2.8498475551605225
Step 661 | grad_norm: 2.4617412090301514
Step 661 | learning_rate: 7.880539499036609e-05
Step 661 | epoch: 0.6368015414258189
Step 662 | loss: 3.863210678100586
Step 662 | grad_norm: 2.791383743286133
Step 662 | learning_rate: 7.877328195247271e-05
Step 662 | epoch: 0.6377649325626205
Step 663 | loss: 3.020855188369751
Step 663 | grad_norm: 2.5651166439056396
Step 663 | learning_rate: 7.874116891457932e-05
Step 663 | epoch: 0.638728323699422
Step 664 | loss: 3.1652917861938477
Step 664 | grad_norm: 2.523050546646118
Step 664 | learning_rate: 7.870905587668594e-05
Step 664 | epoch: 0.6396917148362236
Step 665 | loss: 3.875746965408325
Step 665 | grad_norm: 2.035499095916748
Step 665 | learning_rate: 7.867694283879256e-05
Step 665 | epoch: 0.640655105973025
Step 666 | loss: 3.1600475311279297
Step 666 | grad_norm: 2.739408254623413
Step 666 | learning_rate: 7.864482980089916e-05
Step 666 | epoch: 0.6416184971098265
Step 667 | loss: 3.622922420501709
Step 667 | grad_norm: 4.578232288360596
Step 667 | learning_rate: 7.861271676300579e-05
Step 667 | epoch: 0.6425818882466281
Step 668 | loss: 3.6649396419525146
Step 668 | grad_norm: 2.686720132827759
Step 668 | learning_rate: 7.85806037251124e-05
Step 668 | epoch: 0.6435452793834296
Step 669 | loss: 3.708482265472412
Step 669 | grad_norm: 2.4283082485198975
Step 669 | learning_rate: 7.854849068721901e-05
Step 669 | epoch: 0.6445086705202312
Step 670 | loss: 2.9660863876342773
Step 670 | grad_norm: 2.7707622051239014
Step 670 | learning_rate: 7.851637764932563e-05
Step 670 | epoch: 0.6454720616570327
Step 671 | loss: 2.9536445140838623
Step 671 | grad_norm: 2.7598562240600586
Step 671 | learning_rate: 7.848426461143224e-05
Step 671 | epoch: 0.6464354527938343
Step 672 | loss: 3.5096518993377686
Step 672 | grad_norm: 3.4395828247070312
Step 672 | learning_rate: 7.845215157353886e-05
Step 672 | epoch: 0.6473988439306358
Step 673 | loss: 3.560555934906006
Step 673 | grad_norm: 2.3570008277893066
Step 673 | learning_rate: 7.842003853564548e-05
Step 673 | epoch: 0.6483622350674374
Step 674 | loss: 3.9773945808410645
Step 674 | grad_norm: 2.7979745864868164
Step 674 | learning_rate: 7.838792549775209e-05
Step 674 | epoch: 0.649325626204239
Step 675 | loss: 3.462739944458008
Step 675 | grad_norm: 2.1998796463012695
Step 675 | learning_rate: 7.83558124598587e-05
Step 675 | epoch: 0.6502890173410405
Step 676 | loss: 2.932389974594116
Step 676 | grad_norm: 2.4389498233795166
Step 676 | learning_rate: 7.832369942196532e-05
Step 676 | epoch: 0.651252408477842
Step 677 | loss: 3.5519304275512695
Step 677 | grad_norm: 3.0719668865203857
Step 677 | learning_rate: 7.829158638407194e-05
Step 677 | epoch: 0.6522157996146436
Step 678 | loss: 2.7943665981292725
Step 678 | grad_norm: 2.3542675971984863
Step 678 | learning_rate: 7.825947334617855e-05
Step 678 | epoch: 0.653179190751445
Step 679 | loss: 2.6487298011779785
Step 679 | grad_norm: 2.4575469493865967
Step 679 | learning_rate: 7.822736030828517e-05
Step 679 | epoch: 0.6541425818882466
Step 680 | loss: 3.1172711849212646
Step 680 | grad_norm: 2.1139423847198486
Step 680 | learning_rate: 7.819524727039178e-05
Step 680 | epoch: 0.6551059730250481
Step 681 | loss: 3.8113274574279785
Step 681 | grad_norm: 3.28564453125
Step 681 | learning_rate: 7.81631342324984e-05
Step 681 | epoch: 0.6560693641618497
Step 682 | loss: 3.7821044921875
Step 682 | grad_norm: 2.2050082683563232
Step 682 | learning_rate: 7.813102119460502e-05
Step 682 | epoch: 0.6570327552986512
Step 683 | loss: 2.8860042095184326
Step 683 | grad_norm: 2.1897106170654297
Step 683 | learning_rate: 7.809890815671163e-05
Step 683 | epoch: 0.6579961464354528
Step 684 | loss: 3.423158884048462
Step 684 | grad_norm: 2.670233726501465
Step 684 | learning_rate: 7.806679511881825e-05
Step 684 | epoch: 0.6589595375722543
Step 685 | loss: 3.3422627449035645
Step 685 | grad_norm: 2.787914276123047
Step 685 | learning_rate: 7.803468208092485e-05
Step 685 | epoch: 0.6599229287090559
Step 686 | loss: 3.445343017578125
Step 686 | grad_norm: 2.46751070022583
Step 686 | learning_rate: 7.800256904303148e-05
Step 686 | epoch: 0.6608863198458574
Step 687 | loss: 4.018755912780762
Step 687 | grad_norm: 2.574594020843506
Step 687 | learning_rate: 7.79704560051381e-05
Step 687 | epoch: 0.661849710982659
Step 688 | loss: 2.7280406951904297
Step 688 | grad_norm: 2.7035346031188965
Step 688 | learning_rate: 7.79383429672447e-05
Step 688 | epoch: 0.6628131021194605
Step 689 | loss: 3.6954586505889893
Step 689 | grad_norm: 3.71458101272583
Step 689 | learning_rate: 7.790622992935131e-05
Step 689 | epoch: 0.6637764932562621
Step 690 | loss: 3.2269110679626465
Step 690 | grad_norm: 2.891145944595337
Step 690 | learning_rate: 7.787411689145793e-05
Step 690 | epoch: 0.6647398843930635
Step 691 | loss: 3.456495761871338
Step 691 | grad_norm: 2.1090750694274902
Step 691 | learning_rate: 7.784200385356455e-05
Step 691 | epoch: 0.6657032755298651
Step 692 | loss: 3.729360580444336
Step 692 | grad_norm: 2.738049268722534
Step 692 | learning_rate: 7.780989081567117e-05
Step 692 | epoch: 0.6666666666666666
Step 693 | loss: 3.36955189704895
Step 693 | grad_norm: 2.5381672382354736
Step 693 | learning_rate: 7.777777777777778e-05
Step 693 | epoch: 0.6676300578034682
Step 694 | loss: 3.2380731105804443
Step 694 | grad_norm: 2.653531312942505
Step 694 | learning_rate: 7.774566473988439e-05
Step 694 | epoch: 0.6685934489402697
Step 695 | loss: 3.4866061210632324
Step 695 | grad_norm: 2.8746259212493896
Step 695 | learning_rate: 7.771355170199101e-05
Step 695 | epoch: 0.6695568400770713
Step 696 | loss: 4.288996696472168
Step 696 | grad_norm: 2.4206535816192627
Step 696 | learning_rate: 7.768143866409763e-05
Step 696 | epoch: 0.6705202312138728
Step 697 | loss: 3.6824986934661865
Step 697 | grad_norm: 2.75895357131958
Step 697 | learning_rate: 7.764932562620424e-05
Step 697 | epoch: 0.6714836223506744
Step 698 | loss: 3.5856306552886963
Step 698 | grad_norm: 2.2851457595825195
Step 698 | learning_rate: 7.761721258831086e-05
Step 698 | epoch: 0.6724470134874759
Step 699 | loss: 3.563037633895874
Step 699 | grad_norm: 2.640070676803589
Step 699 | learning_rate: 7.758509955041747e-05
Step 699 | epoch: 0.6734104046242775
Step 700 | loss: 4.078418731689453
Step 700 | grad_norm: 3.0856804847717285
Step 700 | learning_rate: 7.755298651252409e-05
Step 700 | epoch: 0.674373795761079
Step 701 | loss: 3.8540334701538086
Step 701 | grad_norm: 2.3882009983062744
Step 701 | learning_rate: 7.752087347463071e-05
Step 701 | epoch: 0.6753371868978806
Step 702 | loss: 2.7122642993927
Step 702 | grad_norm: 2.3508777618408203
Step 702 | learning_rate: 7.748876043673732e-05
Step 702 | epoch: 0.6763005780346821
Step 703 | loss: 2.469289541244507
Step 703 | grad_norm: 2.3832430839538574
Step 703 | learning_rate: 7.745664739884392e-05
Step 703 | epoch: 0.6772639691714836
Step 704 | loss: 3.535177707672119
Step 704 | grad_norm: 3.1248888969421387
Step 704 | learning_rate: 7.742453436095056e-05
Step 704 | epoch: 0.6782273603082851
Step 705 | loss: 2.599248170852661
Step 705 | grad_norm: 2.0764169692993164
Step 705 | learning_rate: 7.739242132305717e-05
Step 705 | epoch: 0.6791907514450867
Step 706 | loss: 3.2462007999420166
Step 706 | grad_norm: 2.148792028427124
Step 706 | learning_rate: 7.736030828516379e-05
Step 706 | epoch: 0.6801541425818882
Step 707 | loss: 3.6915571689605713
Step 707 | grad_norm: 2.497469186782837
Step 707 | learning_rate: 7.73281952472704e-05
Step 707 | epoch: 0.6811175337186898
Step 708 | loss: 3.5660159587860107
Step 708 | grad_norm: 2.549391269683838
Step 708 | learning_rate: 7.7296082209377e-05
Step 708 | epoch: 0.6820809248554913
Step 709 | loss: 3.2556469440460205
Step 709 | grad_norm: 2.894585132598877
Step 709 | learning_rate: 7.726396917148364e-05
Step 709 | epoch: 0.6830443159922929
Step 710 | loss: 3.1757006645202637
Step 710 | grad_norm: 2.3856470584869385
Step 710 | learning_rate: 7.723185613359024e-05
Step 710 | epoch: 0.6840077071290944
Step 711 | loss: 3.4702584743499756
Step 711 | grad_norm: 2.151062250137329
Step 711 | learning_rate: 7.719974309569685e-05
Step 711 | epoch: 0.684971098265896
Step 712 | loss: 4.354666709899902
Step 712 | grad_norm: 2.278331995010376
Step 712 | learning_rate: 7.716763005780347e-05
Step 712 | epoch: 0.6859344894026975
Step 713 | loss: 3.923504114151001
Step 713 | grad_norm: 2.2806131839752197
Step 713 | learning_rate: 7.713551701991009e-05
Step 713 | epoch: 0.6868978805394991
Step 714 | loss: 3.5124781131744385
Step 714 | grad_norm: 2.3237111568450928
Step 714 | learning_rate: 7.71034039820167e-05
Step 714 | epoch: 0.6878612716763006
Step 715 | loss: 2.9748177528381348
Step 715 | grad_norm: 1.9619303941726685
Step 715 | learning_rate: 7.707129094412332e-05
Step 715 | epoch: 0.6888246628131022
Step 716 | loss: 3.1358730792999268
Step 716 | grad_norm: 2.574420690536499
Step 716 | learning_rate: 7.703917790622993e-05
Step 716 | epoch: 0.6897880539499036
Step 717 | loss: 3.9286510944366455
Step 717 | grad_norm: 3.3541886806488037
Step 717 | learning_rate: 7.700706486833655e-05
Step 717 | epoch: 0.6907514450867052
Step 718 | loss: 2.7142322063446045
Step 718 | grad_norm: 2.1768596172332764
Step 718 | learning_rate: 7.697495183044317e-05
Step 718 | epoch: 0.6917148362235067
Step 719 | loss: 3.2843711376190186
Step 719 | grad_norm: 2.905625820159912
Step 719 | learning_rate: 7.694283879254978e-05
Step 719 | epoch: 0.6926782273603083
Step 720 | loss: 2.844325542449951
Step 720 | grad_norm: 3.6535003185272217
Step 720 | learning_rate: 7.69107257546564e-05
Step 720 | epoch: 0.6936416184971098
Step 721 | loss: 3.803123712539673
Step 721 | grad_norm: 3.0520336627960205
Step 721 | learning_rate: 7.6878612716763e-05
Step 721 | epoch: 0.6946050096339114
Step 722 | loss: 3.421011209487915
Step 722 | grad_norm: 2.2097079753875732
Step 722 | learning_rate: 7.684649967886963e-05
Step 722 | epoch: 0.6955684007707129
Step 723 | loss: 2.7216455936431885
Step 723 | grad_norm: 2.0602731704711914
Step 723 | learning_rate: 7.681438664097625e-05
Step 723 | epoch: 0.6965317919075145
Step 724 | loss: 3.6752896308898926
Step 724 | grad_norm: 2.8489487171173096
Step 724 | learning_rate: 7.678227360308286e-05
Step 724 | epoch: 0.697495183044316
Step 725 | loss: 3.2902562618255615
Step 725 | grad_norm: 2.94842529296875
Step 725 | learning_rate: 7.675016056518946e-05
Step 725 | epoch: 0.6984585741811176
Step 726 | loss: 3.2955093383789062
Step 726 | grad_norm: 1.865064024925232
Step 726 | learning_rate: 7.671804752729608e-05
Step 726 | epoch: 0.6994219653179191
Step 727 | loss: 4.015690803527832
Step 727 | grad_norm: 3.042788028717041
Step 727 | learning_rate: 7.66859344894027e-05
Step 727 | epoch: 0.7003853564547207
Step 728 | loss: 4.02041482925415
Step 728 | grad_norm: 3.991262912750244
Step 728 | learning_rate: 7.665382145150931e-05
Step 728 | epoch: 0.7013487475915221
Step 729 | loss: 2.0884690284729004
Step 729 | grad_norm: 2.224216938018799
Step 729 | learning_rate: 7.662170841361593e-05
Step 729 | epoch: 0.7023121387283237
Step 730 | loss: 3.081810235977173
Step 730 | grad_norm: 2.0816147327423096
Step 730 | learning_rate: 7.658959537572254e-05
Step 730 | epoch: 0.7032755298651252
Step 731 | loss: 3.86665940284729
Step 731 | grad_norm: 2.1711392402648926
Step 731 | learning_rate: 7.655748233782916e-05
Step 731 | epoch: 0.7042389210019268
Step 732 | loss: 3.252721071243286
Step 732 | grad_norm: 2.4465246200561523
Step 732 | learning_rate: 7.652536929993578e-05
Step 732 | epoch: 0.7052023121387283
Step 733 | loss: 2.9498090744018555
Step 733 | grad_norm: 2.1404926776885986
Step 733 | learning_rate: 7.649325626204239e-05
Step 733 | epoch: 0.7061657032755299
Step 734 | loss: 3.500462532043457
Step 734 | grad_norm: 2.0131964683532715
Step 734 | learning_rate: 7.646114322414901e-05
Step 734 | epoch: 0.7071290944123314
Step 735 | loss: 3.8842968940734863
Step 735 | grad_norm: 3.226602077484131
Step 735 | learning_rate: 7.642903018625562e-05
Step 735 | epoch: 0.708092485549133
Step 736 | loss: 3.4837005138397217
Step 736 | grad_norm: 2.488978624343872
Step 736 | learning_rate: 7.639691714836224e-05
Step 736 | epoch: 0.7090558766859345
Step 737 | loss: 3.147794485092163
Step 737 | grad_norm: 2.568281412124634
Step 737 | learning_rate: 7.636480411046886e-05
Step 737 | epoch: 0.710019267822736
Step 738 | loss: 2.629307508468628
Step 738 | grad_norm: 1.8785587549209595
Step 738 | learning_rate: 7.633269107257547e-05
Step 738 | epoch: 0.7109826589595376
Step 739 | loss: 2.968940496444702
Step 739 | grad_norm: 2.270328998565674
Step 739 | learning_rate: 7.630057803468207e-05
Step 739 | epoch: 0.7119460500963392
Step 740 | loss: 3.0857839584350586
Step 740 | grad_norm: 1.9646533727645874
Step 740 | learning_rate: 7.62684649967887e-05
Step 740 | epoch: 0.7129094412331407
Step 741 | loss: 3.74440598487854
Step 741 | grad_norm: 3.263221502304077
Step 741 | learning_rate: 7.623635195889532e-05
Step 741 | epoch: 0.7138728323699421
Step 742 | loss: 3.065133810043335
Step 742 | grad_norm: 2.0265557765960693
Step 742 | learning_rate: 7.620423892100194e-05
Step 742 | epoch: 0.7148362235067437
Step 743 | loss: 2.7759811878204346
Step 743 | grad_norm: 2.5987818241119385
Step 743 | learning_rate: 7.617212588310855e-05
Step 743 | epoch: 0.7157996146435452
Step 744 | loss: 2.935335397720337
Step 744 | grad_norm: 2.705702304840088
Step 744 | learning_rate: 7.614001284521515e-05
Step 744 | epoch: 0.7167630057803468
Step 745 | loss: 3.624424457550049
Step 745 | grad_norm: 3.5734851360321045
Step 745 | learning_rate: 7.610789980732179e-05
Step 745 | epoch: 0.7177263969171483
Step 746 | loss: 3.7036867141723633
Step 746 | grad_norm: 2.5237791538238525
Step 746 | learning_rate: 7.60757867694284e-05
Step 746 | epoch: 0.7186897880539499
Step 747 | loss: 3.7323336601257324
Step 747 | grad_norm: 2.7830963134765625
Step 747 | learning_rate: 7.6043673731535e-05
Step 747 | epoch: 0.7196531791907514
Step 748 | loss: 3.7565929889678955
Step 748 | grad_norm: 2.2861554622650146
Step 748 | learning_rate: 7.601156069364162e-05
Step 748 | epoch: 0.720616570327553
Step 749 | loss: 3.444664716720581
Step 749 | grad_norm: 2.3605854511260986
Step 749 | learning_rate: 7.597944765574824e-05
Step 749 | epoch: 0.7215799614643545
Step 750 | loss: 3.49090576171875
Step 750 | grad_norm: 2.4200501441955566
Step 750 | learning_rate: 7.594733461785485e-05
Step 750 | epoch: 0.7225433526011561
Step 751 | loss: 3.74053955078125
Step 751 | grad_norm: 2.752912759780884
Step 751 | learning_rate: 7.591522157996147e-05
Step 751 | epoch: 0.7235067437379576
Step 752 | loss: 3.4031221866607666
Step 752 | grad_norm: 2.126507043838501
Step 752 | learning_rate: 7.588310854206808e-05
Step 752 | epoch: 0.7244701348747592
Step 753 | loss: 2.8888278007507324
Step 753 | grad_norm: 2.7149362564086914
Step 753 | learning_rate: 7.585099550417469e-05
Step 753 | epoch: 0.7254335260115607
Step 754 | loss: 2.98016619682312
Step 754 | grad_norm: 2.381836414337158
Step 754 | learning_rate: 7.581888246628132e-05
Step 754 | epoch: 0.7263969171483622
Step 755 | loss: 4.3003129959106445
Step 755 | grad_norm: 3.2585792541503906
Step 755 | learning_rate: 7.578676942838793e-05
Step 755 | epoch: 0.7273603082851637
Step 756 | loss: 4.269313812255859
Step 756 | grad_norm: 2.270207405090332
Step 756 | learning_rate: 7.575465639049455e-05
Step 756 | epoch: 0.7283236994219653
Step 757 | loss: 3.977412462234497
Step 757 | grad_norm: 2.73437237739563
Step 757 | learning_rate: 7.572254335260116e-05
Step 757 | epoch: 0.7292870905587668
Step 758 | loss: 3.362739324569702
Step 758 | grad_norm: 2.670070171356201
Step 758 | learning_rate: 7.569043031470778e-05
Step 758 | epoch: 0.7302504816955684
Step 759 | loss: 2.842228412628174
Step 759 | grad_norm: 1.9478448629379272
Step 759 | learning_rate: 7.56583172768144e-05
Step 759 | epoch: 0.7312138728323699
Step 760 | loss: 2.8481011390686035
Step 760 | grad_norm: 1.7297264337539673
Step 760 | learning_rate: 7.5626204238921e-05
Step 760 | epoch: 0.7321772639691715
Step 761 | loss: 3.2418646812438965
Step 761 | grad_norm: 2.258899211883545
Step 761 | learning_rate: 7.559409120102761e-05
Step 761 | epoch: 0.733140655105973
Step 762 | loss: 3.5922181606292725
Step 762 | grad_norm: 2.2538530826568604
Step 762 | learning_rate: 7.556197816313423e-05
Step 762 | epoch: 0.7341040462427746
Step 763 | loss: 3.421409845352173
Step 763 | grad_norm: 2.1381211280822754
Step 763 | learning_rate: 7.552986512524086e-05
Step 763 | epoch: 0.7350674373795761
Step 764 | loss: 4.069689750671387
Step 764 | grad_norm: 2.1030640602111816
Step 764 | learning_rate: 7.549775208734746e-05
Step 764 | epoch: 0.7360308285163777
Step 765 | loss: 3.849500894546509
Step 765 | grad_norm: 2.334247350692749
Step 765 | learning_rate: 7.546563904945408e-05
Step 765 | epoch: 0.7369942196531792
Step 766 | loss: 3.856339454650879
Step 766 | grad_norm: 2.601814031600952
Step 766 | learning_rate: 7.543352601156069e-05
Step 766 | epoch: 0.7379576107899807
Step 767 | loss: 3.005568742752075
Step 767 | grad_norm: 2.407400608062744
Step 767 | learning_rate: 7.540141297366731e-05
Step 767 | epoch: 0.7389210019267822
Step 768 | loss: 2.9739673137664795
Step 768 | grad_norm: 2.3722786903381348
Step 768 | learning_rate: 7.536929993577393e-05
Step 768 | epoch: 0.7398843930635838
Step 769 | loss: 3.729438304901123
Step 769 | grad_norm: 1.9554589986801147
Step 769 | learning_rate: 7.533718689788054e-05
Step 769 | epoch: 0.7408477842003853
Step 770 | loss: 3.397960901260376
Step 770 | grad_norm: 2.5571699142456055
Step 770 | learning_rate: 7.530507385998716e-05
Step 770 | epoch: 0.7418111753371869
Step 771 | loss: 3.1968295574188232
Step 771 | grad_norm: 2.698538303375244
Step 771 | learning_rate: 7.527296082209377e-05
Step 771 | epoch: 0.7427745664739884
Step 772 | loss: 3.497248649597168
Step 772 | grad_norm: 2.816967010498047
Step 772 | learning_rate: 7.524084778420039e-05
Step 772 | epoch: 0.74373795761079
Step 773 | loss: 3.499776601791382
Step 773 | grad_norm: 2.4236643314361572
Step 773 | learning_rate: 7.520873474630701e-05
Step 773 | epoch: 0.7447013487475915
Step 774 | loss: 3.3078560829162598
Step 774 | grad_norm: 2.2798681259155273
Step 774 | learning_rate: 7.517662170841362e-05
Step 774 | epoch: 0.7456647398843931
Step 775 | loss: 4.195250511169434
Step 775 | grad_norm: 3.524775505065918
Step 775 | learning_rate: 7.514450867052023e-05
Step 775 | epoch: 0.7466281310211946
Step 776 | loss: 3.7854647636413574
Step 776 | grad_norm: 2.9561831951141357
Step 776 | learning_rate: 7.511239563262685e-05
Step 776 | epoch: 0.7475915221579962
Step 777 | loss: 3.1607089042663574
Step 777 | grad_norm: 2.612483501434326
Step 777 | learning_rate: 7.508028259473347e-05
Step 777 | epoch: 0.7485549132947977
Step 778 | loss: 3.318241834640503
Step 778 | grad_norm: 2.2020926475524902
Step 778 | learning_rate: 7.504816955684008e-05
Step 778 | epoch: 0.7495183044315993
Step 779 | loss: 3.0496113300323486
Step 779 | grad_norm: 2.8439457416534424
Step 779 | learning_rate: 7.50160565189467e-05
Step 779 | epoch: 0.7504816955684007
Step 780 | loss: 4.1564483642578125
Step 780 | grad_norm: 3.2744877338409424
Step 780 | learning_rate: 7.49839434810533e-05
Step 780 | epoch: 0.7514450867052023
Step 781 | loss: 4.131008148193359
Step 781 | grad_norm: 3.8726532459259033
Step 781 | learning_rate: 7.495183044315992e-05
Step 781 | epoch: 0.7524084778420038
Step 782 | loss: 3.803691864013672
Step 782 | grad_norm: 3.638761043548584
Step 782 | learning_rate: 7.491971740526655e-05
Step 782 | epoch: 0.7533718689788054
Step 783 | loss: 3.447233200073242
Step 783 | grad_norm: 2.4620938301086426
Step 783 | learning_rate: 7.488760436737315e-05
Step 783 | epoch: 0.7543352601156069
Step 784 | loss: 4.010500431060791
Step 784 | grad_norm: 2.556391954421997
Step 784 | learning_rate: 7.485549132947977e-05
Step 784 | epoch: 0.7552986512524085
Step 785 | loss: 3.3740363121032715
Step 785 | grad_norm: 2.3197479248046875
Step 785 | learning_rate: 7.482337829158638e-05
Step 785 | epoch: 0.75626204238921
Step 786 | loss: 2.6662096977233887
Step 786 | grad_norm: 2.6730222702026367
Step 786 | learning_rate: 7.4791265253693e-05
Step 786 | epoch: 0.7572254335260116
Step 787 | loss: 3.22361421585083
Step 787 | grad_norm: 2.986445188522339
Step 787 | learning_rate: 7.475915221579962e-05
Step 787 | epoch: 0.7581888246628131
Step 788 | loss: 3.9575536251068115
Step 788 | grad_norm: 2.3533873558044434
Step 788 | learning_rate: 7.472703917790623e-05
Step 788 | epoch: 0.7591522157996147
Step 789 | loss: 3.0232994556427
Step 789 | grad_norm: 1.7595564126968384
Step 789 | learning_rate: 7.469492614001284e-05
Step 789 | epoch: 0.7601156069364162
Step 790 | loss: 3.3523805141448975
Step 790 | grad_norm: 2.4823436737060547
Step 790 | learning_rate: 7.466281310211947e-05
Step 790 | epoch: 0.7610789980732178
Step 791 | loss: 3.2216274738311768
Step 791 | grad_norm: 2.4095804691314697
Step 791 | learning_rate: 7.463070006422608e-05
Step 791 | epoch: 0.7620423892100193
Step 792 | loss: 3.4490230083465576
Step 792 | grad_norm: 2.716973066329956
Step 792 | learning_rate: 7.459858702633269e-05
Step 792 | epoch: 0.7630057803468208
Step 793 | loss: 3.8147146701812744
Step 793 | grad_norm: 2.628648042678833
Step 793 | learning_rate: 7.456647398843931e-05
Step 793 | epoch: 0.7639691714836223
Step 794 | loss: 2.8894145488739014
Step 794 | grad_norm: 1.9589372873306274
Step 794 | learning_rate: 7.453436095054592e-05
Step 794 | epoch: 0.7649325626204239
Step 795 | loss: 2.8376717567443848
Step 795 | grad_norm: 2.337259292602539
Step 795 | learning_rate: 7.450224791265255e-05
Step 795 | epoch: 0.7658959537572254
Step 796 | loss: 2.766930341720581
Step 796 | grad_norm: 1.8637341260910034
Step 796 | learning_rate: 7.447013487475916e-05
Step 796 | epoch: 0.766859344894027
Step 797 | loss: 2.0475993156433105
Step 797 | grad_norm: 1.5346628427505493
Step 797 | learning_rate: 7.443802183686577e-05
Step 797 | epoch: 0.7678227360308285
Step 798 | loss: 2.9699459075927734
Step 798 | grad_norm: 1.9338709115982056
Step 798 | learning_rate: 7.440590879897239e-05
Step 798 | epoch: 0.7687861271676301
Step 799 | loss: 4.270510673522949
Step 799 | grad_norm: 2.9511570930480957
Step 799 | learning_rate: 7.437379576107901e-05
Step 799 | epoch: 0.7697495183044316
Step 800 | loss: 4.0068511962890625
Step 800 | grad_norm: 2.3632004261016846
Step 800 | learning_rate: 7.434168272318561e-05
Step 800 | epoch: 0.7707129094412332
Step 801 | loss: 3.4127676486968994
Step 801 | grad_norm: 2.463866949081421
Step 801 | learning_rate: 7.430956968529224e-05
Step 801 | epoch: 0.7716763005780347
Step 802 | loss: 3.3024168014526367
Step 802 | grad_norm: 2.5576534271240234
Step 802 | learning_rate: 7.427745664739884e-05
Step 802 | epoch: 0.7726396917148363
Step 803 | loss: 3.544250965118408
Step 803 | grad_norm: 3.2719833850860596
Step 803 | learning_rate: 7.424534360950546e-05
Step 803 | epoch: 0.7736030828516378
Step 804 | loss: 3.263211965560913
Step 804 | grad_norm: 2.067415237426758
Step 804 | learning_rate: 7.421323057161209e-05
Step 804 | epoch: 0.7745664739884393
Step 805 | loss: 4.105470657348633
Step 805 | grad_norm: 8.319518089294434
Step 805 | learning_rate: 7.418111753371869e-05
Step 805 | epoch: 0.7755298651252408
Step 806 | loss: 3.8601934909820557
Step 806 | grad_norm: 2.4481914043426514
Step 806 | learning_rate: 7.414900449582531e-05
Step 806 | epoch: 0.7764932562620424
Step 807 | loss: 3.14231538772583
Step 807 | grad_norm: 2.8207859992980957
Step 807 | learning_rate: 7.411689145793192e-05
Step 807 | epoch: 0.7774566473988439
Step 808 | loss: 2.818516254425049
Step 808 | grad_norm: 2.0452842712402344
Step 808 | learning_rate: 7.408477842003854e-05
Step 808 | epoch: 0.7784200385356455
Step 809 | loss: 3.994295120239258
Step 809 | grad_norm: 2.91105318069458
Step 809 | learning_rate: 7.405266538214516e-05
Step 809 | epoch: 0.779383429672447
Step 810 | loss: 2.9881491661071777
Step 810 | grad_norm: 2.1414804458618164
Step 810 | learning_rate: 7.402055234425177e-05
Step 810 | epoch: 0.7803468208092486
Step 811 | loss: 2.767775297164917
Step 811 | grad_norm: 2.116041660308838
Step 811 | learning_rate: 7.398843930635838e-05
Step 811 | epoch: 0.7813102119460501
Step 812 | loss: 3.102316379547119
Step 812 | grad_norm: 2.0579161643981934
Step 812 | learning_rate: 7.3956326268465e-05
Step 812 | epoch: 0.7822736030828517
Step 813 | loss: 3.261507034301758
Step 813 | grad_norm: 2.4987146854400635
Step 813 | learning_rate: 7.392421323057162e-05
Step 813 | epoch: 0.7832369942196532
Step 814 | loss: 3.169883966445923
Step 814 | grad_norm: 2.243485689163208
Step 814 | learning_rate: 7.389210019267823e-05
Step 814 | epoch: 0.7842003853564548
Step 815 | loss: 3.5482027530670166
Step 815 | grad_norm: 2.462859869003296
Step 815 | learning_rate: 7.385998715478485e-05
Step 815 | epoch: 0.7851637764932563
Step 816 | loss: 3.214348793029785
Step 816 | grad_norm: 2.0656800270080566
Step 816 | learning_rate: 7.382787411689146e-05
Step 816 | epoch: 0.7861271676300579
Step 817 | loss: 3.5865559577941895
Step 817 | grad_norm: 2.5279428958892822
Step 817 | learning_rate: 7.379576107899808e-05
Step 817 | epoch: 0.7870905587668593
Step 818 | loss: 3.5872750282287598
Step 818 | grad_norm: 2.5463781356811523
Step 818 | learning_rate: 7.37636480411047e-05
Step 818 | epoch: 0.7880539499036608
Step 819 | loss: 3.186067819595337
Step 819 | grad_norm: 2.243040084838867
Step 819 | learning_rate: 7.37315350032113e-05
Step 819 | epoch: 0.7890173410404624
Step 820 | loss: 3.2294199466705322
Step 820 | grad_norm: 2.8799195289611816
Step 820 | learning_rate: 7.369942196531793e-05
Step 820 | epoch: 0.789980732177264
Step 821 | loss: 3.6744277477264404
Step 821 | grad_norm: 1.9889214038848877
Step 821 | learning_rate: 7.366730892742453e-05
Step 821 | epoch: 0.7909441233140655
Step 822 | loss: 3.467264175415039
Step 822 | grad_norm: 2.826111078262329
Step 822 | learning_rate: 7.363519588953115e-05
Step 822 | epoch: 0.791907514450867
Step 823 | loss: 4.009341239929199
Step 823 | grad_norm: 2.081188917160034
Step 823 | learning_rate: 7.360308285163777e-05
Step 823 | epoch: 0.7928709055876686
Step 824 | loss: 3.5971486568450928
Step 824 | grad_norm: 2.0860280990600586
Step 824 | learning_rate: 7.357096981374438e-05
Step 824 | epoch: 0.7938342967244701
Step 825 | loss: 3.9597280025482178
Step 825 | grad_norm: 2.7990009784698486
Step 825 | learning_rate: 7.353885677585099e-05
Step 825 | epoch: 0.7947976878612717
Step 826 | loss: 2.843183994293213
Step 826 | grad_norm: 1.9960447549819946
Step 826 | learning_rate: 7.350674373795761e-05
Step 826 | epoch: 0.7957610789980732
Step 827 | loss: 3.2010087966918945
Step 827 | grad_norm: 2.7118024826049805
Step 827 | learning_rate: 7.347463070006423e-05
Step 827 | epoch: 0.7967244701348748
Step 828 | loss: 2.8395540714263916
Step 828 | grad_norm: 2.1872575283050537
Step 828 | learning_rate: 7.344251766217084e-05
Step 828 | epoch: 0.7976878612716763
Step 829 | loss: 3.0608224868774414
Step 829 | grad_norm: 2.2048850059509277
Step 829 | learning_rate: 7.341040462427746e-05
Step 829 | epoch: 0.7986512524084779
Step 830 | loss: 3.1454429626464844
Step 830 | grad_norm: 2.610130548477173
Step 830 | learning_rate: 7.337829158638407e-05
Step 830 | epoch: 0.7996146435452793
Step 831 | loss: 2.964799642562866
Step 831 | grad_norm: 2.118732452392578
Step 831 | learning_rate: 7.33461785484907e-05
Step 831 | epoch: 0.8005780346820809
Step 832 | loss: 3.3047571182250977
Step 832 | grad_norm: 2.2704458236694336
Step 832 | learning_rate: 7.331406551059731e-05
Step 832 | epoch: 0.8015414258188824
Step 833 | loss: 2.9865031242370605
Step 833 | grad_norm: 2.4436497688293457
Step 833 | learning_rate: 7.328195247270392e-05
Step 833 | epoch: 0.802504816955684
Step 834 | loss: 2.7851157188415527
Step 834 | grad_norm: 2.1693222522735596
Step 834 | learning_rate: 7.324983943481054e-05
Step 834 | epoch: 0.8034682080924855
Step 835 | loss: 2.7601020336151123
Step 835 | grad_norm: 2.147334098815918
Step 835 | learning_rate: 7.321772639691716e-05
Step 835 | epoch: 0.8044315992292871
Step 836 | loss: 2.9052767753601074
Step 836 | grad_norm: 2.770048141479492
Step 836 | learning_rate: 7.318561335902377e-05
Step 836 | epoch: 0.8053949903660886
Step 837 | loss: 3.628722906112671
Step 837 | grad_norm: 2.430422306060791
Step 837 | learning_rate: 7.315350032113039e-05
Step 837 | epoch: 0.8063583815028902
Step 838 | loss: 2.778646945953369
Step 838 | grad_norm: 3.523399829864502
Step 838 | learning_rate: 7.3121387283237e-05
Step 838 | epoch: 0.8073217726396917
Step 839 | loss: 3.9983208179473877
Step 839 | grad_norm: 3.2662038803100586
Step 839 | learning_rate: 7.30892742453436e-05
Step 839 | epoch: 0.8082851637764933
Step 840 | loss: 3.5377845764160156
Step 840 | grad_norm: 2.9515738487243652
Step 840 | learning_rate: 7.305716120745024e-05
Step 840 | epoch: 0.8092485549132948
Step 841 | loss: 2.9111688137054443
Step 841 | grad_norm: 2.8976094722747803
Step 841 | learning_rate: 7.302504816955684e-05
Step 841 | epoch: 0.8102119460500964
Step 842 | loss: 3.4159176349639893
Step 842 | grad_norm: 3.558627128601074
Step 842 | learning_rate: 7.299293513166345e-05
Step 842 | epoch: 0.8111753371868978
Step 843 | loss: 2.6355011463165283
Step 843 | grad_norm: 2.4508249759674072
Step 843 | learning_rate: 7.296082209377007e-05
Step 843 | epoch: 0.8121387283236994
Step 844 | loss: 3.6449830532073975
Step 844 | grad_norm: 2.967581033706665
Step 844 | learning_rate: 7.29287090558767e-05
Step 844 | epoch: 0.8131021194605009
Step 845 | loss: 2.5220463275909424
Step 845 | grad_norm: 2.449932813644409
Step 845 | learning_rate: 7.289659601798331e-05
Step 845 | epoch: 0.8140655105973025
Step 846 | loss: 2.685870885848999
Step 846 | grad_norm: 2.4030516147613525
Step 846 | learning_rate: 7.286448298008992e-05
Step 846 | epoch: 0.815028901734104
Step 847 | loss: 3.244070529937744
Step 847 | grad_norm: 2.656658172607422
Step 847 | learning_rate: 7.283236994219653e-05
Step 847 | epoch: 0.8159922928709056
Step 848 | loss: 3.1516122817993164
Step 848 | grad_norm: 2.1388189792633057
Step 848 | learning_rate: 7.280025690430315e-05
Step 848 | epoch: 0.8169556840077071
Step 849 | loss: 2.951211929321289
Step 849 | grad_norm: 2.5310003757476807
Step 849 | learning_rate: 7.276814386640977e-05
Step 849 | epoch: 0.8179190751445087
Step 850 | loss: 3.514751434326172
Step 850 | grad_norm: 2.6712746620178223
Step 850 | learning_rate: 7.273603082851638e-05
Step 850 | epoch: 0.8188824662813102
Step 851 | loss: 2.7422611713409424
Step 851 | grad_norm: 2.1978487968444824
Step 851 | learning_rate: 7.2703917790623e-05
Step 851 | epoch: 0.8198458574181118
Step 852 | loss: 3.4972476959228516
Step 852 | grad_norm: 2.4727413654327393
Step 852 | learning_rate: 7.26718047527296e-05
Step 852 | epoch: 0.8208092485549133
Step 853 | loss: 2.6326794624328613
Step 853 | grad_norm: 1.7093960046768188
Step 853 | learning_rate: 7.263969171483623e-05
Step 853 | epoch: 0.8217726396917149
Step 854 | loss: 2.941092014312744
Step 854 | grad_norm: 2.1310675144195557
Step 854 | learning_rate: 7.260757867694285e-05
Step 854 | epoch: 0.8227360308285164
Step 855 | loss: 3.13390851020813
Step 855 | grad_norm: 2.139523983001709
Step 855 | learning_rate: 7.257546563904946e-05
Step 855 | epoch: 0.8236994219653179
Step 856 | loss: 3.666733980178833
Step 856 | grad_norm: 2.5305840969085693
Step 856 | learning_rate: 7.254335260115608e-05
Step 856 | epoch: 0.8246628131021194
Step 857 | loss: 3.1352059841156006
Step 857 | grad_norm: 2.4041786193847656
Step 857 | learning_rate: 7.251123956326268e-05
Step 857 | epoch: 0.825626204238921
Step 858 | loss: 3.8000895977020264
Step 858 | grad_norm: 2.871450901031494
Step 858 | learning_rate: 7.24791265253693e-05
Step 858 | epoch: 0.8265895953757225
Step 859 | loss: 4.689367771148682
Step 859 | grad_norm: 3.7842533588409424
Step 859 | learning_rate: 7.244701348747593e-05
Step 859 | epoch: 0.8275529865125241
Step 860 | loss: 3.6319668292999268
Step 860 | grad_norm: 2.4432437419891357
Step 860 | learning_rate: 7.241490044958253e-05
Step 860 | epoch: 0.8285163776493256
Step 861 | loss: 3.37439227104187
Step 861 | grad_norm: 3.3363330364227295
Step 861 | learning_rate: 7.238278741168914e-05
Step 861 | epoch: 0.8294797687861272
Step 862 | loss: 4.164863586425781
Step 862 | grad_norm: 3.366736888885498
Step 862 | learning_rate: 7.235067437379576e-05
Step 862 | epoch: 0.8304431599229287
Step 863 | loss: 3.0251331329345703
Step 863 | grad_norm: 3.2736918926239014
Step 863 | learning_rate: 7.231856133590238e-05
Step 863 | epoch: 0.8314065510597303
Step 864 | loss: 3.536113739013672
Step 864 | grad_norm: 2.731577157974243
Step 864 | learning_rate: 7.228644829800899e-05
Step 864 | epoch: 0.8323699421965318
Step 865 | loss: 3.0215771198272705
Step 865 | grad_norm: 2.181241273880005
Step 865 | learning_rate: 7.225433526011561e-05
Step 865 | epoch: 0.8333333333333334
Step 866 | loss: 2.8182778358459473
Step 866 | grad_norm: 2.2922778129577637
Step 866 | learning_rate: 7.222222222222222e-05
Step 866 | epoch: 0.8342967244701349
Step 867 | loss: 3.4921016693115234
Step 867 | grad_norm: 3.008120059967041
Step 867 | learning_rate: 7.219010918432884e-05
Step 867 | epoch: 0.8352601156069365
Step 868 | loss: 3.4684574604034424
Step 868 | grad_norm: 2.6933979988098145
Step 868 | learning_rate: 7.215799614643546e-05
Step 868 | epoch: 0.8362235067437379
Step 869 | loss: 3.033087968826294
Step 869 | grad_norm: 2.232430934906006
Step 869 | learning_rate: 7.212588310854207e-05
Step 869 | epoch: 0.8371868978805395
Step 870 | loss: 2.9987170696258545
Step 870 | grad_norm: 2.303267002105713
Step 870 | learning_rate: 7.209377007064869e-05
Step 870 | epoch: 0.838150289017341
Step 871 | loss: 2.7519772052764893
Step 871 | grad_norm: 2.7210021018981934
Step 871 | learning_rate: 7.20616570327553e-05
Step 871 | epoch: 0.8391136801541426
Step 872 | loss: 2.9588418006896973
Step 872 | grad_norm: 2.122196674346924
Step 872 | learning_rate: 7.202954399486192e-05
Step 872 | epoch: 0.8400770712909441
Step 873 | loss: 3.4088523387908936
Step 873 | grad_norm: 2.544311046600342
Step 873 | learning_rate: 7.199743095696854e-05
Step 873 | epoch: 0.8410404624277457
Step 874 | loss: 3.274945020675659
Step 874 | grad_norm: 2.3610520362854004
Step 874 | learning_rate: 7.196531791907515e-05
Step 874 | epoch: 0.8420038535645472
Step 875 | loss: 3.985426425933838
Step 875 | grad_norm: 3.125948429107666
Step 875 | learning_rate: 7.193320488118175e-05
Step 875 | epoch: 0.8429672447013488
Step 876 | loss: 2.097991943359375
Step 876 | grad_norm: 1.7927542924880981
Step 876 | learning_rate: 7.190109184328839e-05
Step 876 | epoch: 0.8439306358381503
Step 877 | loss: 3.8022587299346924
Step 877 | grad_norm: 2.531486749649048
Step 877 | learning_rate: 7.1868978805395e-05
Step 877 | epoch: 0.8448940269749519
Step 878 | loss: 3.045379877090454
Step 878 | grad_norm: 2.087451219558716
Step 878 | learning_rate: 7.18368657675016e-05
Step 878 | epoch: 0.8458574181117534
Step 879 | loss: 3.5747087001800537
Step 879 | grad_norm: 2.466883420944214
Step 879 | learning_rate: 7.180475272960822e-05
Step 879 | epoch: 0.846820809248555
Step 880 | loss: 3.7268402576446533
Step 880 | grad_norm: 2.807021141052246
Step 880 | learning_rate: 7.177263969171484e-05
Step 880 | epoch: 0.8477842003853564
Step 881 | loss: 2.4193546772003174
Step 881 | grad_norm: 2.2061867713928223
Step 881 | learning_rate: 7.174052665382147e-05
Step 881 | epoch: 0.848747591522158
Step 882 | loss: 3.2437856197357178
Step 882 | grad_norm: 2.2515439987182617
Step 882 | learning_rate: 7.170841361592807e-05
Step 882 | epoch: 0.8497109826589595
Step 883 | loss: 3.4070756435394287
Step 883 | grad_norm: 2.4550976753234863
Step 883 | learning_rate: 7.167630057803468e-05
Step 883 | epoch: 0.850674373795761
Step 884 | loss: 2.9203810691833496
Step 884 | grad_norm: 2.526113986968994
Step 884 | learning_rate: 7.16441875401413e-05
Step 884 | epoch: 0.8516377649325626
Step 885 | loss: 2.924450635910034
Step 885 | grad_norm: 2.245908737182617
Step 885 | learning_rate: 7.161207450224792e-05
Step 885 | epoch: 0.8526011560693642
Step 886 | loss: 2.205636978149414
Step 886 | grad_norm: 2.3896379470825195
Step 886 | learning_rate: 7.157996146435453e-05
Step 886 | epoch: 0.8535645472061657
Step 887 | loss: 3.4327216148376465
Step 887 | grad_norm: 3.4856650829315186
Step 887 | learning_rate: 7.154784842646115e-05
Step 887 | epoch: 0.8545279383429673
Step 888 | loss: 2.687035322189331
Step 888 | grad_norm: 1.7290449142456055
Step 888 | learning_rate: 7.151573538856776e-05
Step 888 | epoch: 0.8554913294797688
Step 889 | loss: 3.0463757514953613
Step 889 | grad_norm: 2.001708745956421
Step 889 | learning_rate: 7.148362235067438e-05
Step 889 | epoch: 0.8564547206165704
Step 890 | loss: 3.7307844161987305
Step 890 | grad_norm: 2.792996644973755
Step 890 | learning_rate: 7.1451509312781e-05
Step 890 | epoch: 0.8574181117533719
Step 891 | loss: 3.656151056289673
Step 891 | grad_norm: 2.389028549194336
Step 891 | learning_rate: 7.141939627488761e-05
Step 891 | epoch: 0.8583815028901735
Step 892 | loss: 2.7791290283203125
Step 892 | grad_norm: 2.281766891479492
Step 892 | learning_rate: 7.138728323699421e-05
Step 892 | epoch: 0.859344894026975
Step 893 | loss: 4.306149005889893
Step 893 | grad_norm: 3.281874418258667
Step 893 | learning_rate: 7.135517019910084e-05
Step 893 | epoch: 0.8603082851637764
Step 894 | loss: 3.8986313343048096
Step 894 | grad_norm: 2.405714988708496
Step 894 | learning_rate: 7.132305716120746e-05
Step 894 | epoch: 0.861271676300578
Step 895 | loss: 2.9899375438690186
Step 895 | grad_norm: 2.4849841594696045
Step 895 | learning_rate: 7.129094412331408e-05
Step 895 | epoch: 0.8622350674373795
Step 896 | loss: 3.788820505142212
Step 896 | grad_norm: 2.9465653896331787
Step 896 | learning_rate: 7.125883108542069e-05
Step 896 | epoch: 0.8631984585741811
Step 897 | loss: 3.2536041736602783
Step 897 | grad_norm: 2.475203275680542
Step 897 | learning_rate: 7.122671804752729e-05
Step 897 | epoch: 0.8641618497109826
Step 898 | loss: 3.3514564037323
Step 898 | grad_norm: 2.5820119380950928
Step 898 | learning_rate: 7.119460500963391e-05
Step 898 | epoch: 0.8651252408477842
Step 899 | loss: 4.517330646514893
Step 899 | grad_norm: 3.2041127681732178
Step 899 | learning_rate: 7.116249197174053e-05
Step 899 | epoch: 0.8660886319845857
Step 900 | loss: 3.945405960083008
Step 900 | grad_norm: 2.6120729446411133
Step 900 | learning_rate: 7.113037893384714e-05
Step 900 | epoch: 0.8670520231213873
Step 901 | loss: 3.5680739879608154
Step 901 | grad_norm: 3.258741617202759
Step 901 | learning_rate: 7.109826589595376e-05
Step 901 | epoch: 0.8680154142581888
Step 902 | loss: 3.073688507080078
Step 902 | grad_norm: 1.959289789199829
Step 902 | learning_rate: 7.106615285806037e-05
Step 902 | epoch: 0.8689788053949904
Step 903 | loss: 3.4783143997192383
Step 903 | grad_norm: 2.4835917949676514
Step 903 | learning_rate: 7.103403982016699e-05
Step 903 | epoch: 0.869942196531792
Step 904 | loss: 3.9931819438934326
Step 904 | grad_norm: 2.656595468521118
Step 904 | learning_rate: 7.100192678227361e-05
Step 904 | epoch: 0.8709055876685935
Step 905 | loss: 3.0455729961395264
Step 905 | grad_norm: 1.9588367938995361
Step 905 | learning_rate: 7.096981374438022e-05
Step 905 | epoch: 0.871868978805395
Step 906 | loss: 3.7692761421203613
Step 906 | grad_norm: 4.164820194244385
Step 906 | learning_rate: 7.093770070648683e-05
Step 906 | epoch: 0.8728323699421965
Step 907 | loss: 3.530038595199585
Step 907 | grad_norm: 1.968692660331726
Step 907 | learning_rate: 7.090558766859345e-05
Step 907 | epoch: 0.873795761078998
Step 908 | loss: 3.7461323738098145
Step 908 | grad_norm: 3.1539385318756104
Step 908 | learning_rate: 7.087347463070007e-05
Step 908 | epoch: 0.8747591522157996
Step 909 | loss: 3.684556007385254
Step 909 | grad_norm: 1.8531885147094727
Step 909 | learning_rate: 7.084136159280669e-05
Step 909 | epoch: 0.8757225433526011
Step 910 | loss: 2.9710869789123535
Step 910 | grad_norm: 2.024153470993042
Step 910 | learning_rate: 7.08092485549133e-05
Step 910 | epoch: 0.8766859344894027
Step 911 | loss: 3.4409570693969727
Step 911 | grad_norm: 2.6866021156311035
Step 911 | learning_rate: 7.07771355170199e-05
Step 911 | epoch: 0.8776493256262042
Step 912 | loss: 3.8317549228668213
Step 912 | grad_norm: 7.409801006317139
Step 912 | learning_rate: 7.074502247912654e-05
Step 912 | epoch: 0.8786127167630058
Step 913 | loss: 3.0796992778778076
Step 913 | grad_norm: 2.0125885009765625
Step 913 | learning_rate: 7.071290944123315e-05
Step 913 | epoch: 0.8795761078998073
Step 914 | loss: 3.308046579360962
Step 914 | grad_norm: 3.207961082458496
Step 914 | learning_rate: 7.068079640333975e-05
Step 914 | epoch: 0.8805394990366089
Step 915 | loss: 3.722029447555542
Step 915 | grad_norm: 3.055894374847412
Step 915 | learning_rate: 7.064868336544638e-05
Step 915 | epoch: 0.8815028901734104
Step 916 | loss: 3.5459868907928467
Step 916 | grad_norm: 3.2668585777282715
Step 916 | learning_rate: 7.061657032755298e-05
Step 916 | epoch: 0.882466281310212
Step 917 | loss: 3.1790592670440674
Step 917 | grad_norm: 2.8593153953552246
Step 917 | learning_rate: 7.05844572896596e-05
Step 917 | epoch: 0.8834296724470135
Step 918 | loss: 2.958472728729248
Step 918 | grad_norm: 2.8476617336273193
Step 918 | learning_rate: 7.055234425176622e-05
Step 918 | epoch: 0.884393063583815
Step 919 | loss: 3.631648302078247
Step 919 | grad_norm: 2.731884717941284
Step 919 | learning_rate: 7.052023121387283e-05
Step 919 | epoch: 0.8853564547206165
Step 920 | loss: 3.2409138679504395
Step 920 | grad_norm: 4.66912841796875
Step 920 | learning_rate: 7.048811817597945e-05
Step 920 | epoch: 0.8863198458574181
Step 921 | loss: 3.6027963161468506
Step 921 | grad_norm: 2.7563633918762207
Step 921 | learning_rate: 7.045600513808607e-05
Step 921 | epoch: 0.8872832369942196
Step 922 | loss: 2.980558156967163
Step 922 | grad_norm: 3.4070870876312256
Step 922 | learning_rate: 7.042389210019268e-05
Step 922 | epoch: 0.8882466281310212
Step 923 | loss: 3.0139689445495605
Step 923 | grad_norm: 2.2771167755126953
Step 923 | learning_rate: 7.03917790622993e-05
Step 923 | epoch: 0.8892100192678227
Step 924 | loss: 2.800102472305298
Step 924 | grad_norm: 2.2660446166992188
Step 924 | learning_rate: 7.035966602440591e-05
Step 924 | epoch: 0.8901734104046243
Step 925 | loss: 3.6961376667022705
Step 925 | grad_norm: 2.3228611946105957
Step 925 | learning_rate: 7.032755298651253e-05
Step 925 | epoch: 0.8911368015414258
Step 926 | loss: 3.8720805644989014
Step 926 | grad_norm: 2.3016304969787598
Step 926 | learning_rate: 7.029543994861915e-05
Step 926 | epoch: 0.8921001926782274
Step 927 | loss: 3.045846939086914
Step 927 | grad_norm: 2.2942678928375244
Step 927 | learning_rate: 7.026332691072576e-05
Step 927 | epoch: 0.8930635838150289
Step 928 | loss: 3.527040958404541
Step 928 | grad_norm: 2.782418727874756
Step 928 | learning_rate: 7.023121387283237e-05
Step 928 | epoch: 0.8940269749518305
Step 929 | loss: 3.7800261974334717
Step 929 | grad_norm: 2.5963735580444336
Step 929 | learning_rate: 7.019910083493899e-05
Step 929 | epoch: 0.894990366088632
Step 930 | loss: 3.3249194622039795
Step 930 | grad_norm: 2.2008485794067383
Step 930 | learning_rate: 7.016698779704561e-05
Step 930 | epoch: 0.8959537572254336
Step 931 | loss: 3.611407995223999
Step 931 | grad_norm: 2.593423843383789
Step 931 | learning_rate: 7.013487475915222e-05
Step 931 | epoch: 0.896917148362235
Step 932 | loss: 3.613926410675049
Step 932 | grad_norm: 2.3728489875793457
Step 932 | learning_rate: 7.010276172125884e-05
Step 932 | epoch: 0.8978805394990366
Step 933 | loss: 3.649286985397339
Step 933 | grad_norm: 2.9918670654296875
Step 933 | learning_rate: 7.007064868336544e-05
Step 933 | epoch: 0.8988439306358381
Step 934 | loss: 3.8375375270843506
Step 934 | grad_norm: 2.5572328567504883
Step 934 | learning_rate: 7.003853564547206e-05
Step 934 | epoch: 0.8998073217726397
Step 935 | loss: 3.083061695098877
Step 935 | grad_norm: 2.492870569229126
Step 935 | learning_rate: 7.000642260757869e-05
Step 935 | epoch: 0.9007707129094412
Step 936 | loss: 3.0383222103118896
Step 936 | grad_norm: 2.5408124923706055
Step 936 | learning_rate: 6.99743095696853e-05
Step 936 | epoch: 0.9017341040462428
Step 937 | loss: 3.251568555831909
Step 937 | grad_norm: 3.1778247356414795
Step 937 | learning_rate: 6.994219653179191e-05
Step 937 | epoch: 0.9026974951830443
Step 938 | loss: 3.136085033416748
Step 938 | grad_norm: 2.418661594390869
Step 938 | learning_rate: 6.991008349389852e-05
Step 938 | epoch: 0.9036608863198459
Step 939 | loss: 3.620236873626709
Step 939 | grad_norm: 3.6010501384735107
Step 939 | learning_rate: 6.987797045600514e-05
Step 939 | epoch: 0.9046242774566474
Step 940 | loss: 3.2012007236480713
Step 940 | grad_norm: 2.1864731311798096
Step 940 | learning_rate: 6.984585741811176e-05
Step 940 | epoch: 0.905587668593449
Step 941 | loss: 2.5377066135406494
Step 941 | grad_norm: 2.5648200511932373
Step 941 | learning_rate: 6.981374438021837e-05
Step 941 | epoch: 0.9065510597302505
Step 942 | loss: 2.845335006713867
Step 942 | grad_norm: 2.192410945892334
Step 942 | learning_rate: 6.978163134232498e-05
Step 942 | epoch: 0.9075144508670521
Step 943 | loss: 3.340547561645508
Step 943 | grad_norm: 2.10164213180542
Step 943 | learning_rate: 6.97495183044316e-05
Step 943 | epoch: 0.9084778420038536
Step 944 | loss: 3.4120593070983887
Step 944 | grad_norm: 2.016850471496582
Step 944 | learning_rate: 6.971740526653822e-05
Step 944 | epoch: 0.9094412331406551
Step 945 | loss: 3.8756279945373535
Step 945 | grad_norm: 2.639599561691284
Step 945 | learning_rate: 6.968529222864484e-05
Step 945 | epoch: 0.9104046242774566
Step 946 | loss: 3.4117650985717773
Step 946 | grad_norm: 2.368435859680176
Step 946 | learning_rate: 6.965317919075145e-05
Step 946 | epoch: 0.9113680154142582
Step 947 | loss: 4.119967460632324
Step 947 | grad_norm: 6.133131980895996
Step 947 | learning_rate: 6.962106615285806e-05
Step 947 | epoch: 0.9123314065510597
Step 948 | loss: 3.842364549636841
Step 948 | grad_norm: 3.139885902404785
Step 948 | learning_rate: 6.958895311496468e-05
Step 948 | epoch: 0.9132947976878613
Step 949 | loss: 2.7173075675964355
Step 949 | grad_norm: 2.68707013130188
Step 949 | learning_rate: 6.95568400770713e-05
Step 949 | epoch: 0.9142581888246628
Step 950 | loss: 3.9588375091552734
Step 950 | grad_norm: 2.346162796020508
Step 950 | learning_rate: 6.95247270391779e-05
Step 950 | epoch: 0.9152215799614644
Step 951 | loss: 3.4447851181030273
Step 951 | grad_norm: 2.3936197757720947
Step 951 | learning_rate: 6.949261400128453e-05
Step 951 | epoch: 0.9161849710982659
Step 952 | loss: 3.647138833999634
Step 952 | grad_norm: 2.457130193710327
Step 952 | learning_rate: 6.946050096339113e-05
Step 952 | epoch: 0.9171483622350675
Step 953 | loss: 4.005981922149658
Step 953 | grad_norm: 2.7469165325164795
Step 953 | learning_rate: 6.942838792549775e-05
Step 953 | epoch: 0.918111753371869
Step 954 | loss: 3.554845094680786
Step 954 | grad_norm: 2.248375415802002
Step 954 | learning_rate: 6.939627488760438e-05
Step 954 | epoch: 0.9190751445086706
Step 955 | loss: 3.998782157897949
Step 955 | grad_norm: 2.750717878341675
Step 955 | learning_rate: 6.936416184971098e-05
Step 955 | epoch: 0.9200385356454721
Step 956 | loss: 2.967603921890259
Step 956 | grad_norm: 1.9458818435668945
Step 956 | learning_rate: 6.933204881181759e-05
Step 956 | epoch: 0.9210019267822736
Step 957 | loss: 3.5036098957061768
Step 957 | grad_norm: 3.750683069229126
Step 957 | learning_rate: 6.929993577392421e-05
Step 957 | epoch: 0.9219653179190751
Step 958 | loss: 3.5603394508361816
Step 958 | grad_norm: 2.8265862464904785
Step 958 | learning_rate: 6.926782273603083e-05
Step 958 | epoch: 0.9229287090558767
Step 959 | loss: 3.2595813274383545
Step 959 | grad_norm: 2.6412458419799805
Step 959 | learning_rate: 6.923570969813745e-05
Step 959 | epoch: 0.9238921001926782
Step 960 | loss: 3.2742738723754883
Step 960 | grad_norm: 2.0483627319335938
Step 960 | learning_rate: 6.920359666024406e-05
Step 960 | epoch: 0.9248554913294798
Step 961 | loss: 2.988290309906006
Step 961 | grad_norm: 1.9051543474197388
Step 961 | learning_rate: 6.917148362235067e-05
Step 961 | epoch: 0.9258188824662813
Step 962 | loss: 3.0388686656951904
Step 962 | grad_norm: 2.369502544403076
Step 962 | learning_rate: 6.91393705844573e-05
Step 962 | epoch: 0.9267822736030829
Step 963 | loss: 3.395425796508789
Step 963 | grad_norm: 2.4221842288970947
Step 963 | learning_rate: 6.910725754656391e-05
Step 963 | epoch: 0.9277456647398844
Step 964 | loss: 3.8262784481048584
Step 964 | grad_norm: 2.471609354019165
Step 964 | learning_rate: 6.907514450867052e-05
Step 964 | epoch: 0.928709055876686
Step 965 | loss: 2.8934483528137207
Step 965 | grad_norm: 2.372173547744751
Step 965 | learning_rate: 6.904303147077714e-05
Step 965 | epoch: 0.9296724470134875
Step 966 | loss: 4.002532482147217
Step 966 | grad_norm: 3.11738657951355
Step 966 | learning_rate: 6.901091843288376e-05
Step 966 | epoch: 0.930635838150289
Step 967 | loss: 3.1714916229248047
Step 967 | grad_norm: 2.032180070877075
Step 967 | learning_rate: 6.897880539499037e-05
Step 967 | epoch: 0.9315992292870906
Step 968 | loss: 4.512871742248535
Step 968 | grad_norm: 3.1123738288879395
Step 968 | learning_rate: 6.894669235709699e-05
Step 968 | epoch: 0.9325626204238922
Step 969 | loss: 3.138684034347534
Step 969 | grad_norm: 2.2858495712280273
Step 969 | learning_rate: 6.89145793192036e-05
Step 969 | epoch: 0.9335260115606936
Step 970 | loss: 3.5236904621124268
Step 970 | grad_norm: 2.5977091789245605
Step 970 | learning_rate: 6.888246628131022e-05
Step 970 | epoch: 0.9344894026974951
Step 971 | loss: 3.7499279975891113
Step 971 | grad_norm: 2.0549302101135254
Step 971 | learning_rate: 6.885035324341684e-05
Step 971 | epoch: 0.9354527938342967
Step 972 | loss: 3.419630527496338
Step 972 | grad_norm: 2.805778980255127
Step 972 | learning_rate: 6.881824020552344e-05
Step 972 | epoch: 0.9364161849710982
Step 973 | loss: 2.824507474899292
Step 973 | grad_norm: 2.745526075363159
Step 973 | learning_rate: 6.878612716763007e-05
Step 973 | epoch: 0.9373795761078998
Step 974 | loss: 5.001659393310547
Step 974 | grad_norm: 3.6027400493621826
Step 974 | learning_rate: 6.875401412973667e-05
Step 974 | epoch: 0.9383429672447013
Step 975 | loss: 3.310872793197632
Step 975 | grad_norm: 2.0402510166168213
Step 975 | learning_rate: 6.87219010918433e-05
Step 975 | epoch: 0.9393063583815029
Step 976 | loss: 2.8494880199432373
Step 976 | grad_norm: 2.545952558517456
Step 976 | learning_rate: 6.868978805394992e-05
Step 976 | epoch: 0.9402697495183044
Step 977 | loss: 3.116922616958618
Step 977 | grad_norm: 2.4680252075195312
Step 977 | learning_rate: 6.865767501605652e-05
Step 977 | epoch: 0.941233140655106
Step 978 | loss: 3.7681167125701904
Step 978 | grad_norm: 2.6520378589630127
Step 978 | learning_rate: 6.862556197816313e-05
Step 978 | epoch: 0.9421965317919075
Step 979 | loss: 3.01934814453125
Step 979 | grad_norm: 2.4792897701263428
Step 979 | learning_rate: 6.859344894026975e-05
Step 979 | epoch: 0.9431599229287091
Step 980 | loss: 3.4323015213012695
Step 980 | grad_norm: 2.751570701599121
Step 980 | learning_rate: 6.856133590237637e-05
Step 980 | epoch: 0.9441233140655106
Step 981 | loss: 2.6086223125457764
Step 981 | grad_norm: 2.485039234161377
Step 981 | learning_rate: 6.852922286448298e-05
Step 981 | epoch: 0.9450867052023122
Step 982 | loss: 3.442601442337036
Step 982 | grad_norm: 2.2831838130950928
Step 982 | learning_rate: 6.84971098265896e-05
Step 982 | epoch: 0.9460500963391136
Step 983 | loss: 2.5259158611297607
Step 983 | grad_norm: 2.141988515853882
Step 983 | learning_rate: 6.846499678869621e-05
Step 983 | epoch: 0.9470134874759152
Step 984 | loss: 3.8329622745513916
Step 984 | grad_norm: 2.664907932281494
Step 984 | learning_rate: 6.843288375080283e-05
Step 984 | epoch: 0.9479768786127167
Step 985 | loss: 3.3090603351593018
Step 985 | grad_norm: 2.3034186363220215
Step 985 | learning_rate: 6.840077071290945e-05
Step 985 | epoch: 0.9489402697495183
Step 986 | loss: 3.651902437210083
Step 986 | grad_norm: 2.7415878772735596
Step 986 | learning_rate: 6.836865767501606e-05
Step 986 | epoch: 0.9499036608863198
Step 987 | loss: 3.458376407623291
Step 987 | grad_norm: 2.631747007369995
Step 987 | learning_rate: 6.833654463712268e-05
Step 987 | epoch: 0.9508670520231214
Step 988 | loss: 4.022623538970947
Step 988 | grad_norm: 3.09564208984375
Step 988 | learning_rate: 6.830443159922929e-05
Step 988 | epoch: 0.9518304431599229
Step 989 | loss: 4.172079563140869
Step 989 | grad_norm: 2.3151113986968994
Step 989 | learning_rate: 6.82723185613359e-05
Step 989 | epoch: 0.9527938342967245
Step 990 | loss: 3.215909004211426
Step 990 | grad_norm: 2.71061372756958
Step 990 | learning_rate: 6.824020552344253e-05
Step 990 | epoch: 0.953757225433526
Step 991 | loss: 4.087185382843018
Step 991 | grad_norm: 2.619617223739624
Step 991 | learning_rate: 6.820809248554913e-05
Step 991 | epoch: 0.9547206165703276
Step 992 | loss: 2.448699712753296
Step 992 | grad_norm: 1.9998419284820557
Step 992 | learning_rate: 6.817597944765574e-05
Step 992 | epoch: 0.9556840077071291
Step 993 | loss: 3.2020206451416016
Step 993 | grad_norm: 2.7391903400421143
Step 993 | learning_rate: 6.814386640976236e-05
Step 993 | epoch: 0.9566473988439307
Step 994 | loss: 3.0846781730651855
Step 994 | grad_norm: 2.2497000694274902
Step 994 | learning_rate: 6.811175337186898e-05
Step 994 | epoch: 0.9576107899807321
Step 995 | loss: 2.826775312423706
Step 995 | grad_norm: 2.2764573097229004
Step 995 | learning_rate: 6.80796403339756e-05
Step 995 | epoch: 0.9585741811175337
Step 996 | loss: 3.6647071838378906
Step 996 | grad_norm: 3.6005208492279053
Step 996 | learning_rate: 6.804752729608221e-05
Step 996 | epoch: 0.9595375722543352
Step 997 | loss: 3.463003396987915
Step 997 | grad_norm: 2.4295475482940674
Step 997 | learning_rate: 6.801541425818882e-05
Step 997 | epoch: 0.9605009633911368
Step 998 | loss: 3.0614678859710693
Step 998 | grad_norm: 2.762474536895752
Step 998 | learning_rate: 6.798330122029545e-05
Step 998 | epoch: 0.9614643545279383
Step 999 | loss: 3.252596616744995
Step 999 | grad_norm: 2.4689676761627197
Step 999 | learning_rate: 6.795118818240206e-05
Step 999 | epoch: 0.9624277456647399
Step 1000 | loss: 3.1207737922668457
Step 1000 | grad_norm: 2.1396870613098145
Step 1000 | learning_rate: 6.791907514450867e-05
Step 1000 | epoch: 0.9633911368015414
Step 1001 | loss: 4.148985862731934
Step 1001 | grad_norm: 3.107463836669922
Step 1001 | learning_rate: 6.788696210661529e-05
Step 1001 | epoch: 0.964354527938343
Step 1002 | loss: 3.6216793060302734
Step 1002 | grad_norm: 2.769773483276367
Step 1002 | learning_rate: 6.78548490687219e-05
Step 1002 | epoch: 0.9653179190751445
Step 1003 | loss: 3.3545517921447754
Step 1003 | grad_norm: 2.054719924926758
Step 1003 | learning_rate: 6.782273603082852e-05
Step 1003 | epoch: 0.9662813102119461
Step 1004 | loss: 2.725316047668457
Step 1004 | grad_norm: 1.8953319787979126
Step 1004 | learning_rate: 6.779062299293514e-05
Step 1004 | epoch: 0.9672447013487476
Step 1005 | loss: 2.663337469100952
Step 1005 | grad_norm: 2.40156626701355
Step 1005 | learning_rate: 6.775850995504175e-05
Step 1005 | epoch: 0.9682080924855492
Step 1006 | loss: 3.6112847328186035
Step 1006 | grad_norm: 3.150545835494995
Step 1006 | learning_rate: 6.772639691714835e-05
Step 1006 | epoch: 0.9691714836223507
Step 1007 | loss: 2.728426456451416
Step 1007 | grad_norm: 2.8188393115997314
Step 1007 | learning_rate: 6.769428387925499e-05
Step 1007 | epoch: 0.9701348747591522
Step 1008 | loss: 3.544801712036133
Step 1008 | grad_norm: 2.1071574687957764
Step 1008 | learning_rate: 6.76621708413616e-05
Step 1008 | epoch: 0.9710982658959537
Step 1009 | loss: 3.161449909210205
Step 1009 | grad_norm: 1.9797327518463135
Step 1009 | learning_rate: 6.763005780346822e-05
Step 1009 | epoch: 0.9720616570327553
Step 1010 | loss: 3.426701307296753
Step 1010 | grad_norm: 2.5896060466766357
Step 1010 | learning_rate: 6.759794476557482e-05
Step 1010 | epoch: 0.9730250481695568
Step 1011 | loss: 2.9684102535247803
Step 1011 | grad_norm: 2.63093900680542
Step 1011 | learning_rate: 6.756583172768145e-05
Step 1011 | epoch: 0.9739884393063584
Step 1012 | loss: 3.1446900367736816
Step 1012 | grad_norm: 2.4215967655181885
Step 1012 | learning_rate: 6.753371868978807e-05
Step 1012 | epoch: 0.9749518304431599
Step 1013 | loss: 3.562742233276367
Step 1013 | grad_norm: 2.92459774017334
Step 1013 | learning_rate: 6.750160565189467e-05
Step 1013 | epoch: 0.9759152215799615
Step 1014 | loss: 3.2395880222320557
Step 1014 | grad_norm: 2.0592105388641357
Step 1014 | learning_rate: 6.746949261400128e-05
Step 1014 | epoch: 0.976878612716763
Step 1015 | loss: 3.973588466644287
Step 1015 | grad_norm: 3.271419048309326
Step 1015 | learning_rate: 6.74373795761079e-05
Step 1015 | epoch: 0.9778420038535646
Step 1016 | loss: 3.698608875274658
Step 1016 | grad_norm: 2.642082452774048
Step 1016 | learning_rate: 6.740526653821452e-05
Step 1016 | epoch: 0.9788053949903661
Step 1017 | loss: 3.347710371017456
Step 1017 | grad_norm: 2.791445732116699
Step 1017 | learning_rate: 6.737315350032113e-05
Step 1017 | epoch: 0.9797687861271677
Step 1018 | loss: 3.960280179977417
Step 1018 | grad_norm: 2.79664945602417
Step 1018 | learning_rate: 6.734104046242775e-05
Step 1018 | epoch: 0.9807321772639692
Step 1019 | loss: 3.5279974937438965
Step 1019 | grad_norm: 2.637643337249756
Step 1019 | learning_rate: 6.730892742453436e-05
Step 1019 | epoch: 0.9816955684007708
Step 1020 | loss: 3.680034875869751
Step 1020 | grad_norm: 3.046997547149658
Step 1020 | learning_rate: 6.727681438664098e-05
Step 1020 | epoch: 0.9826589595375722
Step 1021 | loss: 2.9580540657043457
Step 1021 | grad_norm: 2.1505281925201416
Step 1021 | learning_rate: 6.72447013487476e-05
Step 1021 | epoch: 0.9836223506743738
Step 1022 | loss: 3.4133450984954834
Step 1022 | grad_norm: 2.343658447265625
Step 1022 | learning_rate: 6.721258831085421e-05
Step 1022 | epoch: 0.9845857418111753
Step 1023 | loss: 3.085214614868164
Step 1023 | grad_norm: 2.7288451194763184
Step 1023 | learning_rate: 6.718047527296083e-05
Step 1023 | epoch: 0.9855491329479769
Step 1024 | loss: 3.716984272003174
Step 1024 | grad_norm: 2.358525514602661
Step 1024 | learning_rate: 6.714836223506744e-05
Step 1024 | epoch: 0.9865125240847784
Step 1025 | loss: 2.9399681091308594
Step 1025 | grad_norm: 2.097761631011963
Step 1025 | learning_rate: 6.711624919717406e-05
Step 1025 | epoch: 0.98747591522158
Step 1026 | loss: 2.6421072483062744
Step 1026 | grad_norm: 2.109041452407837
Step 1026 | learning_rate: 6.708413615928068e-05
Step 1026 | epoch: 0.9884393063583815
Step 1027 | loss: 3.164278030395508
Step 1027 | grad_norm: 2.3990402221679688
Step 1027 | learning_rate: 6.705202312138729e-05
Step 1027 | epoch: 0.9894026974951831
Step 1028 | loss: 4.031832695007324
Step 1028 | grad_norm: 3.2098913192749023
Step 1028 | learning_rate: 6.70199100834939e-05
Step 1028 | epoch: 0.9903660886319846
Step 1029 | loss: 3.511824607849121
Step 1029 | grad_norm: 4.149477005004883
Step 1029 | learning_rate: 6.698779704560051e-05
Step 1029 | epoch: 0.9913294797687862
Step 1030 | loss: 2.801694393157959
Step 1030 | grad_norm: 2.9755377769470215
Step 1030 | learning_rate: 6.695568400770714e-05
Step 1030 | epoch: 0.9922928709055877
Step 1031 | loss: 3.8979549407958984
Step 1031 | grad_norm: 2.6715474128723145
Step 1031 | learning_rate: 6.692357096981374e-05
Step 1031 | epoch: 0.9932562620423893
Step 1032 | loss: 3.0172080993652344
Step 1032 | grad_norm: 2.6570348739624023
Step 1032 | learning_rate: 6.689145793192036e-05
Step 1032 | epoch: 0.9942196531791907
Step 1033 | loss: 2.663748264312744
Step 1033 | grad_norm: 2.168614625930786
Step 1033 | learning_rate: 6.685934489402697e-05
Step 1033 | epoch: 0.9951830443159922
Step 1034 | loss: 3.4936487674713135
Step 1034 | grad_norm: 2.5457875728607178
Step 1034 | learning_rate: 6.682723185613359e-05
Step 1034 | epoch: 0.9961464354527938
Step 1035 | loss: 4.815717697143555
Step 1035 | grad_norm: 3.8546829223632812
Step 1035 | learning_rate: 6.679511881824021e-05
Step 1035 | epoch: 0.9971098265895953
Step 1036 | loss: 3.352679967880249
Step 1036 | grad_norm: 2.4921646118164062
Step 1036 | learning_rate: 6.676300578034682e-05
Step 1036 | epoch: 0.9980732177263969
Step 1037 | loss: 4.438437461853027
Step 1037 | grad_norm: 2.976727247238159
Step 1037 | learning_rate: 6.673089274245344e-05
Step 1037 | epoch: 0.9990366088631984
Step 1038 | loss: 4.147632122039795
Step 1038 | grad_norm: 6.344655513763428
Step 1038 | learning_rate: 6.669877970456005e-05
Step 1038 | epoch: 1.0
Step 1039 | loss: 3.237138032913208
Step 1039 | grad_norm: 2.495964288711548
Step 1039 | learning_rate: 6.666666666666667e-05
Step 1039 | epoch: 1.0009633911368014
Step 1040 | loss: 3.392385482788086
Step 1040 | grad_norm: 3.357009172439575
Step 1040 | learning_rate: 6.663455362877329e-05
Step 1040 | epoch: 1.001926782273603
Step 1041 | loss: 3.6932952404022217
Step 1041 | grad_norm: 2.5688729286193848
Step 1041 | learning_rate: 6.66024405908799e-05
Step 1041 | epoch: 1.0028901734104045
Step 1042 | loss: 3.5079245567321777
Step 1042 | grad_norm: 2.4683916568756104
Step 1042 | learning_rate: 6.65703275529865e-05
Step 1042 | epoch: 1.0038535645472062
Step 1043 | loss: 3.4090328216552734
Step 1043 | grad_norm: 2.9436306953430176
Step 1043 | learning_rate: 6.653821451509314e-05
Step 1043 | epoch: 1.0048169556840076
Step 1044 | loss: 3.7646079063415527
Step 1044 | grad_norm: 3.3244729042053223
Step 1044 | learning_rate: 6.650610147719975e-05
Step 1044 | epoch: 1.0057803468208093
Step 1045 | loss: 3.6084835529327393
Step 1045 | grad_norm: 2.6799979209899902
Step 1045 | learning_rate: 6.647398843930635e-05
Step 1045 | epoch: 1.0067437379576107
Step 1046 | loss: 3.2623796463012695
Step 1046 | grad_norm: 2.529625177383423
Step 1046 | learning_rate: 6.644187540141298e-05
Step 1046 | epoch: 1.0077071290944124
Step 1047 | loss: 3.5740010738372803
Step 1047 | grad_norm: 2.241021156311035
Step 1047 | learning_rate: 6.640976236351958e-05
Step 1047 | epoch: 1.0086705202312138
Step 1048 | loss: 3.534367799758911
Step 1048 | grad_norm: 2.95509934425354
Step 1048 | learning_rate: 6.637764932562622e-05
Step 1048 | epoch: 1.0096339113680155
Step 1049 | loss: 2.965235948562622
Step 1049 | grad_norm: 2.1488840579986572
Step 1049 | learning_rate: 6.634553628773283e-05
Step 1049 | epoch: 1.010597302504817
Step 1050 | loss: 2.655977964401245
Step 1050 | grad_norm: 2.392019033432007
Step 1050 | learning_rate: 6.631342324983943e-05
Step 1050 | epoch: 1.0115606936416186
Step 1051 | loss: 4.066375732421875
Step 1051 | grad_norm: 2.5225582122802734
Step 1051 | learning_rate: 6.628131021194605e-05
Step 1051 | epoch: 1.01252408477842
Step 1052 | loss: 4.011355400085449
Step 1052 | grad_norm: 2.528961181640625
Step 1052 | learning_rate: 6.624919717405267e-05
Step 1052 | epoch: 1.0134874759152215
Step 1053 | loss: 3.033298969268799
Step 1053 | grad_norm: 2.3450777530670166
Step 1053 | learning_rate: 6.621708413615928e-05
Step 1053 | epoch: 1.0144508670520231
Step 1054 | loss: 2.7523844242095947
Step 1054 | grad_norm: 3.80790114402771
Step 1054 | learning_rate: 6.61849710982659e-05
Step 1054 | epoch: 1.0154142581888246
Step 1055 | loss: 3.563471555709839
Step 1055 | grad_norm: 2.2909483909606934
Step 1055 | learning_rate: 6.615285806037251e-05
Step 1055 | epoch: 1.0163776493256262
Step 1056 | loss: 3.592256546020508
Step 1056 | grad_norm: 2.2778208255767822
Step 1056 | learning_rate: 6.612074502247913e-05
Step 1056 | epoch: 1.0173410404624277
Step 1057 | loss: 3.0656192302703857
Step 1057 | grad_norm: 2.3965702056884766
Step 1057 | learning_rate: 6.608863198458575e-05
Step 1057 | epoch: 1.0183044315992293
Step 1058 | loss: 3.2971999645233154
Step 1058 | grad_norm: 2.435600996017456
Step 1058 | learning_rate: 6.605651894669236e-05
Step 1058 | epoch: 1.0192678227360308
Step 1059 | loss: 3.617103099822998
Step 1059 | grad_norm: 2.781384229660034
Step 1059 | learning_rate: 6.602440590879898e-05
Step 1059 | epoch: 1.0202312138728324
Step 1060 | loss: 3.8615169525146484
Step 1060 | grad_norm: 3.8724117279052734
Step 1060 | learning_rate: 6.599229287090559e-05
Step 1060 | epoch: 1.0211946050096339
Step 1061 | loss: 3.480854034423828
Step 1061 | grad_norm: 2.340269088745117
Step 1061 | learning_rate: 6.596017983301221e-05
Step 1061 | epoch: 1.0221579961464355
Step 1062 | loss: 3.6229913234710693
Step 1062 | grad_norm: 2.766965866088867
Step 1062 | learning_rate: 6.592806679511883e-05
Step 1062 | epoch: 1.023121387283237
Step 1063 | loss: 3.7196528911590576
Step 1063 | grad_norm: 2.321625232696533
Step 1063 | learning_rate: 6.589595375722544e-05
Step 1063 | epoch: 1.0240847784200386
Step 1064 | loss: 2.6710855960845947
Step 1064 | grad_norm: 2.208017349243164
Step 1064 | learning_rate: 6.586384071933204e-05
Step 1064 | epoch: 1.02504816955684
Step 1065 | loss: 3.2373239994049072
Step 1065 | grad_norm: 2.64782977104187
Step 1065 | learning_rate: 6.583172768143867e-05
Step 1065 | epoch: 1.0260115606936415
Step 1066 | loss: 3.7389512062072754
Step 1066 | grad_norm: 2.6021506786346436
Step 1066 | learning_rate: 6.579961464354529e-05
Step 1066 | epoch: 1.0269749518304432
Step 1067 | loss: 2.8531734943389893
Step 1067 | grad_norm: 2.301313877105713
Step 1067 | learning_rate: 6.57675016056519e-05
Step 1067 | epoch: 1.0279383429672446
Step 1068 | loss: 3.0175390243530273
Step 1068 | grad_norm: 3.102327823638916
Step 1068 | learning_rate: 6.573538856775852e-05
Step 1068 | epoch: 1.0289017341040463
Step 1069 | loss: 3.915445327758789
Step 1069 | grad_norm: 2.837444543838501
Step 1069 | learning_rate: 6.570327552986512e-05
Step 1069 | epoch: 1.0298651252408477
Step 1070 | loss: 2.618527889251709
Step 1070 | grad_norm: 2.535090208053589
Step 1070 | learning_rate: 6.567116249197174e-05
Step 1070 | epoch: 1.0308285163776494
Step 1071 | loss: 2.8633735179901123
Step 1071 | grad_norm: 2.861797332763672
Step 1071 | learning_rate: 6.563904945407836e-05
Step 1071 | epoch: 1.0317919075144508
Step 1072 | loss: 2.7547502517700195
Step 1072 | grad_norm: 2.0518124103546143
Step 1072 | learning_rate: 6.560693641618497e-05
Step 1072 | epoch: 1.0327552986512525
Step 1073 | loss: 2.708080768585205
Step 1073 | grad_norm: 2.0154078006744385
Step 1073 | learning_rate: 6.557482337829159e-05
Step 1073 | epoch: 1.033718689788054
Step 1074 | loss: 3.7270069122314453
Step 1074 | grad_norm: 4.099812030792236
Step 1074 | learning_rate: 6.55427103403982e-05
Step 1074 | epoch: 1.0346820809248556
Step 1075 | loss: 3.3343265056610107
Step 1075 | grad_norm: 2.9848058223724365
Step 1075 | learning_rate: 6.551059730250482e-05
Step 1075 | epoch: 1.035645472061657
Step 1076 | loss: 3.6155874729156494
Step 1076 | grad_norm: 3.662445068359375
Step 1076 | learning_rate: 6.547848426461144e-05
Step 1076 | epoch: 1.0366088631984587
Step 1077 | loss: 3.6049437522888184
Step 1077 | grad_norm: 3.028059959411621
Step 1077 | learning_rate: 6.544637122671805e-05
Step 1077 | epoch: 1.0375722543352601
Step 1078 | loss: 3.412369966506958
Step 1078 | grad_norm: 2.6355059146881104
Step 1078 | learning_rate: 6.541425818882466e-05
Step 1078 | epoch: 1.0385356454720616
Step 1079 | loss: 3.7018942832946777
Step 1079 | grad_norm: 3.068730115890503
Step 1079 | learning_rate: 6.538214515093128e-05
Step 1079 | epoch: 1.0394990366088632
Step 1080 | loss: 3.087090015411377
Step 1080 | grad_norm: 2.7238271236419678
Step 1080 | learning_rate: 6.53500321130379e-05
Step 1080 | epoch: 1.0404624277456647
Step 1081 | loss: 3.0855424404144287
Step 1081 | grad_norm: 2.0979912281036377
Step 1081 | learning_rate: 6.53179190751445e-05
Step 1081 | epoch: 1.0414258188824663
Step 1082 | loss: 3.588209867477417
Step 1082 | grad_norm: 2.495638132095337
Step 1082 | learning_rate: 6.528580603725113e-05
Step 1082 | epoch: 1.0423892100192678
Step 1083 | loss: 3.6425509452819824
Step 1083 | grad_norm: 2.4029200077056885
Step 1083 | learning_rate: 6.525369299935773e-05
Step 1083 | epoch: 1.0433526011560694
Step 1084 | loss: 3.523717164993286
Step 1084 | grad_norm: 3.304368495941162
Step 1084 | learning_rate: 6.522157996146437e-05
Step 1084 | epoch: 1.0443159922928709
Step 1085 | loss: 3.5529398918151855
Step 1085 | grad_norm: 2.5751686096191406
Step 1085 | learning_rate: 6.518946692357098e-05
Step 1085 | epoch: 1.0452793834296725
Step 1086 | loss: 4.152658462524414
Step 1086 | grad_norm: 3.707481622695923
Step 1086 | learning_rate: 6.515735388567758e-05
Step 1086 | epoch: 1.046242774566474
Step 1087 | loss: 3.1165757179260254
Step 1087 | grad_norm: 2.2953367233276367
Step 1087 | learning_rate: 6.51252408477842e-05
Step 1087 | epoch: 1.0472061657032756
Step 1088 | loss: 3.3090622425079346
Step 1088 | grad_norm: 2.3148295879364014
Step 1088 | learning_rate: 6.509312780989083e-05
Step 1088 | epoch: 1.048169556840077
Step 1089 | loss: 3.2899067401885986
Step 1089 | grad_norm: 3.2087976932525635
Step 1089 | learning_rate: 6.506101477199743e-05
Step 1089 | epoch: 1.0491329479768785
Step 1090 | loss: 3.4952216148376465
Step 1090 | grad_norm: 3.126734972000122
Step 1090 | learning_rate: 6.502890173410405e-05
Step 1090 | epoch: 1.0500963391136802
Step 1091 | loss: 3.1175806522369385
Step 1091 | grad_norm: 2.9535505771636963
Step 1091 | learning_rate: 6.499678869621066e-05
Step 1091 | epoch: 1.0510597302504816
Step 1092 | loss: 3.1561379432678223
Step 1092 | grad_norm: 2.3895390033721924
Step 1092 | learning_rate: 6.496467565831727e-05
Step 1092 | epoch: 1.0520231213872833
Step 1093 | loss: 3.2276508808135986
Step 1093 | grad_norm: 3.069378137588501
Step 1093 | learning_rate: 6.49325626204239e-05
Step 1093 | epoch: 1.0529865125240847
Step 1094 | loss: 3.4188575744628906
Step 1094 | grad_norm: 2.6371686458587646
Step 1094 | learning_rate: 6.490044958253051e-05
Step 1094 | epoch: 1.0539499036608864
Step 1095 | loss: 4.414178371429443
Step 1095 | grad_norm: 2.681283473968506
Step 1095 | learning_rate: 6.486833654463712e-05
Step 1095 | epoch: 1.0549132947976878
Step 1096 | loss: 3.5022599697113037
Step 1096 | grad_norm: 2.8207919597625732
Step 1096 | learning_rate: 6.483622350674374e-05
Step 1096 | epoch: 1.0558766859344895
Step 1097 | loss: 3.715144634246826
Step 1097 | grad_norm: 3.4051401615142822
Step 1097 | learning_rate: 6.480411046885036e-05
Step 1097 | epoch: 1.056840077071291
Step 1098 | loss: 3.6417489051818848
Step 1098 | grad_norm: 2.246417284011841
Step 1098 | learning_rate: 6.477199743095698e-05
Step 1098 | epoch: 1.0578034682080926
Step 1099 | loss: 3.6121060848236084
Step 1099 | grad_norm: 2.907630443572998
Step 1099 | learning_rate: 6.473988439306359e-05
Step 1099 | epoch: 1.058766859344894
Step 1100 | loss: 3.4941914081573486
Step 1100 | grad_norm: 2.5128395557403564
Step 1100 | learning_rate: 6.47077713551702e-05
Step 1100 | epoch: 1.0597302504816957
Step 1101 | loss: 3.2180378437042236
Step 1101 | grad_norm: 2.171241044998169
Step 1101 | learning_rate: 6.467565831727682e-05
Step 1101 | epoch: 1.060693641618497
Step 1102 | loss: 2.746312141418457
Step 1102 | grad_norm: 4.60798454284668
Step 1102 | learning_rate: 6.464354527938344e-05
Step 1102 | epoch: 1.0616570327552985
Step 1103 | loss: 4.899930953979492
Step 1103 | grad_norm: 3.856424570083618
Step 1103 | learning_rate: 6.461143224149005e-05
Step 1103 | epoch: 1.0626204238921002
Step 1104 | loss: 3.5012269020080566
Step 1104 | grad_norm: 2.3511712551116943
Step 1104 | learning_rate: 6.457931920359667e-05
Step 1104 | epoch: 1.0635838150289016
Step 1105 | loss: 3.105464220046997
Step 1105 | grad_norm: 2.0865702629089355
Step 1105 | learning_rate: 6.454720616570327e-05
Step 1105 | epoch: 1.0645472061657033
Step 1106 | loss: 3.602832794189453
Step 1106 | grad_norm: 3.530090570449829
Step 1106 | learning_rate: 6.45150931278099e-05
Step 1106 | epoch: 1.0655105973025047
Step 1107 | loss: 3.796262264251709
Step 1107 | grad_norm: 2.7815017700195312
Step 1107 | learning_rate: 6.448298008991652e-05
Step 1107 | epoch: 1.0664739884393064
Step 1108 | loss: 3.7230091094970703
Step 1108 | grad_norm: 3.0392470359802246
Step 1108 | learning_rate: 6.445086705202312e-05
Step 1108 | epoch: 1.0674373795761078
Step 1109 | loss: 3.0236713886260986
Step 1109 | grad_norm: 2.9923553466796875
Step 1109 | learning_rate: 6.441875401412974e-05
Step 1109 | epoch: 1.0684007707129095
Step 1110 | loss: 3.0399534702301025
Step 1110 | grad_norm: 3.2107958793640137
Step 1110 | learning_rate: 6.438664097623635e-05
Step 1110 | epoch: 1.069364161849711
Step 1111 | loss: 3.611759662628174
Step 1111 | grad_norm: 2.4029300212860107
Step 1111 | learning_rate: 6.435452793834297e-05
Step 1111 | epoch: 1.0703275529865126
Step 1112 | loss: 3.4054758548736572
Step 1112 | grad_norm: 3.445481061935425
Step 1112 | learning_rate: 6.43224149004496e-05
Step 1112 | epoch: 1.071290944123314
Step 1113 | loss: 3.1435258388519287
Step 1113 | grad_norm: 2.648885488510132
Step 1113 | learning_rate: 6.42903018625562e-05
Step 1113 | epoch: 1.0722543352601157
Step 1114 | loss: 3.268547534942627
Step 1114 | grad_norm: 2.192934513092041
Step 1114 | learning_rate: 6.425818882466281e-05
Step 1114 | epoch: 1.0732177263969171
Step 1115 | loss: 3.3465030193328857
Step 1115 | grad_norm: 2.5266175270080566
Step 1115 | learning_rate: 6.422607578676943e-05
Step 1115 | epoch: 1.0741811175337186
Step 1116 | loss: 3.18631649017334
Step 1116 | grad_norm: 2.6757938861846924
Step 1116 | learning_rate: 6.419396274887605e-05
Step 1116 | epoch: 1.0751445086705202
Step 1117 | loss: 3.9657323360443115
Step 1117 | grad_norm: 3.228168487548828
Step 1117 | learning_rate: 6.416184971098266e-05
Step 1117 | epoch: 1.0761078998073217
Step 1118 | loss: 4.1597819328308105
Step 1118 | grad_norm: 5.741883754730225
Step 1118 | learning_rate: 6.412973667308928e-05
Step 1118 | epoch: 1.0770712909441233
Step 1119 | loss: 3.5060298442840576
Step 1119 | grad_norm: 2.5352652072906494
Step 1119 | learning_rate: 6.409762363519589e-05
Step 1119 | epoch: 1.0780346820809248
Step 1120 | loss: 4.28612756729126
Step 1120 | grad_norm: 2.8758325576782227
Step 1120 | learning_rate: 6.406551059730251e-05
Step 1120 | epoch: 1.0789980732177264
Step 1121 | loss: 3.928701400756836
Step 1121 | grad_norm: 2.755256175994873
Step 1121 | learning_rate: 6.403339755940913e-05
Step 1121 | epoch: 1.079961464354528
Step 1122 | loss: 3.4926297664642334
Step 1122 | grad_norm: 2.5676090717315674
Step 1122 | learning_rate: 6.400128452151574e-05
Step 1122 | epoch: 1.0809248554913296
Step 1123 | loss: 3.3367397785186768
Step 1123 | grad_norm: 3.0327794551849365
Step 1123 | learning_rate: 6.396917148362236e-05
Step 1123 | epoch: 1.081888246628131
Step 1124 | loss: 2.9739580154418945
Step 1124 | grad_norm: 2.2010717391967773
Step 1124 | learning_rate: 6.393705844572896e-05
Step 1124 | epoch: 1.0828516377649327
Step 1125 | loss: 3.354858636856079
Step 1125 | grad_norm: 2.3621203899383545
Step 1125 | learning_rate: 6.390494540783558e-05
Step 1125 | epoch: 1.083815028901734
Step 1126 | loss: 2.777369499206543
Step 1126 | grad_norm: 2.2724080085754395
Step 1126 | learning_rate: 6.38728323699422e-05
Step 1126 | epoch: 1.0847784200385355
Step 1127 | loss: 3.4342072010040283
Step 1127 | grad_norm: 2.317309856414795
Step 1127 | learning_rate: 6.384071933204881e-05
Step 1127 | epoch: 1.0857418111753372
Step 1128 | loss: 2.527144193649292
Step 1128 | grad_norm: 2.6206483840942383
Step 1128 | learning_rate: 6.380860629415542e-05
Step 1128 | epoch: 1.0867052023121386
Step 1129 | loss: 2.3045380115509033
Step 1129 | grad_norm: 2.492218255996704
Step 1129 | learning_rate: 6.377649325626206e-05
Step 1129 | epoch: 1.0876685934489403
Step 1130 | loss: 3.314634323120117
Step 1130 | grad_norm: 2.8689043521881104
Step 1130 | learning_rate: 6.374438021836866e-05
Step 1130 | epoch: 1.0886319845857417
Step 1131 | loss: 3.724339246749878
Step 1131 | grad_norm: 2.719395399093628
Step 1131 | learning_rate: 6.371226718047527e-05
Step 1131 | epoch: 1.0895953757225434
Step 1132 | loss: 2.635852336883545
Step 1132 | grad_norm: 2.1108877658843994
Step 1132 | learning_rate: 6.368015414258189e-05
Step 1132 | epoch: 1.0905587668593448
Step 1133 | loss: 3.6333529949188232
Step 1133 | grad_norm: 2.5781171321868896
Step 1133 | learning_rate: 6.36480411046885e-05
Step 1133 | epoch: 1.0915221579961465
Step 1134 | loss: 3.253262758255005
Step 1134 | grad_norm: 1.9686719179153442
Step 1134 | learning_rate: 6.361592806679513e-05
Step 1134 | epoch: 1.092485549132948
Step 1135 | loss: 3.373278856277466
Step 1135 | grad_norm: 3.5472888946533203
Step 1135 | learning_rate: 6.358381502890174e-05
Step 1135 | epoch: 1.0934489402697496
Step 1136 | loss: 3.3506033420562744
Step 1136 | grad_norm: 2.732752799987793
Step 1136 | learning_rate: 6.355170199100835e-05
Step 1136 | epoch: 1.094412331406551
Step 1137 | loss: 3.2761857509613037
Step 1137 | grad_norm: 2.944153308868408
Step 1137 | learning_rate: 6.351958895311497e-05
Step 1137 | epoch: 1.0953757225433527
Step 1138 | loss: 3.118102788925171
Step 1138 | grad_norm: 2.7822325229644775
Step 1138 | learning_rate: 6.348747591522159e-05
Step 1138 | epoch: 1.0963391136801541
Step 1139 | loss: 3.3104071617126465
Step 1139 | grad_norm: 3.2259278297424316
Step 1139 | learning_rate: 6.34553628773282e-05
Step 1139 | epoch: 1.0973025048169558
Step 1140 | loss: 3.156928777694702
Step 1140 | grad_norm: 2.2592780590057373
Step 1140 | learning_rate: 6.342324983943482e-05
Step 1140 | epoch: 1.0982658959537572
Step 1141 | loss: 2.886025905609131
Step 1141 | grad_norm: 1.9996154308319092
Step 1141 | learning_rate: 6.339113680154143e-05
Step 1141 | epoch: 1.0992292870905587
Step 1142 | loss: 3.152066469192505
Step 1142 | grad_norm: 3.1673200130462646
Step 1142 | learning_rate: 6.335902376364805e-05
Step 1142 | epoch: 1.1001926782273603
Step 1143 | loss: 3.8421874046325684
Step 1143 | grad_norm: 2.4255940914154053
Step 1143 | learning_rate: 6.332691072575467e-05
Step 1143 | epoch: 1.1011560693641618
Step 1144 | loss: 3.084261894226074
Step 1144 | grad_norm: 2.6462905406951904
Step 1144 | learning_rate: 6.329479768786127e-05
Step 1144 | epoch: 1.1021194605009634
Step 1145 | loss: 3.466280460357666
Step 1145 | grad_norm: 3.044116258621216
Step 1145 | learning_rate: 6.326268464996788e-05
Step 1145 | epoch: 1.1030828516377649
Step 1146 | loss: 3.3545103073120117
Step 1146 | grad_norm: 2.6500535011291504
Step 1146 | learning_rate: 6.32305716120745e-05
Step 1146 | epoch: 1.1040462427745665
Step 1147 | loss: 3.7095248699188232
Step 1147 | grad_norm: 2.2654783725738525
Step 1147 | learning_rate: 6.319845857418112e-05
Step 1147 | epoch: 1.105009633911368
Step 1148 | loss: 2.687936544418335
Step 1148 | grad_norm: 2.156385660171509
Step 1148 | learning_rate: 6.316634553628775e-05
Step 1148 | epoch: 1.1059730250481696
Step 1149 | loss: 3.735507011413574
Step 1149 | grad_norm: 3.0843756198883057
Step 1149 | learning_rate: 6.313423249839435e-05
Step 1149 | epoch: 1.106936416184971
Step 1150 | loss: 2.99061918258667
Step 1150 | grad_norm: 2.460660457611084
Step 1150 | learning_rate: 6.310211946050096e-05
Step 1150 | epoch: 1.1078998073217727
Step 1151 | loss: 3.442951202392578
Step 1151 | grad_norm: 2.7999911308288574
Step 1151 | learning_rate: 6.307000642260758e-05
Step 1151 | epoch: 1.1088631984585742
Step 1152 | loss: 2.960737943649292
Step 1152 | grad_norm: 2.5577595233917236
Step 1152 | learning_rate: 6.30378933847142e-05
Step 1152 | epoch: 1.1098265895953756
Step 1153 | loss: 3.4331247806549072
Step 1153 | grad_norm: 3.191370725631714
Step 1153 | learning_rate: 6.300578034682081e-05
Step 1153 | epoch: 1.1107899807321773
Step 1154 | loss: 2.761934757232666
Step 1154 | grad_norm: 2.783869504928589
Step 1154 | learning_rate: 6.297366730892743e-05
Step 1154 | epoch: 1.1117533718689787
Step 1155 | loss: 2.793901205062866
Step 1155 | grad_norm: 2.1901233196258545
Step 1155 | learning_rate: 6.294155427103404e-05
Step 1155 | epoch: 1.1127167630057804
Step 1156 | loss: 3.075284719467163
Step 1156 | grad_norm: 3.574681282043457
Step 1156 | learning_rate: 6.290944123314066e-05
Step 1156 | epoch: 1.1136801541425818
Step 1157 | loss: 2.876868486404419
Step 1157 | grad_norm: 2.652254581451416
Step 1157 | learning_rate: 6.287732819524728e-05
Step 1157 | epoch: 1.1146435452793835
Step 1158 | loss: 3.6036431789398193
Step 1158 | grad_norm: 2.7649166584014893
Step 1158 | learning_rate: 6.284521515735389e-05
Step 1158 | epoch: 1.115606936416185
Step 1159 | loss: 3.4089138507843018
Step 1159 | grad_norm: 2.2885448932647705
Step 1159 | learning_rate: 6.28131021194605e-05
Step 1159 | epoch: 1.1165703275529866
Step 1160 | loss: 3.207486629486084
Step 1160 | grad_norm: 3.031209945678711
Step 1160 | learning_rate: 6.278098908156712e-05
Step 1160 | epoch: 1.117533718689788
Step 1161 | loss: 2.8493611812591553
Step 1161 | grad_norm: 1.7541836500167847
Step 1161 | learning_rate: 6.274887604367374e-05
Step 1161 | epoch: 1.1184971098265897
Step 1162 | loss: 4.054068088531494
Step 1162 | grad_norm: 3.045714855194092
Step 1162 | learning_rate: 6.271676300578036e-05
Step 1162 | epoch: 1.1194605009633911
Step 1163 | loss: 3.6150901317596436
Step 1163 | grad_norm: 2.656156539916992
Step 1163 | learning_rate: 6.268464996788696e-05
Step 1163 | epoch: 1.1204238921001928
Step 1164 | loss: 4.451478958129883
Step 1164 | grad_norm: 2.7745468616485596
Step 1164 | learning_rate: 6.265253692999357e-05
Step 1164 | epoch: 1.1213872832369942
Step 1165 | loss: 2.702133893966675
Step 1165 | grad_norm: 2.3890445232391357
Step 1165 | learning_rate: 6.262042389210019e-05
Step 1165 | epoch: 1.1223506743737959
Step 1166 | loss: 3.7969021797180176
Step 1166 | grad_norm: 2.8149824142456055
Step 1166 | learning_rate: 6.258831085420681e-05
Step 1166 | epoch: 1.1233140655105973
Step 1167 | loss: 2.406496524810791
Step 1167 | grad_norm: 2.1635677814483643
Step 1167 | learning_rate: 6.255619781631342e-05
Step 1167 | epoch: 1.1242774566473988
Step 1168 | loss: 3.393069267272949
Step 1168 | grad_norm: 2.862703561782837
Step 1168 | learning_rate: 6.252408477842004e-05
Step 1168 | epoch: 1.1252408477842004
Step 1169 | loss: 3.4469945430755615
Step 1169 | grad_norm: 2.3604495525360107
Step 1169 | learning_rate: 6.249197174052665e-05
Step 1169 | epoch: 1.1262042389210019
Step 1170 | loss: 4.1401591300964355
Step 1170 | grad_norm: 2.4471960067749023
Step 1170 | learning_rate: 6.245985870263327e-05
Step 1170 | epoch: 1.1271676300578035
Step 1171 | loss: 3.5937137603759766
Step 1171 | grad_norm: 2.810997247695923
Step 1171 | learning_rate: 6.242774566473989e-05
Step 1171 | epoch: 1.128131021194605
Step 1172 | loss: 3.325444459915161
Step 1172 | grad_norm: 2.775266170501709
Step 1172 | learning_rate: 6.23956326268465e-05
Step 1172 | epoch: 1.1290944123314066
Step 1173 | loss: 3.0907747745513916
Step 1173 | grad_norm: 2.1721060276031494
Step 1173 | learning_rate: 6.236351958895312e-05
Step 1173 | epoch: 1.130057803468208
Step 1174 | loss: 3.4727299213409424
Step 1174 | grad_norm: 2.825845956802368
Step 1174 | learning_rate: 6.233140655105974e-05
Step 1174 | epoch: 1.1310211946050097
Step 1175 | loss: 3.7464513778686523
Step 1175 | grad_norm: 2.3652145862579346
Step 1175 | learning_rate: 6.229929351316635e-05
Step 1175 | epoch: 1.1319845857418112
Step 1176 | loss: 3.2459557056427
Step 1176 | grad_norm: 2.2489027976989746
Step 1176 | learning_rate: 6.226718047527297e-05
Step 1176 | epoch: 1.1329479768786128
Step 1177 | loss: 3.5703682899475098
Step 1177 | grad_norm: 2.9781908988952637
Step 1177 | learning_rate: 6.223506743737958e-05
Step 1177 | epoch: 1.1339113680154143
Step 1178 | loss: 3.378384828567505
Step 1178 | grad_norm: 2.8020033836364746
Step 1178 | learning_rate: 6.220295439948618e-05
Step 1178 | epoch: 1.1348747591522157
Step 1179 | loss: 3.0615997314453125
Step 1179 | grad_norm: 2.4809794425964355
Step 1179 | learning_rate: 6.217084136159282e-05
Step 1179 | epoch: 1.1358381502890174
Step 1180 | loss: 3.2780938148498535
Step 1180 | grad_norm: 2.93223237991333
Step 1180 | learning_rate: 6.213872832369943e-05
Step 1180 | epoch: 1.1368015414258188
Step 1181 | loss: 3.456010580062866
Step 1181 | grad_norm: 2.8967409133911133
Step 1181 | learning_rate: 6.210661528580603e-05
Step 1181 | epoch: 1.1377649325626205
Step 1182 | loss: 3.2647531032562256
Step 1182 | grad_norm: 2.7230451107025146
Step 1182 | learning_rate: 6.207450224791265e-05
Step 1182 | epoch: 1.138728323699422
Step 1183 | loss: 3.4871387481689453
Step 1183 | grad_norm: 2.259286880493164
Step 1183 | learning_rate: 6.204238921001928e-05
Step 1183 | epoch: 1.1396917148362236
Step 1184 | loss: 3.396430015563965
Step 1184 | grad_norm: 3.2994375228881836
Step 1184 | learning_rate: 6.201027617212588e-05
Step 1184 | epoch: 1.140655105973025
Step 1185 | loss: 2.5289981365203857
Step 1185 | grad_norm: 2.222303867340088
Step 1185 | learning_rate: 6.19781631342325e-05
Step 1185 | epoch: 1.1416184971098267
Step 1186 | loss: 2.570204973220825
Step 1186 | grad_norm: 1.977569580078125
Step 1186 | learning_rate: 6.194605009633911e-05
Step 1186 | epoch: 1.142581888246628
Step 1187 | loss: 3.411877155303955
Step 1187 | grad_norm: 3.100898504257202
Step 1187 | learning_rate: 6.191393705844573e-05
Step 1187 | epoch: 1.1435452793834298
Step 1188 | loss: 3.7988383769989014
Step 1188 | grad_norm: 2.5039830207824707
Step 1188 | learning_rate: 6.188182402055235e-05
Step 1188 | epoch: 1.1445086705202312
Step 1189 | loss: 3.1023752689361572
Step 1189 | grad_norm: 2.3570170402526855
Step 1189 | learning_rate: 6.184971098265896e-05
Step 1189 | epoch: 1.1454720616570326
Step 1190 | loss: 3.738492488861084
Step 1190 | grad_norm: 2.779013156890869
Step 1190 | learning_rate: 6.181759794476558e-05
Step 1190 | epoch: 1.1464354527938343
Step 1191 | loss: 3.2932190895080566
Step 1191 | grad_norm: 2.180246353149414
Step 1191 | learning_rate: 6.178548490687219e-05
Step 1191 | epoch: 1.147398843930636
Step 1192 | loss: 3.1446914672851562
Step 1192 | grad_norm: 4.887776851654053
Step 1192 | learning_rate: 6.175337186897881e-05
Step 1192 | epoch: 1.1483622350674374
Step 1193 | loss: 3.7513949871063232
Step 1193 | grad_norm: 3.811022996902466
Step 1193 | learning_rate: 6.172125883108543e-05
Step 1193 | epoch: 1.1493256262042388
Step 1194 | loss: 2.6559512615203857
Step 1194 | grad_norm: 2.0133304595947266
Step 1194 | learning_rate: 6.168914579319204e-05
Step 1194 | epoch: 1.1502890173410405
Step 1195 | loss: 3.0165154933929443
Step 1195 | grad_norm: 2.163083076477051
Step 1195 | learning_rate: 6.165703275529865e-05
Step 1195 | epoch: 1.151252408477842
Step 1196 | loss: 3.1861679553985596
Step 1196 | grad_norm: 2.572648763656616
Step 1196 | learning_rate: 6.162491971740527e-05
Step 1196 | epoch: 1.1522157996146436
Step 1197 | loss: 3.689410924911499
Step 1197 | grad_norm: 2.915677547454834
Step 1197 | learning_rate: 6.159280667951189e-05
Step 1197 | epoch: 1.153179190751445
Step 1198 | loss: 2.9565718173980713
Step 1198 | grad_norm: 2.7505171298980713
Step 1198 | learning_rate: 6.156069364161851e-05
Step 1198 | epoch: 1.1541425818882467
Step 1199 | loss: 3.8519957065582275
Step 1199 | grad_norm: 2.4483160972595215
Step 1199 | learning_rate: 6.152858060372512e-05
Step 1199 | epoch: 1.1551059730250481
Step 1200 | loss: 2.8089261054992676
Step 1200 | grad_norm: 2.6759164333343506
Step 1200 | learning_rate: 6.149646756583172e-05
Step 1200 | epoch: 1.1560693641618498
Step 1201 | loss: 3.3967292308807373
Step 1201 | grad_norm: 2.442763090133667
Step 1201 | learning_rate: 6.146435452793834e-05
Step 1201 | epoch: 1.1570327552986512
Step 1202 | loss: 3.339656352996826
Step 1202 | grad_norm: 2.106529474258423
Step 1202 | learning_rate: 6.143224149004497e-05
Step 1202 | epoch: 1.157996146435453
Step 1203 | loss: 3.215452194213867
Step 1203 | grad_norm: 2.9303624629974365
Step 1203 | learning_rate: 6.140012845215157e-05
Step 1203 | epoch: 1.1589595375722543
Step 1204 | loss: 3.0674760341644287
Step 1204 | grad_norm: 2.482196092605591
Step 1204 | learning_rate: 6.13680154142582e-05
Step 1204 | epoch: 1.1599229287090558
Step 1205 | loss: 3.6467835903167725
Step 1205 | grad_norm: 2.361008882522583
Step 1205 | learning_rate: 6.13359023763648e-05
Step 1205 | epoch: 1.1608863198458574
Step 1206 | loss: 3.0993425846099854
Step 1206 | grad_norm: 3.2032318115234375
Step 1206 | learning_rate: 6.130378933847142e-05
Step 1206 | epoch: 1.1618497109826589
Step 1207 | loss: 3.2842721939086914
Step 1207 | grad_norm: 2.7301509380340576
Step 1207 | learning_rate: 6.127167630057804e-05
Step 1207 | epoch: 1.1628131021194605
Step 1208 | loss: 3.3185853958129883
Step 1208 | grad_norm: 3.513291597366333
Step 1208 | learning_rate: 6.123956326268465e-05
Step 1208 | epoch: 1.163776493256262
Step 1209 | loss: 3.7051143646240234
Step 1209 | grad_norm: 2.588196039199829
Step 1209 | learning_rate: 6.120745022479126e-05
Step 1209 | epoch: 1.1647398843930636
Step 1210 | loss: 3.150092363357544
Step 1210 | grad_norm: 2.286081075668335
Step 1210 | learning_rate: 6.117533718689788e-05
Step 1210 | epoch: 1.165703275529865
Step 1211 | loss: 3.314876079559326
Step 1211 | grad_norm: 3.9455950260162354
Step 1211 | learning_rate: 6.11432241490045e-05
Step 1211 | epoch: 1.1666666666666667
Step 1212 | loss: 4.024413108825684
Step 1212 | grad_norm: 3.441781997680664
Step 1212 | learning_rate: 6.111111111111112e-05
Step 1212 | epoch: 1.1676300578034682
Step 1213 | loss: 3.5235981941223145
Step 1213 | grad_norm: 4.389991283416748
Step 1213 | learning_rate: 6.107899807321773e-05
Step 1213 | epoch: 1.1685934489402698
Step 1214 | loss: 3.542766571044922
Step 1214 | grad_norm: 2.500680685043335
Step 1214 | learning_rate: 6.104688503532434e-05
Step 1214 | epoch: 1.1695568400770713
Step 1215 | loss: 3.803946018218994
Step 1215 | grad_norm: 4.397311687469482
Step 1215 | learning_rate: 6.101477199743096e-05
Step 1215 | epoch: 1.1705202312138727
Step 1216 | loss: 3.756793975830078
Step 1216 | grad_norm: 2.6201672554016113
Step 1216 | learning_rate: 6.098265895953758e-05
Step 1216 | epoch: 1.1714836223506744
Step 1217 | loss: 4.416192531585693
Step 1217 | grad_norm: 2.7738730907440186
Step 1217 | learning_rate: 6.0950545921644185e-05
Step 1217 | epoch: 1.1724470134874758
Step 1218 | loss: 2.909552574157715
Step 1218 | grad_norm: 3.878594398498535
Step 1218 | learning_rate: 6.0918432883750806e-05
Step 1218 | epoch: 1.1734104046242775
Step 1219 | loss: 2.818946599960327
Step 1219 | grad_norm: 2.528796911239624
Step 1219 | learning_rate: 6.088631984585742e-05
Step 1219 | epoch: 1.174373795761079
Step 1220 | loss: 2.681525945663452
Step 1220 | grad_norm: 2.6114938259124756
Step 1220 | learning_rate: 6.0854206807964034e-05
Step 1220 | epoch: 1.1753371868978806
Step 1221 | loss: 3.4229466915130615
Step 1221 | grad_norm: 3.019080400466919
Step 1221 | learning_rate: 6.0822093770070655e-05
Step 1221 | epoch: 1.176300578034682
Step 1222 | loss: 3.301577568054199
Step 1222 | grad_norm: 2.426348924636841
Step 1222 | learning_rate: 6.078998073217726e-05
Step 1222 | epoch: 1.1772639691714837
Step 1223 | loss: 3.4731690883636475
Step 1223 | grad_norm: 2.2023401260375977
Step 1223 | learning_rate: 6.075786769428389e-05
Step 1223 | epoch: 1.1782273603082851
Step 1224 | loss: 3.361410140991211
Step 1224 | grad_norm: 2.615900993347168
Step 1224 | learning_rate: 6.07257546563905e-05
Step 1224 | epoch: 1.1791907514450868
Step 1225 | loss: 3.7752480506896973
Step 1225 | grad_norm: 2.4640045166015625
Step 1225 | learning_rate: 6.069364161849711e-05
Step 1225 | epoch: 1.1801541425818882
Step 1226 | loss: 3.337618112564087
Step 1226 | grad_norm: 2.777146339416504
Step 1226 | learning_rate: 6.066152858060373e-05
Step 1226 | epoch: 1.1811175337186899
Step 1227 | loss: 2.6572749614715576
Step 1227 | grad_norm: 2.566419839859009
Step 1227 | learning_rate: 6.062941554271035e-05
Step 1227 | epoch: 1.1820809248554913
Step 1228 | loss: 3.291749954223633
Step 1228 | grad_norm: 3.6440281867980957
Step 1228 | learning_rate: 6.0597302504816955e-05
Step 1228 | epoch: 1.183044315992293
Step 1229 | loss: 4.295148849487305
Step 1229 | grad_norm: 3.457557201385498
Step 1229 | learning_rate: 6.0565189466923576e-05
Step 1229 | epoch: 1.1840077071290944
Step 1230 | loss: 3.899216890335083
Step 1230 | grad_norm: 3.0250332355499268
Step 1230 | learning_rate: 6.053307642903019e-05
Step 1230 | epoch: 1.1849710982658959
Step 1231 | loss: 2.9480984210968018
Step 1231 | grad_norm: 2.6081392765045166
Step 1231 | learning_rate: 6.05009633911368e-05
Step 1231 | epoch: 1.1859344894026975
Step 1232 | loss: 3.100604772567749
Step 1232 | grad_norm: 3.127556085586548
Step 1232 | learning_rate: 6.0468850353243425e-05
Step 1232 | epoch: 1.186897880539499
Step 1233 | loss: 3.764369487762451
Step 1233 | grad_norm: 2.5752785205841064
Step 1233 | learning_rate: 6.043673731535003e-05
Step 1233 | epoch: 1.1878612716763006
Step 1234 | loss: 3.6015586853027344
Step 1234 | grad_norm: 3.0454723834991455
Step 1234 | learning_rate: 6.0404624277456646e-05
Step 1234 | epoch: 1.188824662813102
Step 1235 | loss: 3.6001272201538086
Step 1235 | grad_norm: 2.881087064743042
Step 1235 | learning_rate: 6.037251123956327e-05
Step 1235 | epoch: 1.1897880539499037
Step 1236 | loss: 3.2704732418060303
Step 1236 | grad_norm: 2.7560482025146484
Step 1236 | learning_rate: 6.034039820166988e-05
Step 1236 | epoch: 1.1907514450867052
Step 1237 | loss: 3.061596393585205
Step 1237 | grad_norm: 2.6006968021392822
Step 1237 | learning_rate: 6.03082851637765e-05
Step 1237 | epoch: 1.1917148362235068
Step 1238 | loss: 3.8673038482666016
Step 1238 | grad_norm: 2.5510997772216797
Step 1238 | learning_rate: 6.027617212588311e-05
Step 1238 | epoch: 1.1926782273603083
Step 1239 | loss: 2.4477739334106445
Step 1239 | grad_norm: 2.3854660987854004
Step 1239 | learning_rate: 6.0244059087989724e-05
Step 1239 | epoch: 1.19364161849711
Step 1240 | loss: 3.6320011615753174
Step 1240 | grad_norm: 3.1770477294921875
Step 1240 | learning_rate: 6.0211946050096345e-05
Step 1240 | epoch: 1.1946050096339114
Step 1241 | loss: 3.7709295749664307
Step 1241 | grad_norm: 2.8415751457214355
Step 1241 | learning_rate: 6.017983301220296e-05
Step 1241 | epoch: 1.1955684007707128
Step 1242 | loss: 2.969268560409546
Step 1242 | grad_norm: 2.1538310050964355
Step 1242 | learning_rate: 6.014771997430957e-05
Step 1242 | epoch: 1.1965317919075145
Step 1243 | loss: 3.155438184738159
Step 1243 | grad_norm: 2.9756500720977783
Step 1243 | learning_rate: 6.0115606936416195e-05
Step 1243 | epoch: 1.197495183044316
Step 1244 | loss: 3.3685922622680664
Step 1244 | grad_norm: 2.8132100105285645
Step 1244 | learning_rate: 6.00834938985228e-05
Step 1244 | epoch: 1.1984585741811176
Step 1245 | loss: 3.645843744277954
Step 1245 | grad_norm: 3.0771589279174805
Step 1245 | learning_rate: 6.0051380860629416e-05
Step 1245 | epoch: 1.199421965317919
Step 1246 | loss: 3.8949320316314697
Step 1246 | grad_norm: 2.634744882583618
Step 1246 | learning_rate: 6.001926782273604e-05
Step 1246 | epoch: 1.2003853564547207
Step 1247 | loss: 3.1727657318115234
Step 1247 | grad_norm: 3.375030517578125
Step 1247 | learning_rate: 5.9987154784842644e-05
Step 1247 | epoch: 1.201348747591522
Step 1248 | loss: 3.720146656036377
Step 1248 | grad_norm: 2.7877604961395264
Step 1248 | learning_rate: 5.995504174694927e-05
Step 1248 | epoch: 1.2023121387283238
Step 1249 | loss: 2.7702691555023193
Step 1249 | grad_norm: 2.2837648391723633
Step 1249 | learning_rate: 5.992292870905588e-05
Step 1249 | epoch: 1.2032755298651252
Step 1250 | loss: 2.7826480865478516
Step 1250 | grad_norm: 2.3772494792938232
Step 1250 | learning_rate: 5.9890815671162494e-05
Step 1250 | epoch: 1.2042389210019269
Step 1251 | loss: 2.77911376953125
Step 1251 | grad_norm: 3.02485728263855
Step 1251 | learning_rate: 5.9858702633269115e-05
Step 1251 | epoch: 1.2052023121387283
Step 1252 | loss: 3.3535521030426025
Step 1252 | grad_norm: 3.278568983078003
Step 1252 | learning_rate: 5.982658959537573e-05
Step 1252 | epoch: 1.2061657032755297
Step 1253 | loss: 3.429225206375122
Step 1253 | grad_norm: 2.6341447830200195
Step 1253 | learning_rate: 5.9794476557482336e-05
Step 1253 | epoch: 1.2071290944123314
Step 1254 | loss: 3.6690971851348877
Step 1254 | grad_norm: 2.9711694717407227
Step 1254 | learning_rate: 5.976236351958896e-05
Step 1254 | epoch: 1.208092485549133
Step 1255 | loss: 3.366575002670288
Step 1255 | grad_norm: 3.433262348175049
Step 1255 | learning_rate: 5.973025048169557e-05
Step 1255 | epoch: 1.2090558766859345
Step 1256 | loss: 3.706394910812378
Step 1256 | grad_norm: 2.635138750076294
Step 1256 | learning_rate: 5.969813744380218e-05
Step 1256 | epoch: 1.210019267822736
Step 1257 | loss: 3.7560312747955322
Step 1257 | grad_norm: 3.046539783477783
Step 1257 | learning_rate: 5.966602440590881e-05
Step 1257 | epoch: 1.2109826589595376
Step 1258 | loss: 3.684523105621338
Step 1258 | grad_norm: 2.839423179626465
Step 1258 | learning_rate: 5.9633911368015414e-05
Step 1258 | epoch: 1.211946050096339
Step 1259 | loss: 3.7435548305511475
Step 1259 | grad_norm: 3.2837822437286377
Step 1259 | learning_rate: 5.960179833012203e-05
Step 1259 | epoch: 1.2129094412331407
Step 1260 | loss: 3.521408796310425
Step 1260 | grad_norm: 2.9117918014526367
Step 1260 | learning_rate: 5.956968529222865e-05
Step 1260 | epoch: 1.2138728323699421
Step 1261 | loss: 3.0063562393188477
Step 1261 | grad_norm: 2.923811197280884
Step 1261 | learning_rate: 5.9537572254335263e-05
Step 1261 | epoch: 1.2148362235067438
Step 1262 | loss: 3.480963945388794
Step 1262 | grad_norm: 2.3472583293914795
Step 1262 | learning_rate: 5.9505459216441884e-05
Step 1262 | epoch: 1.2157996146435452
Step 1263 | loss: 2.784879446029663
Step 1263 | grad_norm: 1.9979708194732666
Step 1263 | learning_rate: 5.947334617854849e-05
Step 1263 | epoch: 1.216763005780347
Step 1264 | loss: 3.0964276790618896
Step 1264 | grad_norm: 2.477086305618286
Step 1264 | learning_rate: 5.9441233140655106e-05
Step 1264 | epoch: 1.2177263969171483
Step 1265 | loss: 3.613137722015381
Step 1265 | grad_norm: 3.4703242778778076
Step 1265 | learning_rate: 5.940912010276173e-05
Step 1265 | epoch: 1.21868978805395
Step 1266 | loss: 2.666767120361328
Step 1266 | grad_norm: 2.4341883659362793
Step 1266 | learning_rate: 5.937700706486834e-05
Step 1266 | epoch: 1.2196531791907514
Step 1267 | loss: 2.2514808177948
Step 1267 | grad_norm: 2.235764980316162
Step 1267 | learning_rate: 5.934489402697495e-05
Step 1267 | epoch: 1.2206165703275529
Step 1268 | loss: 3.743544101715088
Step 1268 | grad_norm: 2.956876754760742
Step 1268 | learning_rate: 5.9312780989081576e-05
Step 1268 | epoch: 1.2215799614643545
Step 1269 | loss: 3.5336804389953613
Step 1269 | grad_norm: 2.868779182434082
Step 1269 | learning_rate: 5.9280667951188184e-05
Step 1269 | epoch: 1.222543352601156
Step 1270 | loss: 3.9304933547973633
Step 1270 | grad_norm: 2.969628095626831
Step 1270 | learning_rate: 5.92485549132948e-05
Step 1270 | epoch: 1.2235067437379576
Step 1271 | loss: 2.78214955329895
Step 1271 | grad_norm: 2.2717044353485107
Step 1271 | learning_rate: 5.921644187540142e-05
Step 1271 | epoch: 1.224470134874759
Step 1272 | loss: 2.9179863929748535
Step 1272 | grad_norm: 2.0613136291503906
Step 1272 | learning_rate: 5.9184328837508026e-05
Step 1272 | epoch: 1.2254335260115607
Step 1273 | loss: 4.112709045410156
Step 1273 | grad_norm: 3.0700669288635254
Step 1273 | learning_rate: 5.915221579961464e-05
Step 1273 | epoch: 1.2263969171483622
Step 1274 | loss: 4.001229286193848
Step 1274 | grad_norm: 3.100628614425659
Step 1274 | learning_rate: 5.912010276172126e-05
Step 1274 | epoch: 1.2273603082851638
Step 1275 | loss: 3.461088180541992
Step 1275 | grad_norm: 2.497576951980591
Step 1275 | learning_rate: 5.9087989723827876e-05
Step 1275 | epoch: 1.2283236994219653
Step 1276 | loss: 3.697824239730835
Step 1276 | grad_norm: 3.434866428375244
Step 1276 | learning_rate: 5.9055876685934497e-05
Step 1276 | epoch: 1.229287090558767
Step 1277 | loss: 3.319666624069214
Step 1277 | grad_norm: 2.1845216751098633
Step 1277 | learning_rate: 5.902376364804111e-05
Step 1277 | epoch: 1.2302504816955684
Step 1278 | loss: 2.995955467224121
Step 1278 | grad_norm: 3.5746073722839355
Step 1278 | learning_rate: 5.899165061014772e-05
Step 1278 | epoch: 1.2312138728323698
Step 1279 | loss: 2.3398492336273193
Step 1279 | grad_norm: 2.7647275924682617
Step 1279 | learning_rate: 5.895953757225434e-05
Step 1279 | epoch: 1.2321772639691715
Step 1280 | loss: 3.120790481567383
Step 1280 | grad_norm: 3.0727531909942627
Step 1280 | learning_rate: 5.892742453436095e-05
Step 1280 | epoch: 1.2331406551059731
Step 1281 | loss: 3.582047700881958
Step 1281 | grad_norm: 3.372913122177124
Step 1281 | learning_rate: 5.889531149646757e-05
Step 1281 | epoch: 1.2341040462427746
Step 1282 | loss: 3.3708536624908447
Step 1282 | grad_norm: 3.694977283477783
Step 1282 | learning_rate: 5.886319845857419e-05
Step 1282 | epoch: 1.235067437379576
Step 1283 | loss: 3.7312989234924316
Step 1283 | grad_norm: 3.261626720428467
Step 1283 | learning_rate: 5.8831085420680796e-05
Step 1283 | epoch: 1.2360308285163777
Step 1284 | loss: 2.1621057987213135
Step 1284 | grad_norm: 2.3225224018096924
Step 1284 | learning_rate: 5.879897238278741e-05
Step 1284 | epoch: 1.2369942196531791
Step 1285 | loss: 4.180283069610596
Step 1285 | grad_norm: 3.554917335510254
Step 1285 | learning_rate: 5.876685934489403e-05
Step 1285 | epoch: 1.2379576107899808
Step 1286 | loss: 3.0759031772613525
Step 1286 | grad_norm: 3.1524715423583984
Step 1286 | learning_rate: 5.8734746307000645e-05
Step 1286 | epoch: 1.2389210019267822
Step 1287 | loss: 3.3148062229156494
Step 1287 | grad_norm: 2.5278167724609375
Step 1287 | learning_rate: 5.8702633269107266e-05
Step 1287 | epoch: 1.239884393063584
Step 1288 | loss: 3.492626905441284
Step 1288 | grad_norm: 2.499621868133545
Step 1288 | learning_rate: 5.8670520231213874e-05
Step 1288 | epoch: 1.2408477842003853
Step 1289 | loss: 2.913665533065796
Step 1289 | grad_norm: 2.1714136600494385
Step 1289 | learning_rate: 5.863840719332049e-05
Step 1289 | epoch: 1.241811175337187
Step 1290 | loss: 2.968639373779297
Step 1290 | grad_norm: 2.0541155338287354
Step 1290 | learning_rate: 5.860629415542711e-05
Step 1290 | epoch: 1.2427745664739884
Step 1291 | loss: 3.3741488456726074
Step 1291 | grad_norm: 2.205026865005493
Step 1291 | learning_rate: 5.857418111753372e-05
Step 1291 | epoch: 1.24373795761079
Step 1292 | loss: 3.5208561420440674
Step 1292 | grad_norm: 2.5224390029907227
Step 1292 | learning_rate: 5.854206807964033e-05
Step 1292 | epoch: 1.2447013487475915
Step 1293 | loss: 3.3994152545928955
Step 1293 | grad_norm: 3.288095235824585
Step 1293 | learning_rate: 5.850995504174696e-05
Step 1293 | epoch: 1.245664739884393
Step 1294 | loss: 2.87546706199646
Step 1294 | grad_norm: 2.450178623199463
Step 1294 | learning_rate: 5.8477842003853566e-05
Step 1294 | epoch: 1.2466281310211946
Step 1295 | loss: 3.1331138610839844
Step 1295 | grad_norm: 3.6189935207366943
Step 1295 | learning_rate: 5.844572896596018e-05
Step 1295 | epoch: 1.247591522157996
Step 1296 | loss: 2.770836591720581
Step 1296 | grad_norm: 2.7538387775421143
Step 1296 | learning_rate: 5.84136159280668e-05
Step 1296 | epoch: 1.2485549132947977
Step 1297 | loss: 2.9093422889709473
Step 1297 | grad_norm: 1.957544207572937
Step 1297 | learning_rate: 5.8381502890173415e-05
Step 1297 | epoch: 1.2495183044315992
Step 1298 | loss: 3.3847408294677734
Step 1298 | grad_norm: 2.7020151615142822
Step 1298 | learning_rate: 5.834938985228002e-05
Step 1298 | epoch: 1.2504816955684008
Step 1299 | loss: 3.375152587890625
Step 1299 | grad_norm: 3.235339403152466
Step 1299 | learning_rate: 5.831727681438664e-05
Step 1299 | epoch: 1.2514450867052023
Step 1300 | loss: 3.364497423171997
Step 1300 | grad_norm: 3.1554176807403564
Step 1300 | learning_rate: 5.828516377649326e-05
Step 1300 | epoch: 1.252408477842004
Step 1301 | loss: 2.7002575397491455
Step 1301 | grad_norm: 2.6383585929870605
Step 1301 | learning_rate: 5.825305073859988e-05
Step 1301 | epoch: 1.2533718689788054
Step 1302 | loss: 4.80570650100708
Step 1302 | grad_norm: 2.954192638397217
Step 1302 | learning_rate: 5.822093770070649e-05
Step 1302 | epoch: 1.254335260115607
Step 1303 | loss: 3.632014751434326
Step 1303 | grad_norm: 2.97094464302063
Step 1303 | learning_rate: 5.81888246628131e-05
Step 1303 | epoch: 1.2552986512524085
Step 1304 | loss: 3.725878953933716
Step 1304 | grad_norm: 3.0409252643585205
Step 1304 | learning_rate: 5.815671162491972e-05
Step 1304 | epoch: 1.25626204238921
Step 1305 | loss: 3.779920816421509
Step 1305 | grad_norm: 3.4657785892486572
Step 1305 | learning_rate: 5.8124598587026335e-05
Step 1305 | epoch: 1.2572254335260116
Step 1306 | loss: 3.197019577026367
Step 1306 | grad_norm: 4.791702747344971
Step 1306 | learning_rate: 5.809248554913295e-05
Step 1306 | epoch: 1.2581888246628132
Step 1307 | loss: 3.9309918880462646
Step 1307 | grad_norm: 2.8134729862213135
Step 1307 | learning_rate: 5.806037251123957e-05
Step 1307 | epoch: 1.2591522157996147
Step 1308 | loss: 2.896104097366333
Step 1308 | grad_norm: 2.4393310546875
Step 1308 | learning_rate: 5.802825947334618e-05
Step 1308 | epoch: 1.260115606936416
Step 1309 | loss: 3.6331944465637207
Step 1309 | grad_norm: 3.080538749694824
Step 1309 | learning_rate: 5.799614643545279e-05
Step 1309 | epoch: 1.2610789980732178
Step 1310 | loss: 3.779491424560547
Step 1310 | grad_norm: 3.1359434127807617
Step 1310 | learning_rate: 5.796403339755941e-05
Step 1310 | epoch: 1.2620423892100192
Step 1311 | loss: 3.3276901245117188
Step 1311 | grad_norm: 2.603748321533203
Step 1311 | learning_rate: 5.793192035966603e-05
Step 1311 | epoch: 1.2630057803468209
Step 1312 | loss: 3.0793538093566895
Step 1312 | grad_norm: 2.7281761169433594
Step 1312 | learning_rate: 5.789980732177265e-05
Step 1312 | epoch: 1.2639691714836223
Step 1313 | loss: 3.723240375518799
Step 1313 | grad_norm: 3.0226564407348633
Step 1313 | learning_rate: 5.786769428387926e-05
Step 1313 | epoch: 1.264932562620424
Step 1314 | loss: 3.564986228942871
Step 1314 | grad_norm: 2.3327386379241943
Step 1314 | learning_rate: 5.783558124598587e-05
Step 1314 | epoch: 1.2658959537572254
Step 1315 | loss: 3.442823648452759
Step 1315 | grad_norm: 3.129307746887207
Step 1315 | learning_rate: 5.780346820809249e-05
Step 1315 | epoch: 1.2668593448940269
Step 1316 | loss: 3.2253201007843018
Step 1316 | grad_norm: 5.027240753173828
Step 1316 | learning_rate: 5.7771355170199105e-05
Step 1316 | epoch: 1.2678227360308285
Step 1317 | loss: 3.090322971343994
Step 1317 | grad_norm: 2.159019708633423
Step 1317 | learning_rate: 5.773924213230571e-05
Step 1317 | epoch: 1.2687861271676302
Step 1318 | loss: 3.543308973312378
Step 1318 | grad_norm: 3.62188458442688
Step 1318 | learning_rate: 5.770712909441234e-05
Step 1318 | epoch: 1.2697495183044316
Step 1319 | loss: 2.656848669052124
Step 1319 | grad_norm: 2.7362115383148193
Step 1319 | learning_rate: 5.767501605651895e-05
Step 1319 | epoch: 1.270712909441233
Step 1320 | loss: 3.216606616973877
Step 1320 | grad_norm: 2.5697174072265625
Step 1320 | learning_rate: 5.764290301862556e-05
Step 1320 | epoch: 1.2716763005780347
Step 1321 | loss: 3.9086625576019287
Step 1321 | grad_norm: 3.4613142013549805
Step 1321 | learning_rate: 5.761078998073218e-05
Step 1321 | epoch: 1.2726396917148362
Step 1322 | loss: 3.9481306076049805
Step 1322 | grad_norm: 3.5296590328216553
Step 1322 | learning_rate: 5.75786769428388e-05
Step 1322 | epoch: 1.2736030828516378
Step 1323 | loss: 3.804395914077759
Step 1323 | grad_norm: 2.929678201675415
Step 1323 | learning_rate: 5.7546563904945404e-05
Step 1323 | epoch: 1.2745664739884393
Step 1324 | loss: 3.1869957447052
Step 1324 | grad_norm: 3.0381457805633545
Step 1324 | learning_rate: 5.7514450867052025e-05
Step 1324 | epoch: 1.275529865125241
Step 1325 | loss: 3.9308927059173584
Step 1325 | grad_norm: 2.8525352478027344
Step 1325 | learning_rate: 5.748233782915864e-05
Step 1325 | epoch: 1.2764932562620424
Step 1326 | loss: 3.451786756515503
Step 1326 | grad_norm: 2.950822591781616
Step 1326 | learning_rate: 5.745022479126526e-05
Step 1326 | epoch: 1.2774566473988438
Step 1327 | loss: 3.2030341625213623
Step 1327 | grad_norm: 2.668574810028076
Step 1327 | learning_rate: 5.7418111753371874e-05
Step 1327 | epoch: 1.2784200385356455
Step 1328 | loss: 2.8773913383483887
Step 1328 | grad_norm: 2.5077571868896484
Step 1328 | learning_rate: 5.738599871547848e-05
Step 1328 | epoch: 1.2793834296724471
Step 1329 | loss: 3.1384077072143555
Step 1329 | grad_norm: 2.3111867904663086
Step 1329 | learning_rate: 5.735388567758511e-05
Step 1329 | epoch: 1.2803468208092486
Step 1330 | loss: 3.0391530990600586
Step 1330 | grad_norm: 2.26147198677063
Step 1330 | learning_rate: 5.732177263969172e-05
Step 1330 | epoch: 1.28131021194605
Step 1331 | loss: 2.9833998680114746
Step 1331 | grad_norm: 2.7041871547698975
Step 1331 | learning_rate: 5.728965960179833e-05
Step 1331 | epoch: 1.2822736030828517
Step 1332 | loss: 3.4053773880004883
Step 1332 | grad_norm: 2.7051639556884766
Step 1332 | learning_rate: 5.725754656390495e-05
Step 1332 | epoch: 1.2832369942196533
Step 1333 | loss: 3.303389310836792
Step 1333 | grad_norm: 2.8029191493988037
Step 1333 | learning_rate: 5.722543352601156e-05
Step 1333 | epoch: 1.2842003853564548
Step 1334 | loss: 3.768908977508545
Step 1334 | grad_norm: 3.457648277282715
Step 1334 | learning_rate: 5.7193320488118174e-05
Step 1334 | epoch: 1.2851637764932562
Step 1335 | loss: 3.6494927406311035
Step 1335 | grad_norm: 2.8333852291107178
Step 1335 | learning_rate: 5.7161207450224795e-05
Step 1335 | epoch: 1.2861271676300579
Step 1336 | loss: 3.325188636779785
Step 1336 | grad_norm: 2.9685287475585938
Step 1336 | learning_rate: 5.712909441233141e-05
Step 1336 | epoch: 1.2870905587668593
Step 1337 | loss: 3.317073345184326
Step 1337 | grad_norm: 2.139857769012451
Step 1337 | learning_rate: 5.709698137443803e-05
Step 1337 | epoch: 1.288053949903661
Step 1338 | loss: 3.3656187057495117
Step 1338 | grad_norm: 2.6598193645477295
Step 1338 | learning_rate: 5.7064868336544644e-05
Step 1338 | epoch: 1.2890173410404624
Step 1339 | loss: 4.257342338562012
Step 1339 | grad_norm: 2.9175007343292236
Step 1339 | learning_rate: 5.703275529865125e-05
Step 1339 | epoch: 1.289980732177264
Step 1340 | loss: 3.1583316326141357
Step 1340 | grad_norm: 3.7597038745880127
Step 1340 | learning_rate: 5.700064226075787e-05
Step 1340 | epoch: 1.2909441233140655
Step 1341 | loss: 3.9808127880096436
Step 1341 | grad_norm: 3.1468474864959717
Step 1341 | learning_rate: 5.6968529222864487e-05
Step 1341 | epoch: 1.291907514450867
Step 1342 | loss: 3.224318265914917
Step 1342 | grad_norm: 2.410043239593506
Step 1342 | learning_rate: 5.69364161849711e-05
Step 1342 | epoch: 1.2928709055876686
Step 1343 | loss: 3.4104807376861572
Step 1343 | grad_norm: 2.1979851722717285
Step 1343 | learning_rate: 5.690430314707772e-05
Step 1343 | epoch: 1.2938342967244703
Step 1344 | loss: 3.3965883255004883
Step 1344 | grad_norm: 2.803900957107544
Step 1344 | learning_rate: 5.687219010918433e-05
Step 1344 | epoch: 1.2947976878612717
Step 1345 | loss: 3.173860549926758
Step 1345 | grad_norm: 2.492659330368042
Step 1345 | learning_rate: 5.684007707129094e-05
Step 1345 | epoch: 1.2957610789980731
Step 1346 | loss: 3.2869250774383545
Step 1346 | grad_norm: 2.603337526321411
Step 1346 | learning_rate: 5.6807964033397564e-05
Step 1346 | epoch: 1.2967244701348748
Step 1347 | loss: 4.018346309661865
Step 1347 | grad_norm: 3.2396833896636963
Step 1347 | learning_rate: 5.677585099550418e-05
Step 1347 | epoch: 1.2976878612716762
Step 1348 | loss: 2.9661800861358643
Step 1348 | grad_norm: 3.0379068851470947
Step 1348 | learning_rate: 5.6743737957610786e-05
Step 1348 | epoch: 1.298651252408478
Step 1349 | loss: 3.3120009899139404
Step 1349 | grad_norm: 2.909188985824585
Step 1349 | learning_rate: 5.671162491971741e-05
Step 1349 | epoch: 1.2996146435452793
Step 1350 | loss: 3.49180006980896
Step 1350 | grad_norm: 3.24988055229187
Step 1350 | learning_rate: 5.667951188182402e-05
Step 1350 | epoch: 1.300578034682081
Step 1351 | loss: 3.729881763458252
Step 1351 | grad_norm: 2.6783835887908936
Step 1351 | learning_rate: 5.664739884393064e-05
Step 1351 | epoch: 1.3015414258188824
Step 1352 | loss: 3.2365283966064453
Step 1352 | grad_norm: 2.8770320415496826
Step 1352 | learning_rate: 5.6615285806037256e-05
Step 1352 | epoch: 1.3025048169556839
Step 1353 | loss: 3.4378786087036133
Step 1353 | grad_norm: 2.581789016723633
Step 1353 | learning_rate: 5.6583172768143864e-05
Step 1353 | epoch: 1.3034682080924855
Step 1354 | loss: 3.6371703147888184
Step 1354 | grad_norm: 3.6515069007873535
Step 1354 | learning_rate: 5.655105973025049e-05
Step 1354 | epoch: 1.3044315992292872
Step 1355 | loss: 3.26172137260437
Step 1355 | grad_norm: 4.782740592956543
Step 1355 | learning_rate: 5.65189466923571e-05
Step 1355 | epoch: 1.3053949903660886
Step 1356 | loss: 3.642984628677368
Step 1356 | grad_norm: 2.727602481842041
Step 1356 | learning_rate: 5.648683365446371e-05
Step 1356 | epoch: 1.30635838150289
Step 1357 | loss: 3.539919853210449
Step 1357 | grad_norm: 2.789823293685913
Step 1357 | learning_rate: 5.6454720616570334e-05
Step 1357 | epoch: 1.3073217726396917
Step 1358 | loss: 2.4703080654144287
Step 1358 | grad_norm: 2.258575916290283
Step 1358 | learning_rate: 5.642260757867695e-05
Step 1358 | epoch: 1.3082851637764932
Step 1359 | loss: 3.1984121799468994
Step 1359 | grad_norm: 2.0401034355163574
Step 1359 | learning_rate: 5.6390494540783555e-05
Step 1359 | epoch: 1.3092485549132948
Step 1360 | loss: 3.018498659133911
Step 1360 | grad_norm: 2.3213560581207275
Step 1360 | learning_rate: 5.6358381502890176e-05
Step 1360 | epoch: 1.3102119460500963
Step 1361 | loss: 3.4133996963500977
Step 1361 | grad_norm: 2.695887565612793
Step 1361 | learning_rate: 5.632626846499679e-05
Step 1361 | epoch: 1.311175337186898
Step 1362 | loss: 3.1254255771636963
Step 1362 | grad_norm: 3.0538594722747803
Step 1362 | learning_rate: 5.629415542710341e-05
Step 1362 | epoch: 1.3121387283236994
Step 1363 | loss: 2.6796915531158447
Step 1363 | grad_norm: 2.2157905101776123
Step 1363 | learning_rate: 5.6262042389210026e-05
Step 1363 | epoch: 1.313102119460501
Step 1364 | loss: 4.199254989624023
Step 1364 | grad_norm: 3.246568202972412
Step 1364 | learning_rate: 5.622992935131663e-05
Step 1364 | epoch: 1.3140655105973025
Step 1365 | loss: 3.5531136989593506
Step 1365 | grad_norm: 2.4336535930633545
Step 1365 | learning_rate: 5.6197816313423254e-05
Step 1365 | epoch: 1.3150289017341041
Step 1366 | loss: 3.9607322216033936
Step 1366 | grad_norm: 4.224705219268799
Step 1366 | learning_rate: 5.616570327552987e-05
Step 1366 | epoch: 1.3159922928709056
Step 1367 | loss: 3.043682098388672
Step 1367 | grad_norm: 2.701918840408325
Step 1367 | learning_rate: 5.613359023763648e-05
Step 1367 | epoch: 1.316955684007707
Step 1368 | loss: 3.602992057800293
Step 1368 | grad_norm: 2.582899332046509
Step 1368 | learning_rate: 5.6101477199743103e-05
Step 1368 | epoch: 1.3179190751445087
Step 1369 | loss: 3.1681201457977295
Step 1369 | grad_norm: 3.431643009185791
Step 1369 | learning_rate: 5.606936416184971e-05
Step 1369 | epoch: 1.3188824662813103
Step 1370 | loss: 3.123870849609375
Step 1370 | grad_norm: 3.9533777236938477
Step 1370 | learning_rate: 5.6037251123956325e-05
Step 1370 | epoch: 1.3198458574181118
Step 1371 | loss: 3.288224220275879
Step 1371 | grad_norm: 3.1028692722320557
Step 1371 | learning_rate: 5.6005138086062946e-05
Step 1371 | epoch: 1.3208092485549132
Step 1372 | loss: 3.76340389251709
Step 1372 | grad_norm: 3.0654423236846924
Step 1372 | learning_rate: 5.597302504816956e-05
Step 1372 | epoch: 1.3217726396917149
Step 1373 | loss: 3.464601516723633
Step 1373 | grad_norm: 3.13177227973938
Step 1373 | learning_rate: 5.594091201027617e-05
Step 1373 | epoch: 1.3227360308285163
Step 1374 | loss: 2.5567612648010254
Step 1374 | grad_norm: 2.767928123474121
Step 1374 | learning_rate: 5.5908798972382795e-05
Step 1374 | epoch: 1.323699421965318
Step 1375 | loss: 3.4729061126708984
Step 1375 | grad_norm: 2.5891237258911133
Step 1375 | learning_rate: 5.58766859344894e-05
Step 1375 | epoch: 1.3246628131021194
Step 1376 | loss: 3.1157147884368896
Step 1376 | grad_norm: 3.537022113800049
Step 1376 | learning_rate: 5.5844572896596024e-05
Step 1376 | epoch: 1.325626204238921
Step 1377 | loss: 2.7526252269744873
Step 1377 | grad_norm: 1.9873943328857422
Step 1377 | learning_rate: 5.581245985870264e-05
Step 1377 | epoch: 1.3265895953757225
Step 1378 | loss: 3.28503155708313
Step 1378 | grad_norm: 3.1392483711242676
Step 1378 | learning_rate: 5.5780346820809245e-05
Step 1378 | epoch: 1.327552986512524
Step 1379 | loss: 3.346928596496582
Step 1379 | grad_norm: 3.837991237640381
Step 1379 | learning_rate: 5.574823378291587e-05
Step 1379 | epoch: 1.3285163776493256
Step 1380 | loss: 3.120525598526001
Step 1380 | grad_norm: 3.169130325317383
Step 1380 | learning_rate: 5.571612074502248e-05
Step 1380 | epoch: 1.3294797687861273
Step 1381 | loss: 3.2743396759033203
Step 1381 | grad_norm: 2.799488067626953
Step 1381 | learning_rate: 5.5684007707129095e-05
Step 1381 | epoch: 1.3304431599229287
Step 1382 | loss: 3.691713333129883
Step 1382 | grad_norm: 2.8474810123443604
Step 1382 | learning_rate: 5.5651894669235716e-05
Step 1382 | epoch: 1.3314065510597302
Step 1383 | loss: 3.3334591388702393
Step 1383 | grad_norm: 4.032984256744385
Step 1383 | learning_rate: 5.561978163134233e-05
Step 1383 | epoch: 1.3323699421965318
Step 1384 | loss: 2.865915060043335
Step 1384 | grad_norm: 3.902251720428467
Step 1384 | learning_rate: 5.558766859344894e-05
Step 1384 | epoch: 1.3333333333333333
Step 1385 | loss: 3.750662088394165
Step 1385 | grad_norm: 2.4139890670776367
Step 1385 | learning_rate: 5.555555555555556e-05
Step 1385 | epoch: 1.334296724470135
Step 1386 | loss: 3.7344369888305664
Step 1386 | grad_norm: 3.614122152328491
Step 1386 | learning_rate: 5.552344251766217e-05
Step 1386 | epoch: 1.3352601156069364
Step 1387 | loss: 3.49668550491333
Step 1387 | grad_norm: 3.228732109069824
Step 1387 | learning_rate: 5.5491329479768787e-05
Step 1387 | epoch: 1.336223506743738
Step 1388 | loss: 2.8493616580963135
Step 1388 | grad_norm: 2.9112095832824707
Step 1388 | learning_rate: 5.545921644187541e-05
Step 1388 | epoch: 1.3371868978805395
Step 1389 | loss: 3.115361452102661
Step 1389 | grad_norm: 2.32460880279541
Step 1389 | learning_rate: 5.5427103403982015e-05
Step 1389 | epoch: 1.3381502890173411
Step 1390 | loss: 3.6836695671081543
Step 1390 | grad_norm: 2.568315029144287
Step 1390 | learning_rate: 5.539499036608864e-05
Step 1390 | epoch: 1.3391136801541426
Step 1391 | loss: 3.857318162918091
Step 1391 | grad_norm: 3.460871458053589
Step 1391 | learning_rate: 5.536287732819525e-05
Step 1391 | epoch: 1.3400770712909442
Step 1392 | loss: 3.1399056911468506
Step 1392 | grad_norm: 2.6640069484710693
Step 1392 | learning_rate: 5.5330764290301864e-05
Step 1392 | epoch: 1.3410404624277457
Step 1393 | loss: 3.8209893703460693
Step 1393 | grad_norm: 3.9398021697998047
Step 1393 | learning_rate: 5.5298651252408485e-05
Step 1393 | epoch: 1.342003853564547
Step 1394 | loss: 2.6718645095825195
Step 1394 | grad_norm: 2.146009683609009
Step 1394 | learning_rate: 5.526653821451509e-05
Step 1394 | epoch: 1.3429672447013488
Step 1395 | loss: 3.778221368789673
Step 1395 | grad_norm: 3.367278814315796
Step 1395 | learning_rate: 5.523442517662171e-05
Step 1395 | epoch: 1.3439306358381504
Step 1396 | loss: 3.276298761367798
Step 1396 | grad_norm: 3.1040501594543457
Step 1396 | learning_rate: 5.520231213872833e-05
Step 1396 | epoch: 1.3448940269749519
Step 1397 | loss: 2.5050671100616455
Step 1397 | grad_norm: 2.139492988586426
Step 1397 | learning_rate: 5.517019910083494e-05
Step 1397 | epoch: 1.3458574181117533
Step 1398 | loss: 3.7974798679351807
Step 1398 | grad_norm: 2.993793487548828
Step 1398 | learning_rate: 5.513808606294155e-05
Step 1398 | epoch: 1.346820809248555
Step 1399 | loss: 2.771439552307129
Step 1399 | grad_norm: 2.356478452682495
Step 1399 | learning_rate: 5.510597302504818e-05
Step 1399 | epoch: 1.3477842003853564
Step 1400 | loss: 4.445875644683838
Step 1400 | grad_norm: 3.420433759689331
Step 1400 | learning_rate: 5.5073859987154785e-05
Step 1400 | epoch: 1.348747591522158
Step 1401 | loss: 4.640501022338867
Step 1401 | grad_norm: 3.355992555618286
Step 1401 | learning_rate: 5.5041746949261406e-05
Step 1401 | epoch: 1.3497109826589595
Step 1402 | loss: 3.4930059909820557
Step 1402 | grad_norm: 2.5532870292663574
Step 1402 | learning_rate: 5.500963391136802e-05
Step 1402 | epoch: 1.3506743737957612
Step 1403 | loss: 3.2846977710723877
Step 1403 | grad_norm: 2.488818883895874
Step 1403 | learning_rate: 5.4977520873474634e-05
Step 1403 | epoch: 1.3516377649325626
Step 1404 | loss: 3.179640054702759
Step 1404 | grad_norm: 2.0549843311309814
Step 1404 | learning_rate: 5.4945407835581255e-05
Step 1404 | epoch: 1.352601156069364
Step 1405 | loss: 3.5273468494415283
Step 1405 | grad_norm: 3.1348092555999756
Step 1405 | learning_rate: 5.491329479768786e-05
Step 1405 | epoch: 1.3535645472061657
Step 1406 | loss: 4.096083641052246
Step 1406 | grad_norm: 2.9603421688079834
Step 1406 | learning_rate: 5.4881181759794476e-05
Step 1406 | epoch: 1.3545279383429674
Step 1407 | loss: 2.814255714416504
Step 1407 | grad_norm: 3.208451747894287
Step 1407 | learning_rate: 5.48490687219011e-05
Step 1407 | epoch: 1.3554913294797688
Step 1408 | loss: 3.1537890434265137
Step 1408 | grad_norm: 2.918001413345337
Step 1408 | learning_rate: 5.481695568400771e-05
Step 1408 | epoch: 1.3564547206165702
Step 1409 | loss: 3.111814022064209
Step 1409 | grad_norm: 2.8679685592651367
Step 1409 | learning_rate: 5.478484264611432e-05
Step 1409 | epoch: 1.357418111753372
Step 1410 | loss: 3.683314800262451
Step 1410 | grad_norm: 2.9670238494873047
Step 1410 | learning_rate: 5.475272960822094e-05
Step 1410 | epoch: 1.3583815028901733
Step 1411 | loss: 3.0142505168914795
Step 1411 | grad_norm: 2.4471170902252197
Step 1411 | learning_rate: 5.4720616570327554e-05
Step 1411 | epoch: 1.359344894026975
Step 1412 | loss: 3.254556179046631
Step 1412 | grad_norm: 2.870161294937134
Step 1412 | learning_rate: 5.468850353243417e-05
Step 1412 | epoch: 1.3603082851637764
Step 1413 | loss: 3.096328020095825
Step 1413 | grad_norm: 3.80279278755188
Step 1413 | learning_rate: 5.465639049454079e-05
Step 1413 | epoch: 1.361271676300578
Step 1414 | loss: 3.0900697708129883
Step 1414 | grad_norm: 2.4528772830963135
Step 1414 | learning_rate: 5.46242774566474e-05
Step 1414 | epoch: 1.3622350674373795
Step 1415 | loss: 2.783109426498413
Step 1415 | grad_norm: 2.5590314865112305
Step 1415 | learning_rate: 5.4592164418754025e-05
Step 1415 | epoch: 1.363198458574181
Step 1416 | loss: 2.817439317703247
Step 1416 | grad_norm: 2.141113758087158
Step 1416 | learning_rate: 5.456005138086063e-05
Step 1416 | epoch: 1.3641618497109826
Step 1417 | loss: 3.638423204421997
Step 1417 | grad_norm: 2.7263214588165283
Step 1417 | learning_rate: 5.4527938342967246e-05
Step 1417 | epoch: 1.3651252408477843
Step 1418 | loss: 2.709925889968872
Step 1418 | grad_norm: 2.0850419998168945
Step 1418 | learning_rate: 5.449582530507387e-05
Step 1418 | epoch: 1.3660886319845857
Step 1419 | loss: 3.4371490478515625
Step 1419 | grad_norm: 2.6560730934143066
Step 1419 | learning_rate: 5.4463712267180474e-05
Step 1419 | epoch: 1.3670520231213872
Step 1420 | loss: 3.0646166801452637
Step 1420 | grad_norm: 3.491549253463745
Step 1420 | learning_rate: 5.443159922928709e-05
Step 1420 | epoch: 1.3680154142581888
Step 1421 | loss: 3.80604887008667
Step 1421 | grad_norm: 3.1564226150512695
Step 1421 | learning_rate: 5.439948619139371e-05
Step 1421 | epoch: 1.3689788053949903
Step 1422 | loss: 3.433006525039673
Step 1422 | grad_norm: 3.8788750171661377
Step 1422 | learning_rate: 5.4367373153500324e-05
Step 1422 | epoch: 1.369942196531792
Step 1423 | loss: 3.247540235519409
Step 1423 | grad_norm: 2.6196296215057373
Step 1423 | learning_rate: 5.433526011560693e-05
Step 1423 | epoch: 1.3709055876685934
Step 1424 | loss: 3.186776876449585
Step 1424 | grad_norm: 2.9036920070648193
Step 1424 | learning_rate: 5.430314707771356e-05
Step 1424 | epoch: 1.371868978805395
Step 1425 | loss: 3.8366780281066895
Step 1425 | grad_norm: 4.015866756439209
Step 1425 | learning_rate: 5.4271034039820166e-05
Step 1425 | epoch: 1.3728323699421965
Step 1426 | loss: 2.9419920444488525
Step 1426 | grad_norm: 3.189784526824951
Step 1426 | learning_rate: 5.423892100192679e-05
Step 1426 | epoch: 1.3737957610789981
Step 1427 | loss: 3.102292776107788
Step 1427 | grad_norm: 2.5125651359558105
Step 1427 | learning_rate: 5.42068079640334e-05
Step 1427 | epoch: 1.3747591522157996
Step 1428 | loss: 4.7327961921691895
Step 1428 | grad_norm: 3.510742664337158
Step 1428 | learning_rate: 5.4174694926140016e-05
Step 1428 | epoch: 1.3757225433526012
Step 1429 | loss: 3.013453960418701
Step 1429 | grad_norm: 1.9788557291030884
Step 1429 | learning_rate: 5.414258188824664e-05
Step 1429 | epoch: 1.3766859344894027
Step 1430 | loss: 2.865299940109253
Step 1430 | grad_norm: 2.5018839836120605
Step 1430 | learning_rate: 5.4110468850353244e-05
Step 1430 | epoch: 1.3776493256262041
Step 1431 | loss: 3.611539840698242
Step 1431 | grad_norm: 2.7572410106658936
Step 1431 | learning_rate: 5.407835581245986e-05
Step 1431 | epoch: 1.3786127167630058
Step 1432 | loss: 3.7061548233032227
Step 1432 | grad_norm: 2.5452630519866943
Step 1432 | learning_rate: 5.404624277456648e-05
Step 1432 | epoch: 1.3795761078998074
Step 1433 | loss: 2.8738176822662354
Step 1433 | grad_norm: 2.530625820159912
Step 1433 | learning_rate: 5.4014129736673093e-05
Step 1433 | epoch: 1.3805394990366089
Step 1434 | loss: 3.4100499153137207
Step 1434 | grad_norm: 3.407392740249634
Step 1434 | learning_rate: 5.39820166987797e-05
Step 1434 | epoch: 1.3815028901734103
Step 1435 | loss: 2.967451810836792
Step 1435 | grad_norm: 2.525233507156372
Step 1435 | learning_rate: 5.394990366088632e-05
Step 1435 | epoch: 1.382466281310212
Step 1436 | loss: 3.076957941055298
Step 1436 | grad_norm: 2.950681447982788
Step 1436 | learning_rate: 5.3917790622992936e-05
Step 1436 | epoch: 1.3834296724470134
Step 1437 | loss: 3.7328367233276367
Step 1437 | grad_norm: 5.295797824859619
Step 1437 | learning_rate: 5.388567758509955e-05
Step 1437 | epoch: 1.384393063583815
Step 1438 | loss: 2.5956151485443115
Step 1438 | grad_norm: 2.5920238494873047
Step 1438 | learning_rate: 5.385356454720617e-05
Step 1438 | epoch: 1.3853564547206165
Step 1439 | loss: 3.1293210983276367
Step 1439 | grad_norm: 2.7460687160491943
Step 1439 | learning_rate: 5.382145150931278e-05
Step 1439 | epoch: 1.3863198458574182
Step 1440 | loss: 3.4027273654937744
Step 1440 | grad_norm: 2.572361469268799
Step 1440 | learning_rate: 5.3789338471419406e-05
Step 1440 | epoch: 1.3872832369942196
Step 1441 | loss: 3.0432651042938232
Step 1441 | grad_norm: 2.2976865768432617
Step 1441 | learning_rate: 5.3757225433526014e-05
Step 1441 | epoch: 1.388246628131021
Step 1442 | loss: 3.5726916790008545
Step 1442 | grad_norm: 3.2915570735931396
Step 1442 | learning_rate: 5.372511239563263e-05
Step 1442 | epoch: 1.3892100192678227
Step 1443 | loss: 2.521155595779419
Step 1443 | grad_norm: 2.5615217685699463
Step 1443 | learning_rate: 5.369299935773925e-05
Step 1443 | epoch: 1.3901734104046244
Step 1444 | loss: 4.0121846199035645
Step 1444 | grad_norm: 3.1316986083984375
Step 1444 | learning_rate: 5.366088631984586e-05
Step 1444 | epoch: 1.3911368015414258
Step 1445 | loss: 3.578117609024048
Step 1445 | grad_norm: 2.7961313724517822
Step 1445 | learning_rate: 5.362877328195247e-05
Step 1445 | epoch: 1.3921001926782273
Step 1446 | loss: 2.6802051067352295
Step 1446 | grad_norm: 2.0924057960510254
Step 1446 | learning_rate: 5.359666024405909e-05
Step 1446 | epoch: 1.393063583815029
Step 1447 | loss: 3.021197557449341
Step 1447 | grad_norm: 2.4384238719940186
Step 1447 | learning_rate: 5.3564547206165706e-05
Step 1447 | epoch: 1.3940269749518304
Step 1448 | loss: 2.7504119873046875
Step 1448 | grad_norm: 2.2526071071624756
Step 1448 | learning_rate: 5.353243416827231e-05
Step 1448 | epoch: 1.394990366088632
Step 1449 | loss: 3.529939651489258
Step 1449 | grad_norm: 2.6128089427948
Step 1449 | learning_rate: 5.350032113037894e-05
Step 1449 | epoch: 1.3959537572254335
Step 1450 | loss: 3.3274383544921875
Step 1450 | grad_norm: 2.401121139526367
Step 1450 | learning_rate: 5.346820809248555e-05
Step 1450 | epoch: 1.3969171483622351
Step 1451 | loss: 2.8161723613739014
Step 1451 | grad_norm: 3.033345937728882
Step 1451 | learning_rate: 5.343609505459217e-05
Step 1451 | epoch: 1.3978805394990366
Step 1452 | loss: 3.0613155364990234
Step 1452 | grad_norm: 3.3581998348236084
Step 1452 | learning_rate: 5.340398201669878e-05
Step 1452 | epoch: 1.3988439306358382
Step 1453 | loss: 3.25254487991333
Step 1453 | grad_norm: 2.7963874340057373
Step 1453 | learning_rate: 5.33718689788054e-05
Step 1453 | epoch: 1.3998073217726397
Step 1454 | loss: 2.843526601791382
Step 1454 | grad_norm: 2.214296579360962
Step 1454 | learning_rate: 5.333975594091202e-05
Step 1454 | epoch: 1.4007707129094413
Step 1455 | loss: 3.3990888595581055
Step 1455 | grad_norm: 2.90767765045166
Step 1455 | learning_rate: 5.3307642903018626e-05
Step 1455 | epoch: 1.4017341040462428
Step 1456 | loss: 3.843172550201416
Step 1456 | grad_norm: 3.590446949005127
Step 1456 | learning_rate: 5.327552986512524e-05
Step 1456 | epoch: 1.4026974951830442
Step 1457 | loss: 3.4403421878814697
Step 1457 | grad_norm: 3.2778573036193848
Step 1457 | learning_rate: 5.324341682723186e-05
Step 1457 | epoch: 1.4036608863198459
Step 1458 | loss: 3.6426444053649902
Step 1458 | grad_norm: 3.530846357345581
Step 1458 | learning_rate: 5.3211303789338475e-05
Step 1458 | epoch: 1.4046242774566475
Step 1459 | loss: 3.418971061706543
Step 1459 | grad_norm: 2.7450220584869385
Step 1459 | learning_rate: 5.317919075144508e-05
Step 1459 | epoch: 1.405587668593449
Step 1460 | loss: 2.394423246383667
Step 1460 | grad_norm: 2.121824026107788
Step 1460 | learning_rate: 5.314707771355171e-05
Step 1460 | epoch: 1.4065510597302504
Step 1461 | loss: 2.7135403156280518
Step 1461 | grad_norm: 2.111896514892578
Step 1461 | learning_rate: 5.311496467565832e-05
Step 1461 | epoch: 1.407514450867052
Step 1462 | loss: 3.0167980194091797
Step 1462 | grad_norm: 2.93027663230896
Step 1462 | learning_rate: 5.308285163776493e-05
Step 1462 | epoch: 1.4084778420038535
Step 1463 | loss: 3.7215065956115723
Step 1463 | grad_norm: 2.2985265254974365
Step 1463 | learning_rate: 5.305073859987155e-05
Step 1463 | epoch: 1.4094412331406552
Step 1464 | loss: 4.53275203704834
Step 1464 | grad_norm: 2.8733644485473633
Step 1464 | learning_rate: 5.301862556197816e-05
Step 1464 | epoch: 1.4104046242774566
Step 1465 | loss: 3.7185709476470947
Step 1465 | grad_norm: 2.7048792839050293
Step 1465 | learning_rate: 5.298651252408479e-05
Step 1465 | epoch: 1.4113680154142583
Step 1466 | loss: 3.695361852645874
Step 1466 | grad_norm: 2.798917293548584
Step 1466 | learning_rate: 5.2954399486191395e-05
Step 1466 | epoch: 1.4123314065510597
Step 1467 | loss: 3.4120700359344482
Step 1467 | grad_norm: 2.54323148727417
Step 1467 | learning_rate: 5.292228644829801e-05
Step 1467 | epoch: 1.4132947976878611
Step 1468 | loss: 3.322421073913574
Step 1468 | grad_norm: 2.5272750854492188
Step 1468 | learning_rate: 5.289017341040463e-05
Step 1468 | epoch: 1.4142581888246628
Step 1469 | loss: 4.205753326416016
Step 1469 | grad_norm: 3.058901071548462
Step 1469 | learning_rate: 5.2858060372511245e-05
Step 1469 | epoch: 1.4152215799614645
Step 1470 | loss: 3.302745819091797
Step 1470 | grad_norm: 2.325322389602661
Step 1470 | learning_rate: 5.282594733461785e-05
Step 1470 | epoch: 1.416184971098266
Step 1471 | loss: 3.452989101409912
Step 1471 | grad_norm: 2.563730478286743
Step 1471 | learning_rate: 5.279383429672447e-05
Step 1471 | epoch: 1.4171483622350673
Step 1472 | loss: 2.626885175704956
Step 1472 | grad_norm: 2.5893266201019287
Step 1472 | learning_rate: 5.276172125883109e-05
Step 1472 | epoch: 1.418111753371869
Step 1473 | loss: 3.587022066116333
Step 1473 | grad_norm: 3.388043165206909
Step 1473 | learning_rate: 5.27296082209377e-05
Step 1473 | epoch: 1.4190751445086704
Step 1474 | loss: 2.772313117980957
Step 1474 | grad_norm: 2.1341729164123535
Step 1474 | learning_rate: 5.269749518304432e-05
Step 1474 | epoch: 1.420038535645472
Step 1475 | loss: 3.2505874633789062
Step 1475 | grad_norm: 2.8879940509796143
Step 1475 | learning_rate: 5.266538214515093e-05
Step 1475 | epoch: 1.4210019267822736
Step 1476 | loss: 3.1890788078308105
Step 1476 | grad_norm: 2.58359956741333
Step 1476 | learning_rate: 5.263326910725756e-05
Step 1476 | epoch: 1.4219653179190752
Step 1477 | loss: 3.5346808433532715
Step 1477 | grad_norm: 3.719341993331909
Step 1477 | learning_rate: 5.2601156069364165e-05
Step 1477 | epoch: 1.4229287090558767
Step 1478 | loss: 3.1156399250030518
Step 1478 | grad_norm: 2.95271372795105
Step 1478 | learning_rate: 5.256904303147078e-05
Step 1478 | epoch: 1.423892100192678
Step 1479 | loss: 3.8690004348754883
Step 1479 | grad_norm: 3.271491527557373
Step 1479 | learning_rate: 5.25369299935774e-05
Step 1479 | epoch: 1.4248554913294798
Step 1480 | loss: 3.740269660949707
Step 1480 | grad_norm: 2.2759757041931152
Step 1480 | learning_rate: 5.250481695568401e-05
Step 1480 | epoch: 1.4258188824662814
Step 1481 | loss: 3.6431267261505127
Step 1481 | grad_norm: 4.479500770568848
Step 1481 | learning_rate: 5.247270391779062e-05
Step 1481 | epoch: 1.4267822736030829
Step 1482 | loss: 4.226675033569336
Step 1482 | grad_norm: 2.8330562114715576
Step 1482 | learning_rate: 5.244059087989724e-05
Step 1482 | epoch: 1.4277456647398843
Step 1483 | loss: 2.94921612739563
Step 1483 | grad_norm: 2.834803581237793
Step 1483 | learning_rate: 5.240847784200386e-05
Step 1483 | epoch: 1.428709055876686
Step 1484 | loss: 2.609048366546631
Step 1484 | grad_norm: 2.5167856216430664
Step 1484 | learning_rate: 5.2376364804110464e-05
Step 1484 | epoch: 1.4296724470134876
Step 1485 | loss: 3.56326961517334
Step 1485 | grad_norm: 3.2828900814056396
Step 1485 | learning_rate: 5.234425176621709e-05
Step 1485 | epoch: 1.430635838150289
Step 1486 | loss: 3.925980567932129
Step 1486 | grad_norm: 2.413147449493408
Step 1486 | learning_rate: 5.23121387283237e-05
Step 1486 | epoch: 1.4315992292870905
Step 1487 | loss: 3.031061887741089
Step 1487 | grad_norm: 2.8063278198242188
Step 1487 | learning_rate: 5.2280025690430314e-05
Step 1487 | epoch: 1.4325626204238922
Step 1488 | loss: 3.229680061340332
Step 1488 | grad_norm: 2.8566317558288574
Step 1488 | learning_rate: 5.2247912652536935e-05
Step 1488 | epoch: 1.4335260115606936
Step 1489 | loss: 3.3158581256866455
Step 1489 | grad_norm: 2.5539276599884033
Step 1489 | learning_rate: 5.221579961464355e-05
Step 1489 | epoch: 1.4344894026974953
Step 1490 | loss: 2.8821794986724854
Step 1490 | grad_norm: 2.457793712615967
Step 1490 | learning_rate: 5.218368657675017e-05
Step 1490 | epoch: 1.4354527938342967
Step 1491 | loss: 3.206479787826538
Step 1491 | grad_norm: 2.4574432373046875
Step 1491 | learning_rate: 5.215157353885678e-05
Step 1491 | epoch: 1.4364161849710984
Step 1492 | loss: 2.9756081104278564
Step 1492 | grad_norm: 2.5401713848114014
Step 1492 | learning_rate: 5.211946050096339e-05
Step 1492 | epoch: 1.4373795761078998
Step 1493 | loss: 3.045161247253418
Step 1493 | grad_norm: 2.381260395050049
Step 1493 | learning_rate: 5.208734746307001e-05
Step 1493 | epoch: 1.4383429672447012
Step 1494 | loss: 3.2932522296905518
Step 1494 | grad_norm: 2.8064048290252686
Step 1494 | learning_rate: 5.205523442517663e-05
Step 1494 | epoch: 1.439306358381503
Step 1495 | loss: 3.235488176345825
Step 1495 | grad_norm: 2.3869473934173584
Step 1495 | learning_rate: 5.2023121387283234e-05
Step 1495 | epoch: 1.4402697495183046
Step 1496 | loss: 3.236767292022705
Step 1496 | grad_norm: 2.4231362342834473
Step 1496 | learning_rate: 5.1991008349389855e-05
Step 1496 | epoch: 1.441233140655106
Step 1497 | loss: 3.3059990406036377
Step 1497 | grad_norm: 3.0838000774383545
Step 1497 | learning_rate: 5.195889531149647e-05
Step 1497 | epoch: 1.4421965317919074
Step 1498 | loss: 3.5924205780029297
Step 1498 | grad_norm: 3.019618272781372
Step 1498 | learning_rate: 5.192678227360308e-05
Step 1498 | epoch: 1.443159922928709
Step 1499 | loss: 2.99705171585083
Step 1499 | grad_norm: 2.488239049911499
Step 1499 | learning_rate: 5.1894669235709704e-05
Step 1499 | epoch: 1.4441233140655105
Step 1500 | loss: 3.6903185844421387
Step 1500 | grad_norm: 2.9225358963012695
Step 1500 | learning_rate: 5.186255619781631e-05
Step 1500 | epoch: 1.4450867052023122
Step 1501 | loss: 3.860466241836548
Step 1501 | grad_norm: 3.4239439964294434
Step 1501 | learning_rate: 5.1830443159922926e-05
Step 1501 | epoch: 1.4460500963391136
Step 1502 | loss: 3.5988969802856445
Step 1502 | grad_norm: 3.222079277038574
Step 1502 | learning_rate: 5.179833012202955e-05
Step 1502 | epoch: 1.4470134874759153
Step 1503 | loss: 3.4607818126678467
Step 1503 | grad_norm: 2.910325050354004
Step 1503 | learning_rate: 5.176621708413616e-05
Step 1503 | epoch: 1.4479768786127167
Step 1504 | loss: 3.8843681812286377
Step 1504 | grad_norm: 2.4165053367614746
Step 1504 | learning_rate: 5.173410404624278e-05
Step 1504 | epoch: 1.4489402697495182
Step 1505 | loss: 3.6948301792144775
Step 1505 | grad_norm: 3.3242416381835938
Step 1505 | learning_rate: 5.1701991008349396e-05
Step 1505 | epoch: 1.4499036608863198
Step 1506 | loss: 4.360815048217773
Step 1506 | grad_norm: 3.520867109298706
Step 1506 | learning_rate: 5.1669877970456004e-05
Step 1506 | epoch: 1.4508670520231215
Step 1507 | loss: 3.656075954437256
Step 1507 | grad_norm: 2.5010550022125244
Step 1507 | learning_rate: 5.1637764932562625e-05
Step 1507 | epoch: 1.451830443159923
Step 1508 | loss: 2.7084574699401855
Step 1508 | grad_norm: 2.685739040374756
Step 1508 | learning_rate: 5.160565189466924e-05
Step 1508 | epoch: 1.4527938342967244
Step 1509 | loss: 2.8589816093444824
Step 1509 | grad_norm: 2.7362160682678223
Step 1509 | learning_rate: 5.1573538856775846e-05
Step 1509 | epoch: 1.453757225433526
Step 1510 | loss: 4.495601177215576
Step 1510 | grad_norm: 3.074071168899536
Step 1510 | learning_rate: 5.1541425818882474e-05
Step 1510 | epoch: 1.4547206165703275
Step 1511 | loss: 3.253154754638672
Step 1511 | grad_norm: 2.5022568702697754
Step 1511 | learning_rate: 5.150931278098908e-05
Step 1511 | epoch: 1.4556840077071291
Step 1512 | loss: 3.0055596828460693
Step 1512 | grad_norm: 2.118995189666748
Step 1512 | learning_rate: 5.1477199743095696e-05
Step 1512 | epoch: 1.4566473988439306
Step 1513 | loss: 2.9249114990234375
Step 1513 | grad_norm: 2.615166664123535
Step 1513 | learning_rate: 5.1445086705202317e-05
Step 1513 | epoch: 1.4576107899807322
Step 1514 | loss: 3.982038974761963
Step 1514 | grad_norm: 3.0515971183776855
Step 1514 | learning_rate: 5.141297366730893e-05
Step 1514 | epoch: 1.4585741811175337
Step 1515 | loss: 3.2095258235931396
Step 1515 | grad_norm: 2.948084592819214
Step 1515 | learning_rate: 5.138086062941555e-05
Step 1515 | epoch: 1.4595375722543353
Step 1516 | loss: 3.538889169692993
Step 1516 | grad_norm: 2.3225977420806885
Step 1516 | learning_rate: 5.134874759152216e-05
Step 1516 | epoch: 1.4605009633911368
Step 1517 | loss: 3.4962456226348877
Step 1517 | grad_norm: 2.448354721069336
Step 1517 | learning_rate: 5.131663455362877e-05
Step 1517 | epoch: 1.4614643545279384
Step 1518 | loss: 2.8742432594299316
Step 1518 | grad_norm: 3.0799946784973145
Step 1518 | learning_rate: 5.1284521515735394e-05
Step 1518 | epoch: 1.4624277456647399
Step 1519 | loss: 2.572279691696167
Step 1519 | grad_norm: 2.302764415740967
Step 1519 | learning_rate: 5.125240847784201e-05
Step 1519 | epoch: 1.4633911368015413
Step 1520 | loss: 3.6528518199920654
Step 1520 | grad_norm: 3.0693838596343994
Step 1520 | learning_rate: 5.1220295439948616e-05
Step 1520 | epoch: 1.464354527938343
Step 1521 | loss: 3.690488576889038
Step 1521 | grad_norm: 2.898946523666382
Step 1521 | learning_rate: 5.1188182402055244e-05
Step 1521 | epoch: 1.4653179190751446
Step 1522 | loss: 3.47196626663208
Step 1522 | grad_norm: 2.8746917247772217
Step 1522 | learning_rate: 5.115606936416185e-05
Step 1522 | epoch: 1.466281310211946
Step 1523 | loss: 3.8218188285827637
Step 1523 | grad_norm: 2.2172224521636963
Step 1523 | learning_rate: 5.1123956326268465e-05
Step 1523 | epoch: 1.4672447013487475
Step 1524 | loss: 4.01796293258667
Step 1524 | grad_norm: 2.9069371223449707
Step 1524 | learning_rate: 5.1091843288375086e-05
Step 1524 | epoch: 1.4682080924855492
Step 1525 | loss: 3.516322612762451
Step 1525 | grad_norm: 2.496960401535034
Step 1525 | learning_rate: 5.1059730250481694e-05
Step 1525 | epoch: 1.4691714836223506
Step 1526 | loss: 4.206760406494141
Step 1526 | grad_norm: 3.2059402465820312
Step 1526 | learning_rate: 5.102761721258831e-05
Step 1526 | epoch: 1.4701348747591523
Step 1527 | loss: 3.108248233795166
Step 1527 | grad_norm: 2.510535955429077
Step 1527 | learning_rate: 5.099550417469493e-05
Step 1527 | epoch: 1.4710982658959537
Step 1528 | loss: 3.2311277389526367
Step 1528 | grad_norm: 2.1065163612365723
Step 1528 | learning_rate: 5.096339113680154e-05
Step 1528 | epoch: 1.4720616570327554
Step 1529 | loss: 3.74761962890625
Step 1529 | grad_norm: 3.2334237098693848
Step 1529 | learning_rate: 5.0931278098908164e-05
Step 1529 | epoch: 1.4730250481695568
Step 1530 | loss: 2.9975521564483643
Step 1530 | grad_norm: 2.462083101272583
Step 1530 | learning_rate: 5.089916506101478e-05
Step 1530 | epoch: 1.4739884393063583
Step 1531 | loss: 2.478442907333374
Step 1531 | grad_norm: 2.0659687519073486
Step 1531 | learning_rate: 5.0867052023121385e-05
Step 1531 | epoch: 1.47495183044316
Step 1532 | loss: 4.346289157867432
Step 1532 | grad_norm: 3.6892309188842773
Step 1532 | learning_rate: 5.0834938985228006e-05
Step 1532 | epoch: 1.4759152215799616
Step 1533 | loss: 2.8561110496520996
Step 1533 | grad_norm: 2.7273876667022705
Step 1533 | learning_rate: 5.080282594733462e-05
Step 1533 | epoch: 1.476878612716763
Step 1534 | loss: 3.365565299987793
Step 1534 | grad_norm: 2.400869607925415
Step 1534 | learning_rate: 5.0770712909441235e-05
Step 1534 | epoch: 1.4778420038535645
Step 1535 | loss: 3.3288395404815674
Step 1535 | grad_norm: 2.525404214859009
Step 1535 | learning_rate: 5.0738599871547856e-05
Step 1535 | epoch: 1.4788053949903661
Step 1536 | loss: 2.7405993938446045
Step 1536 | grad_norm: 1.9410285949707031
Step 1536 | learning_rate: 5.070648683365446e-05
Step 1536 | epoch: 1.4797687861271676
Step 1537 | loss: 3.945101737976074
Step 1537 | grad_norm: 3.101494789123535
Step 1537 | learning_rate: 5.067437379576108e-05
Step 1537 | epoch: 1.4807321772639692
Step 1538 | loss: 3.4721131324768066
Step 1538 | grad_norm: 2.553724765777588
Step 1538 | learning_rate: 5.06422607578677e-05
Step 1538 | epoch: 1.4816955684007707
Step 1539 | loss: 3.7275424003601074
Step 1539 | grad_norm: 3.4658844470977783
Step 1539 | learning_rate: 5.061014771997431e-05
Step 1539 | epoch: 1.4826589595375723
Step 1540 | loss: 2.977778911590576
Step 1540 | grad_norm: 2.269164800643921
Step 1540 | learning_rate: 5.0578034682080933e-05
Step 1540 | epoch: 1.4836223506743738
Step 1541 | loss: 4.085809230804443
Step 1541 | grad_norm: 3.4899401664733887
Step 1541 | learning_rate: 5.054592164418754e-05
Step 1541 | epoch: 1.4845857418111754
Step 1542 | loss: 2.681948184967041
Step 1542 | grad_norm: 3.7926137447357178
Step 1542 | learning_rate: 5.0513808606294155e-05
Step 1542 | epoch: 1.4855491329479769
Step 1543 | loss: 3.310879707336426
Step 1543 | grad_norm: 3.4694712162017822
Step 1543 | learning_rate: 5.0481695568400776e-05
Step 1543 | epoch: 1.4865125240847785
Step 1544 | loss: 3.209481716156006
Step 1544 | grad_norm: 2.719393014907837
Step 1544 | learning_rate: 5.044958253050739e-05
Step 1544 | epoch: 1.48747591522158
Step 1545 | loss: 3.2242233753204346
Step 1545 | grad_norm: 2.972832679748535
Step 1545 | learning_rate: 5.0417469492614e-05
Step 1545 | epoch: 1.4884393063583814
Step 1546 | loss: 3.663811445236206
Step 1546 | grad_norm: 2.683026075363159
Step 1546 | learning_rate: 5.0385356454720625e-05
Step 1546 | epoch: 1.489402697495183
Step 1547 | loss: 3.5268449783325195
Step 1547 | grad_norm: 2.535468578338623
Step 1547 | learning_rate: 5.035324341682723e-05
Step 1547 | epoch: 1.4903660886319847
Step 1548 | loss: 2.7644004821777344
Step 1548 | grad_norm: 2.6446175575256348
Step 1548 | learning_rate: 5.032113037893385e-05
Step 1548 | epoch: 1.4913294797687862
Step 1549 | loss: 2.607109785079956
Step 1549 | grad_norm: 2.761303424835205
Step 1549 | learning_rate: 5.028901734104047e-05
Step 1549 | epoch: 1.4922928709055876
Step 1550 | loss: 4.100269794464111
Step 1550 | grad_norm: 3.2923572063446045
Step 1550 | learning_rate: 5.025690430314708e-05
Step 1550 | epoch: 1.4932562620423893
Step 1551 | loss: 3.112229585647583
Step 1551 | grad_norm: 2.5350863933563232
Step 1551 | learning_rate: 5.022479126525369e-05
Step 1551 | epoch: 1.4942196531791907
Step 1552 | loss: 2.862396001815796
Step 1552 | grad_norm: 2.959249258041382
Step 1552 | learning_rate: 5.019267822736031e-05
Step 1552 | epoch: 1.4951830443159924
Step 1553 | loss: 3.7882518768310547
Step 1553 | grad_norm: 2.937978744506836
Step 1553 | learning_rate: 5.0160565189466925e-05
Step 1553 | epoch: 1.4961464354527938
Step 1554 | loss: 3.5690481662750244
Step 1554 | grad_norm: 3.0896530151367188
Step 1554 | learning_rate: 5.0128452151573546e-05
Step 1554 | epoch: 1.4971098265895955
Step 1555 | loss: 3.3695127964019775
Step 1555 | grad_norm: 2.9513111114501953
Step 1555 | learning_rate: 5.009633911368016e-05
Step 1555 | epoch: 1.498073217726397
Step 1556 | loss: 3.168609857559204
Step 1556 | grad_norm: 2.4534692764282227
Step 1556 | learning_rate: 5.006422607578677e-05
Step 1556 | epoch: 1.4990366088631983
Step 1557 | loss: 3.8090806007385254
Step 1557 | grad_norm: 2.694317579269409
Step 1557 | learning_rate: 5.003211303789339e-05
Step 1557 | epoch: 1.5
Step 1558 | loss: 2.6699204444885254
Step 1558 | grad_norm: 2.2184979915618896
Step 1558 | learning_rate: 5e-05
Step 1558 | epoch: 1.5009633911368017
Step 1559 | loss: 3.326306104660034
Step 1559 | grad_norm: 2.8434698581695557
Step 1559 | learning_rate: 4.9967886962106617e-05
Step 1559 | epoch: 1.501926782273603
Step 1560 | loss: 3.5685577392578125
Step 1560 | grad_norm: 2.680063009262085
Step 1560 | learning_rate: 4.993577392421323e-05
Step 1560 | epoch: 1.5028901734104045
Step 1561 | loss: 3.3997652530670166
Step 1561 | grad_norm: 2.920551061630249
Step 1561 | learning_rate: 4.9903660886319845e-05
Step 1561 | epoch: 1.5038535645472062
Step 1562 | loss: 2.6804540157318115
Step 1562 | grad_norm: 2.3348915576934814
Step 1562 | learning_rate: 4.9871547848426466e-05
Step 1562 | epoch: 1.5048169556840079
Step 1563 | loss: 3.5276427268981934
Step 1563 | grad_norm: 4.020771503448486
Step 1563 | learning_rate: 4.983943481053308e-05
Step 1563 | epoch: 1.5057803468208093
Step 1564 | loss: 3.3624486923217773
Step 1564 | grad_norm: 2.794240951538086
Step 1564 | learning_rate: 4.9807321772639694e-05
Step 1564 | epoch: 1.5067437379576107
Step 1565 | loss: 4.057067394256592
Step 1565 | grad_norm: 3.342733383178711
Step 1565 | learning_rate: 4.977520873474631e-05
Step 1565 | epoch: 1.5077071290944124
Step 1566 | loss: 3.5585429668426514
Step 1566 | grad_norm: 2.514355182647705
Step 1566 | learning_rate: 4.974309569685292e-05
Step 1566 | epoch: 1.5086705202312138
Step 1567 | loss: 3.5603270530700684
Step 1567 | grad_norm: 3.3374478816986084
Step 1567 | learning_rate: 4.971098265895954e-05
Step 1567 | epoch: 1.5096339113680153
Step 1568 | loss: 3.4076616764068604
Step 1568 | grad_norm: 2.9291794300079346
Step 1568 | learning_rate: 4.967886962106615e-05
Step 1568 | epoch: 1.510597302504817
Step 1569 | loss: 3.96209716796875
Step 1569 | grad_norm: 2.755119562149048
Step 1569 | learning_rate: 4.964675658317277e-05
Step 1569 | epoch: 1.5115606936416186
Step 1570 | loss: 2.8995273113250732
Step 1570 | grad_norm: 2.158797025680542
Step 1570 | learning_rate: 4.9614643545279386e-05
Step 1570 | epoch: 1.51252408477842
Step 1571 | loss: 3.024597406387329
Step 1571 | grad_norm: 2.803483009338379
Step 1571 | learning_rate: 4.9582530507386e-05
Step 1571 | epoch: 1.5134874759152215
Step 1572 | loss: 2.848069429397583
Step 1572 | grad_norm: 2.82214617729187
Step 1572 | learning_rate: 4.9550417469492615e-05
Step 1572 | epoch: 1.5144508670520231
Step 1573 | loss: 3.3071441650390625
Step 1573 | grad_norm: 2.68801212310791
Step 1573 | learning_rate: 4.9518304431599236e-05
Step 1573 | epoch: 1.5154142581888248
Step 1574 | loss: 3.3353302478790283
Step 1574 | grad_norm: 3.1308798789978027
Step 1574 | learning_rate: 4.948619139370585e-05
Step 1574 | epoch: 1.5163776493256262
Step 1575 | loss: 3.96797776222229
Step 1575 | grad_norm: 3.131565809249878
Step 1575 | learning_rate: 4.9454078355812464e-05
Step 1575 | epoch: 1.5173410404624277
Step 1576 | loss: 3.6844325065612793
Step 1576 | grad_norm: 3.181643009185791
Step 1576 | learning_rate: 4.942196531791908e-05
Step 1576 | epoch: 1.5183044315992293
Step 1577 | loss: 3.642686605453491
Step 1577 | grad_norm: 2.4925458431243896
Step 1577 | learning_rate: 4.938985228002569e-05
Step 1577 | epoch: 1.5192678227360308
Step 1578 | loss: 3.542388916015625
Step 1578 | grad_norm: 3.406473398208618
Step 1578 | learning_rate: 4.9357739242132306e-05
Step 1578 | epoch: 1.5202312138728322
Step 1579 | loss: 4.321348667144775
Step 1579 | grad_norm: 2.8442609310150146
Step 1579 | learning_rate: 4.932562620423892e-05
Step 1579 | epoch: 1.5211946050096339
Step 1580 | loss: 3.2737462520599365
Step 1580 | grad_norm: 2.366356611251831
Step 1580 | learning_rate: 4.929351316634554e-05
Step 1580 | epoch: 1.5221579961464355
Step 1581 | loss: 2.6748573780059814
Step 1581 | grad_norm: 2.578482151031494
Step 1581 | learning_rate: 4.9261400128452156e-05
Step 1581 | epoch: 1.523121387283237
Step 1582 | loss: 3.3884477615356445
Step 1582 | grad_norm: 3.0241143703460693
Step 1582 | learning_rate: 4.922928709055877e-05
Step 1582 | epoch: 1.5240847784200384
Step 1583 | loss: 4.517447471618652
Step 1583 | grad_norm: 2.912423849105835
Step 1583 | learning_rate: 4.9197174052665384e-05
Step 1583 | epoch: 1.52504816955684
Step 1584 | loss: 4.0288567543029785
Step 1584 | grad_norm: 3.8334290981292725
Step 1584 | learning_rate: 4.9165061014772e-05
Step 1584 | epoch: 1.5260115606936417
Step 1585 | loss: 2.994473934173584
Step 1585 | grad_norm: 2.7337708473205566
Step 1585 | learning_rate: 4.913294797687861e-05
Step 1585 | epoch: 1.5269749518304432
Step 1586 | loss: 3.4077320098876953
Step 1586 | grad_norm: 2.7622663974761963
Step 1586 | learning_rate: 4.910083493898523e-05
Step 1586 | epoch: 1.5279383429672446
Step 1587 | loss: 3.260094165802002
Step 1587 | grad_norm: 3.319387912750244
Step 1587 | learning_rate: 4.906872190109185e-05
Step 1587 | epoch: 1.5289017341040463
Step 1588 | loss: 3.446934700012207
Step 1588 | grad_norm: 2.3195600509643555
Step 1588 | learning_rate: 4.903660886319846e-05
Step 1588 | epoch: 1.529865125240848
Step 1589 | loss: 3.6964707374572754
Step 1589 | grad_norm: 2.999955415725708
Step 1589 | learning_rate: 4.9004495825305076e-05
Step 1589 | epoch: 1.5308285163776492
Step 1590 | loss: 3.605281352996826
Step 1590 | grad_norm: 2.5439975261688232
Step 1590 | learning_rate: 4.897238278741169e-05
Step 1590 | epoch: 1.5317919075144508
Step 1591 | loss: 3.119293451309204
Step 1591 | grad_norm: 2.7467567920684814
Step 1591 | learning_rate: 4.894026974951831e-05
Step 1591 | epoch: 1.5327552986512525
Step 1592 | loss: 3.5831258296966553
Step 1592 | grad_norm: 3.0571506023406982
Step 1592 | learning_rate: 4.890815671162492e-05
Step 1592 | epoch: 1.533718689788054
Step 1593 | loss: 2.9003889560699463
Step 1593 | grad_norm: 3.2589378356933594
Step 1593 | learning_rate: 4.887604367373154e-05
Step 1593 | epoch: 1.5346820809248554
Step 1594 | loss: 2.9235377311706543
Step 1594 | grad_norm: 2.5547540187835693
Step 1594 | learning_rate: 4.8843930635838154e-05
Step 1594 | epoch: 1.535645472061657
Step 1595 | loss: 3.2102925777435303
Step 1595 | grad_norm: 2.598874568939209
Step 1595 | learning_rate: 4.881181759794477e-05
Step 1595 | epoch: 1.5366088631984587
Step 1596 | loss: 3.829941511154175
Step 1596 | grad_norm: 2.8081281185150146
Step 1596 | learning_rate: 4.877970456005138e-05
Step 1596 | epoch: 1.5375722543352601
Step 1597 | loss: 3.4743306636810303
Step 1597 | grad_norm: 3.1388139724731445
Step 1597 | learning_rate: 4.8747591522157996e-05
Step 1597 | epoch: 1.5385356454720616
Step 1598 | loss: 3.6482012271881104
Step 1598 | grad_norm: 2.8399240970611572
Step 1598 | learning_rate: 4.871547848426462e-05
Step 1598 | epoch: 1.5394990366088632
Step 1599 | loss: 3.3434717655181885
Step 1599 | grad_norm: 3.1125032901763916
Step 1599 | learning_rate: 4.8683365446371225e-05
Step 1599 | epoch: 1.5404624277456649
Step 1600 | loss: 3.556776285171509
Step 1600 | grad_norm: 2.6153371334075928
Step 1600 | learning_rate: 4.8651252408477846e-05
Step 1600 | epoch: 1.5414258188824663
Step 1601 | loss: 3.7687621116638184
Step 1601 | grad_norm: 2.7296206951141357
Step 1601 | learning_rate: 4.861913937058446e-05
Step 1601 | epoch: 1.5423892100192678
Step 1602 | loss: 3.516343593597412
Step 1602 | grad_norm: 2.647383689880371
Step 1602 | learning_rate: 4.8587026332691074e-05
Step 1602 | epoch: 1.5433526011560694
Step 1603 | loss: 2.837111234664917
Step 1603 | grad_norm: 2.5969936847686768
Step 1603 | learning_rate: 4.855491329479769e-05
Step 1603 | epoch: 1.5443159922928709
Step 1604 | loss: 3.557854413986206
Step 1604 | grad_norm: 2.7625222206115723
Step 1604 | learning_rate: 4.85228002569043e-05
Step 1604 | epoch: 1.5452793834296723
Step 1605 | loss: 2.8403775691986084
Step 1605 | grad_norm: 2.708855390548706
Step 1605 | learning_rate: 4.8490687219010923e-05
Step 1605 | epoch: 1.546242774566474
Step 1606 | loss: 3.329585313796997
Step 1606 | grad_norm: 2.91443133354187
Step 1606 | learning_rate: 4.845857418111754e-05
Step 1606 | epoch: 1.5472061657032756
Step 1607 | loss: 3.672205686569214
Step 1607 | grad_norm: 3.5622897148132324
Step 1607 | learning_rate: 4.842646114322415e-05
Step 1607 | epoch: 1.548169556840077
Step 1608 | loss: 2.7905266284942627
Step 1608 | grad_norm: 2.558438301086426
Step 1608 | learning_rate: 4.8394348105330766e-05
Step 1608 | epoch: 1.5491329479768785
Step 1609 | loss: 2.9593114852905273
Step 1609 | grad_norm: 2.3399665355682373
Step 1609 | learning_rate: 4.836223506743739e-05
Step 1609 | epoch: 1.5500963391136802
Step 1610 | loss: 3.7989108562469482
Step 1610 | grad_norm: 2.5888073444366455
Step 1610 | learning_rate: 4.8330122029543994e-05
Step 1610 | epoch: 1.5510597302504818
Step 1611 | loss: 3.6645994186401367
Step 1611 | grad_norm: 3.8242266178131104
Step 1611 | learning_rate: 4.829800899165061e-05
Step 1611 | epoch: 1.5520231213872833
Step 1612 | loss: 2.9844939708709717
Step 1612 | grad_norm: 2.4794461727142334
Step 1612 | learning_rate: 4.826589595375723e-05
Step 1612 | epoch: 1.5529865125240847
Step 1613 | loss: 2.8889365196228027
Step 1613 | grad_norm: 2.558321475982666
Step 1613 | learning_rate: 4.8233782915863844e-05
Step 1613 | epoch: 1.5539499036608864
Step 1614 | loss: 3.8237152099609375
Step 1614 | grad_norm: 3.047456979751587
Step 1614 | learning_rate: 4.820166987797046e-05
Step 1614 | epoch: 1.5549132947976878
Step 1615 | loss: 2.548067569732666
Step 1615 | grad_norm: 1.9326093196868896
Step 1615 | learning_rate: 4.816955684007707e-05
Step 1615 | epoch: 1.5558766859344892
Step 1616 | loss: 2.640946388244629
Step 1616 | grad_norm: 2.484156608581543
Step 1616 | learning_rate: 4.813744380218369e-05
Step 1616 | epoch: 1.556840077071291
Step 1617 | loss: 3.318706512451172
Step 1617 | grad_norm: 2.5931320190429688
Step 1617 | learning_rate: 4.81053307642903e-05
Step 1617 | epoch: 1.5578034682080926
Step 1618 | loss: 3.544283390045166
Step 1618 | grad_norm: 2.301197052001953
Step 1618 | learning_rate: 4.807321772639692e-05
Step 1618 | epoch: 1.558766859344894
Step 1619 | loss: 3.4814820289611816
Step 1619 | grad_norm: 2.78196120262146
Step 1619 | learning_rate: 4.8041104688503536e-05
Step 1619 | epoch: 1.5597302504816954
Step 1620 | loss: 3.075147867202759
Step 1620 | grad_norm: 2.9987335205078125
Step 1620 | learning_rate: 4.800899165061015e-05
Step 1620 | epoch: 1.560693641618497
Step 1621 | loss: 3.8454575538635254
Step 1621 | grad_norm: 3.0441699028015137
Step 1621 | learning_rate: 4.7976878612716764e-05
Step 1621 | epoch: 1.5616570327552988
Step 1622 | loss: 3.3454785346984863
Step 1622 | grad_norm: 3.1448974609375
Step 1622 | learning_rate: 4.794476557482338e-05
Step 1622 | epoch: 1.5626204238921002
Step 1623 | loss: 3.6330339908599854
Step 1623 | grad_norm: 2.3607900142669678
Step 1623 | learning_rate: 4.791265253693e-05
Step 1623 | epoch: 1.5635838150289016
Step 1624 | loss: 3.345791816711426
Step 1624 | grad_norm: 3.593938112258911
Step 1624 | learning_rate: 4.7880539499036607e-05
Step 1624 | epoch: 1.5645472061657033
Step 1625 | loss: 3.0646770000457764
Step 1625 | grad_norm: 2.6921310424804688
Step 1625 | learning_rate: 4.784842646114323e-05
Step 1625 | epoch: 1.565510597302505
Step 1626 | loss: 3.333289861679077
Step 1626 | grad_norm: 2.592285394668579
Step 1626 | learning_rate: 4.781631342324984e-05
Step 1626 | epoch: 1.5664739884393064
Step 1627 | loss: 2.6957085132598877
Step 1627 | grad_norm: 2.575725555419922
Step 1627 | learning_rate: 4.7784200385356456e-05
Step 1627 | epoch: 1.5674373795761078
Step 1628 | loss: 2.8245043754577637
Step 1628 | grad_norm: 2.3014073371887207
Step 1628 | learning_rate: 4.775208734746307e-05
Step 1628 | epoch: 1.5684007707129095
Step 1629 | loss: 3.211615562438965
Step 1629 | grad_norm: 3.9829416275024414
Step 1629 | learning_rate: 4.7719974309569684e-05
Step 1629 | epoch: 1.569364161849711
Step 1630 | loss: 3.9149680137634277
Step 1630 | grad_norm: 3.037998914718628
Step 1630 | learning_rate: 4.7687861271676305e-05
Step 1630 | epoch: 1.5703275529865124
Step 1631 | loss: 3.2797791957855225
Step 1631 | grad_norm: 2.5279414653778076
Step 1631 | learning_rate: 4.765574823378292e-05
Step 1631 | epoch: 1.571290944123314
Step 1632 | loss: 3.1985652446746826
Step 1632 | grad_norm: 2.7463903427124023
Step 1632 | learning_rate: 4.7623635195889534e-05
Step 1632 | epoch: 1.5722543352601157
Step 1633 | loss: 3.906000852584839
Step 1633 | grad_norm: 3.012939929962158
Step 1633 | learning_rate: 4.759152215799615e-05
Step 1633 | epoch: 1.5732177263969171
Step 1634 | loss: 3.2047195434570312
Step 1634 | grad_norm: 3.943258762359619
Step 1634 | learning_rate: 4.755940912010277e-05
Step 1634 | epoch: 1.5741811175337186
Step 1635 | loss: 2.74334454536438
Step 1635 | grad_norm: 3.07462739944458
Step 1635 | learning_rate: 4.7527296082209376e-05
Step 1635 | epoch: 1.5751445086705202
Step 1636 | loss: 3.618502140045166
Step 1636 | grad_norm: 3.2824649810791016
Step 1636 | learning_rate: 4.7495183044316e-05
Step 1636 | epoch: 1.576107899807322
Step 1637 | loss: 3.455929756164551
Step 1637 | grad_norm: 2.9269258975982666
Step 1637 | learning_rate: 4.746307000642261e-05
Step 1637 | epoch: 1.5770712909441233
Step 1638 | loss: 3.6446170806884766
Step 1638 | grad_norm: 2.2132508754730225
Step 1638 | learning_rate: 4.7430956968529225e-05
Step 1638 | epoch: 1.5780346820809248
Step 1639 | loss: 3.6911211013793945
Step 1639 | grad_norm: 3.34421706199646
Step 1639 | learning_rate: 4.739884393063584e-05
Step 1639 | epoch: 1.5789980732177264
Step 1640 | loss: 2.9191672801971436
Step 1640 | grad_norm: 2.240816354751587
Step 1640 | learning_rate: 4.7366730892742454e-05
Step 1640 | epoch: 1.579961464354528
Step 1641 | loss: 3.179400682449341
Step 1641 | grad_norm: 2.7525997161865234
Step 1641 | learning_rate: 4.7334617854849075e-05
Step 1641 | epoch: 1.5809248554913293
Step 1642 | loss: 3.4979770183563232
Step 1642 | grad_norm: 3.4871277809143066
Step 1642 | learning_rate: 4.730250481695568e-05
Step 1642 | epoch: 1.581888246628131
Step 1643 | loss: 3.3757548332214355
Step 1643 | grad_norm: 3.153026819229126
Step 1643 | learning_rate: 4.72703917790623e-05
Step 1643 | epoch: 1.5828516377649327
Step 1644 | loss: 3.2965216636657715
Step 1644 | grad_norm: 2.815281867980957
Step 1644 | learning_rate: 4.723827874116892e-05
Step 1644 | epoch: 1.583815028901734
Step 1645 | loss: 3.0411171913146973
Step 1645 | grad_norm: 2.821136713027954
Step 1645 | learning_rate: 4.720616570327553e-05
Step 1645 | epoch: 1.5847784200385355
Step 1646 | loss: 3.329341411590576
Step 1646 | grad_norm: 2.34370493888855
Step 1646 | learning_rate: 4.7174052665382146e-05
Step 1646 | epoch: 1.5857418111753372
Step 1647 | loss: 3.111104726791382
Step 1647 | grad_norm: 2.854461908340454
Step 1647 | learning_rate: 4.714193962748876e-05
Step 1647 | epoch: 1.5867052023121389
Step 1648 | loss: 4.048500061035156
Step 1648 | grad_norm: 4.555839538574219
Step 1648 | learning_rate: 4.710982658959538e-05
Step 1648 | epoch: 1.5876685934489403
Step 1649 | loss: 3.351850986480713
Step 1649 | grad_norm: 2.527390480041504
Step 1649 | learning_rate: 4.707771355170199e-05
Step 1649 | epoch: 1.5886319845857417
Step 1650 | loss: 2.864096164703369
Step 1650 | grad_norm: 2.568984270095825
Step 1650 | learning_rate: 4.704560051380861e-05
Step 1650 | epoch: 1.5895953757225434
Step 1651 | loss: 3.289780855178833
Step 1651 | grad_norm: 2.470870018005371
Step 1651 | learning_rate: 4.7013487475915223e-05
Step 1651 | epoch: 1.590558766859345
Step 1652 | loss: 3.725104570388794
Step 1652 | grad_norm: 3.0858354568481445
Step 1652 | learning_rate: 4.6981374438021844e-05
Step 1652 | epoch: 1.5915221579961463
Step 1653 | loss: 3.56254506111145
Step 1653 | grad_norm: 3.016676187515259
Step 1653 | learning_rate: 4.694926140012845e-05
Step 1653 | epoch: 1.592485549132948
Step 1654 | loss: 3.385634660720825
Step 1654 | grad_norm: 2.9184253215789795
Step 1654 | learning_rate: 4.6917148362235066e-05
Step 1654 | epoch: 1.5934489402697496
Step 1655 | loss: 3.1300301551818848
Step 1655 | grad_norm: 2.8240561485290527
Step 1655 | learning_rate: 4.688503532434169e-05
Step 1655 | epoch: 1.594412331406551
Step 1656 | loss: 2.8421123027801514
Step 1656 | grad_norm: 3.3387625217437744
Step 1656 | learning_rate: 4.6852922286448294e-05
Step 1656 | epoch: 1.5953757225433525
Step 1657 | loss: 3.0829596519470215
Step 1657 | grad_norm: 2.6432361602783203
Step 1657 | learning_rate: 4.6820809248554915e-05
Step 1657 | epoch: 1.5963391136801541
Step 1658 | loss: 3.290332317352295
Step 1658 | grad_norm: 2.4921951293945312
Step 1658 | learning_rate: 4.678869621066153e-05
Step 1658 | epoch: 1.5973025048169558
Step 1659 | loss: 2.7603046894073486
Step 1659 | grad_norm: 2.7562241554260254
Step 1659 | learning_rate: 4.675658317276815e-05
Step 1659 | epoch: 1.5982658959537572
Step 1660 | loss: 3.7260963916778564
Step 1660 | grad_norm: 3.2242941856384277
Step 1660 | learning_rate: 4.672447013487476e-05
Step 1660 | epoch: 1.5992292870905587
Step 1661 | loss: 3.6056666374206543
Step 1661 | grad_norm: 2.8701171875
Step 1661 | learning_rate: 4.669235709698138e-05
Step 1661 | epoch: 1.6001926782273603
Step 1662 | loss: 2.896028995513916
Step 1662 | grad_norm: 2.132009506225586
Step 1662 | learning_rate: 4.666024405908799e-05
Step 1662 | epoch: 1.601156069364162
Step 1663 | loss: 2.581960916519165
Step 1663 | grad_norm: 2.136826515197754
Step 1663 | learning_rate: 4.662813102119461e-05
Step 1663 | epoch: 1.6021194605009634
Step 1664 | loss: 2.7320621013641357
Step 1664 | grad_norm: 3.427544116973877
Step 1664 | learning_rate: 4.659601798330122e-05
Step 1664 | epoch: 1.6030828516377649
Step 1665 | loss: 4.27435302734375
Step 1665 | grad_norm: 3.364553213119507
Step 1665 | learning_rate: 4.6563904945407836e-05
Step 1665 | epoch: 1.6040462427745665
Step 1666 | loss: 2.771765947341919
Step 1666 | grad_norm: 2.414557933807373
Step 1666 | learning_rate: 4.653179190751446e-05
Step 1666 | epoch: 1.605009633911368
Step 1667 | loss: 3.3211841583251953
Step 1667 | grad_norm: 2.3514444828033447
Step 1667 | learning_rate: 4.6499678869621064e-05
Step 1667 | epoch: 1.6059730250481694
Step 1668 | loss: 3.946107864379883
Step 1668 | grad_norm: 3.1699154376983643
Step 1668 | learning_rate: 4.6467565831727685e-05
Step 1668 | epoch: 1.606936416184971
Step 1669 | loss: 3.616004228591919
Step 1669 | grad_norm: 3.7993366718292236
Step 1669 | learning_rate: 4.64354527938343e-05
Step 1669 | epoch: 1.6078998073217727
Step 1670 | loss: 2.9199910163879395
Step 1670 | grad_norm: 2.651082754135132
Step 1670 | learning_rate: 4.640333975594091e-05
Step 1670 | epoch: 1.6088631984585742
Step 1671 | loss: 2.724933624267578
Step 1671 | grad_norm: 2.449218988418579
Step 1671 | learning_rate: 4.637122671804753e-05
Step 1671 | epoch: 1.6098265895953756
Step 1672 | loss: 4.31089448928833
Step 1672 | grad_norm: 4.413071632385254
Step 1672 | learning_rate: 4.633911368015414e-05
Step 1672 | epoch: 1.6107899807321773
Step 1673 | loss: 3.4305777549743652
Step 1673 | grad_norm: 3.238192319869995
Step 1673 | learning_rate: 4.630700064226076e-05
Step 1673 | epoch: 1.611753371868979
Step 1674 | loss: 3.3636958599090576
Step 1674 | grad_norm: 3.020871877670288
Step 1674 | learning_rate: 4.627488760436737e-05
Step 1674 | epoch: 1.6127167630057804
Step 1675 | loss: 3.76190185546875
Step 1675 | grad_norm: 3.3367979526519775
Step 1675 | learning_rate: 4.624277456647399e-05
Step 1675 | epoch: 1.6136801541425818
Step 1676 | loss: 2.971403121948242
Step 1676 | grad_norm: 2.455648183822632
Step 1676 | learning_rate: 4.6210661528580605e-05
Step 1676 | epoch: 1.6146435452793835
Step 1677 | loss: 3.9799187183380127
Step 1677 | grad_norm: 3.0692617893218994
Step 1677 | learning_rate: 4.6178548490687226e-05
Step 1677 | epoch: 1.6156069364161851
Step 1678 | loss: 3.1430606842041016
Step 1678 | grad_norm: 3.1184370517730713
Step 1678 | learning_rate: 4.6146435452793834e-05
Step 1678 | epoch: 1.6165703275529864
Step 1679 | loss: 3.4895458221435547
Step 1679 | grad_norm: 2.577214479446411
Step 1679 | learning_rate: 4.6114322414900455e-05
Step 1679 | epoch: 1.617533718689788
Step 1680 | loss: 3.7233359813690186
Step 1680 | grad_norm: 2.8255176544189453
Step 1680 | learning_rate: 4.608220937700707e-05
Step 1680 | epoch: 1.6184971098265897
Step 1681 | loss: 2.915727138519287
Step 1681 | grad_norm: 2.4813332557678223
Step 1681 | learning_rate: 4.605009633911368e-05
Step 1681 | epoch: 1.6194605009633911
Step 1682 | loss: 3.7458815574645996
Step 1682 | grad_norm: 3.4791228771209717
Step 1682 | learning_rate: 4.60179833012203e-05
Step 1682 | epoch: 1.6204238921001926
Step 1683 | loss: 3.3145413398742676
Step 1683 | grad_norm: 2.0947060585021973
Step 1683 | learning_rate: 4.598587026332691e-05
Step 1683 | epoch: 1.6213872832369942
Step 1684 | loss: 3.008361339569092
Step 1684 | grad_norm: 3.113821029663086
Step 1684 | learning_rate: 4.595375722543353e-05
Step 1684 | epoch: 1.6223506743737959
Step 1685 | loss: 2.5724947452545166
Step 1685 | grad_norm: 2.160336494445801
Step 1685 | learning_rate: 4.592164418754014e-05
Step 1685 | epoch: 1.6233140655105973
Step 1686 | loss: 3.1514668464660645
Step 1686 | grad_norm: 2.6358118057250977
Step 1686 | learning_rate: 4.588953114964676e-05
Step 1686 | epoch: 1.6242774566473988
Step 1687 | loss: 3.0402979850769043
Step 1687 | grad_norm: 2.912799119949341
Step 1687 | learning_rate: 4.5857418111753375e-05
Step 1687 | epoch: 1.6252408477842004
Step 1688 | loss: 3.839304208755493
Step 1688 | grad_norm: 3.076451539993286
Step 1688 | learning_rate: 4.582530507385999e-05
Step 1688 | epoch: 1.626204238921002
Step 1689 | loss: 3.1488850116729736
Step 1689 | grad_norm: 3.171138286590576
Step 1689 | learning_rate: 4.57931920359666e-05
Step 1689 | epoch: 1.6271676300578035
Step 1690 | loss: 3.237753391265869
Step 1690 | grad_norm: 2.830230712890625
Step 1690 | learning_rate: 4.576107899807322e-05
Step 1690 | epoch: 1.628131021194605
Step 1691 | loss: 3.7933735847473145
Step 1691 | grad_norm: 2.8064637184143066
Step 1691 | learning_rate: 4.572896596017984e-05
Step 1691 | epoch: 1.6290944123314066
Step 1692 | loss: 3.0989596843719482
Step 1692 | grad_norm: 2.7497854232788086
Step 1692 | learning_rate: 4.5696852922286446e-05
Step 1692 | epoch: 1.630057803468208
Step 1693 | loss: 3.487241744995117
Step 1693 | grad_norm: 4.0007734298706055
Step 1693 | learning_rate: 4.566473988439307e-05
Step 1693 | epoch: 1.6310211946050095
Step 1694 | loss: 3.631413698196411
Step 1694 | grad_norm: 3.2876880168914795
Step 1694 | learning_rate: 4.563262684649968e-05
Step 1694 | epoch: 1.6319845857418112
Step 1695 | loss: 3.2295987606048584
Step 1695 | grad_norm: 2.717957019805908
Step 1695 | learning_rate: 4.56005138086063e-05
Step 1695 | epoch: 1.6329479768786128
Step 1696 | loss: 3.0673508644104004
Step 1696 | grad_norm: 2.8492796421051025
Step 1696 | learning_rate: 4.556840077071291e-05
Step 1696 | epoch: 1.6339113680154143
Step 1697 | loss: 3.4612648487091064
Step 1697 | grad_norm: 2.574157238006592
Step 1697 | learning_rate: 4.553628773281953e-05
Step 1697 | epoch: 1.6348747591522157
Step 1698 | loss: 3.542684316635132
Step 1698 | grad_norm: 2.681234121322632
Step 1698 | learning_rate: 4.5504174694926145e-05
Step 1698 | epoch: 1.6358381502890174
Step 1699 | loss: 3.5555944442749023
Step 1699 | grad_norm: 3.0246591567993164
Step 1699 | learning_rate: 4.547206165703275e-05
Step 1699 | epoch: 1.636801541425819
Step 1700 | loss: 3.0084128379821777
Step 1700 | grad_norm: 2.9954912662506104
Step 1700 | learning_rate: 4.543994861913937e-05
Step 1700 | epoch: 1.6377649325626205
Step 1701 | loss: 3.2487173080444336
Step 1701 | grad_norm: 2.73873233795166
Step 1701 | learning_rate: 4.540783558124599e-05
Step 1701 | epoch: 1.638728323699422
Step 1702 | loss: 3.6266136169433594
Step 1702 | grad_norm: 3.445404052734375
Step 1702 | learning_rate: 4.537572254335261e-05
Step 1702 | epoch: 1.6396917148362236
Step 1703 | loss: 3.830366849899292
Step 1703 | grad_norm: 3.813399314880371
Step 1703 | learning_rate: 4.5343609505459215e-05
Step 1703 | epoch: 1.640655105973025
Step 1704 | loss: 3.173680543899536
Step 1704 | grad_norm: 3.221865177154541
Step 1704 | learning_rate: 4.5311496467565836e-05
Step 1704 | epoch: 1.6416184971098264
Step 1705 | loss: 3.7288219928741455
Step 1705 | grad_norm: 2.3857452869415283
Step 1705 | learning_rate: 4.527938342967245e-05
Step 1705 | epoch: 1.642581888246628
Step 1706 | loss: 3.4375
Step 1706 | grad_norm: 2.8699450492858887
Step 1706 | learning_rate: 4.5247270391779065e-05
Step 1706 | epoch: 1.6435452793834298
Step 1707 | loss: 4.3034844398498535
Step 1707 | grad_norm: 2.9507033824920654
Step 1707 | learning_rate: 4.521515735388568e-05
Step 1707 | epoch: 1.6445086705202312
Step 1708 | loss: 2.8988523483276367
Step 1708 | grad_norm: 2.3782100677490234
Step 1708 | learning_rate: 4.518304431599229e-05
Step 1708 | epoch: 1.6454720616570326
Step 1709 | loss: 3.3037338256835938
Step 1709 | grad_norm: 2.5968620777130127
Step 1709 | learning_rate: 4.5150931278098914e-05
Step 1709 | epoch: 1.6464354527938343
Step 1710 | loss: 3.859333038330078
Step 1710 | grad_norm: 3.022900104522705
Step 1710 | learning_rate: 4.511881824020552e-05
Step 1710 | epoch: 1.647398843930636
Step 1711 | loss: 3.6090362071990967
Step 1711 | grad_norm: 3.6550374031066895
Step 1711 | learning_rate: 4.508670520231214e-05
Step 1711 | epoch: 1.6483622350674374
Step 1712 | loss: 2.928112506866455
Step 1712 | grad_norm: 2.1801321506500244
Step 1712 | learning_rate: 4.505459216441876e-05
Step 1712 | epoch: 1.6493256262042388
Step 1713 | loss: 3.350238084793091
Step 1713 | grad_norm: 3.4014642238616943
Step 1713 | learning_rate: 4.502247912652537e-05
Step 1713 | epoch: 1.6502890173410405
Step 1714 | loss: 3.9553496837615967
Step 1714 | grad_norm: 3.0323479175567627
Step 1714 | learning_rate: 4.4990366088631985e-05
Step 1714 | epoch: 1.6512524084778422
Step 1715 | loss: 3.2174551486968994
Step 1715 | grad_norm: 2.798168182373047
Step 1715 | learning_rate: 4.49582530507386e-05
Step 1715 | epoch: 1.6522157996146436
Step 1716 | loss: 4.094215393066406
Step 1716 | grad_norm: 3.6358025074005127
Step 1716 | learning_rate: 4.492614001284522e-05
Step 1716 | epoch: 1.653179190751445
Step 1717 | loss: 2.7819278240203857
Step 1717 | grad_norm: 2.09260892868042
Step 1717 | learning_rate: 4.489402697495183e-05
Step 1717 | epoch: 1.6541425818882467
Step 1718 | loss: 3.7620797157287598
Step 1718 | grad_norm: 3.1760623455047607
Step 1718 | learning_rate: 4.486191393705845e-05
Step 1718 | epoch: 1.6551059730250481
Step 1719 | loss: 3.59643292427063
Step 1719 | grad_norm: 2.789903402328491
Step 1719 | learning_rate: 4.482980089916506e-05
Step 1719 | epoch: 1.6560693641618496
Step 1720 | loss: 3.2018604278564453
Step 1720 | grad_norm: 2.72645902633667
Step 1720 | learning_rate: 4.4797687861271684e-05
Step 1720 | epoch: 1.6570327552986512
Step 1721 | loss: 3.6108481884002686
Step 1721 | grad_norm: 3.8029518127441406
Step 1721 | learning_rate: 4.476557482337829e-05
Step 1721 | epoch: 1.657996146435453
Step 1722 | loss: 4.009017467498779
Step 1722 | grad_norm: 3.12933611869812
Step 1722 | learning_rate: 4.473346178548491e-05
Step 1722 | epoch: 1.6589595375722543
Step 1723 | loss: 3.4755754470825195
Step 1723 | grad_norm: 3.0923800468444824
Step 1723 | learning_rate: 4.4701348747591526e-05
Step 1723 | epoch: 1.6599229287090558
Step 1724 | loss: 3.0292866230010986
Step 1724 | grad_norm: 2.620368719100952
Step 1724 | learning_rate: 4.466923570969814e-05
Step 1724 | epoch: 1.6608863198458574
Step 1725 | loss: 2.647883653640747
Step 1725 | grad_norm: 2.566986083984375
Step 1725 | learning_rate: 4.4637122671804755e-05
Step 1725 | epoch: 1.661849710982659
Step 1726 | loss: 3.6468560695648193
Step 1726 | grad_norm: 5.843142032623291
Step 1726 | learning_rate: 4.460500963391137e-05
Step 1726 | epoch: 1.6628131021194605
Step 1727 | loss: 3.378920555114746
Step 1727 | grad_norm: 3.1656415462493896
Step 1727 | learning_rate: 4.457289659601799e-05
Step 1727 | epoch: 1.663776493256262
Step 1728 | loss: 3.222893476486206
Step 1728 | grad_norm: 2.557835817337036
Step 1728 | learning_rate: 4.45407835581246e-05
Step 1728 | epoch: 1.6647398843930636
Step 1729 | loss: 3.267606019973755
Step 1729 | grad_norm: 2.485222578048706
Step 1729 | learning_rate: 4.450867052023122e-05
Step 1729 | epoch: 1.665703275529865
Step 1730 | loss: 3.023728370666504
Step 1730 | grad_norm: 2.7958908081054688
Step 1730 | learning_rate: 4.447655748233783e-05
Step 1730 | epoch: 1.6666666666666665
Step 1731 | loss: 3.6773715019226074
Step 1731 | grad_norm: 3.358323812484741
Step 1731 | learning_rate: 4.4444444444444447e-05
Step 1731 | epoch: 1.6676300578034682
Step 1732 | loss: 3.1731693744659424
Step 1732 | grad_norm: 3.290069103240967
Step 1732 | learning_rate: 4.441233140655106e-05
Step 1732 | epoch: 1.6685934489402698
Step 1733 | loss: 3.4918711185455322
Step 1733 | grad_norm: 2.2122771739959717
Step 1733 | learning_rate: 4.4380218368657675e-05
Step 1733 | epoch: 1.6695568400770713
Step 1734 | loss: 3.3926641941070557
Step 1734 | grad_norm: 2.821937084197998
Step 1734 | learning_rate: 4.4348105330764296e-05
Step 1734 | epoch: 1.6705202312138727
Step 1735 | loss: 3.147061824798584
Step 1735 | grad_norm: 3.4038021564483643
Step 1735 | learning_rate: 4.43159922928709e-05
Step 1735 | epoch: 1.6714836223506744
Step 1736 | loss: 3.0527212619781494
Step 1736 | grad_norm: 4.833924770355225
Step 1736 | learning_rate: 4.4283879254977524e-05
Step 1736 | epoch: 1.672447013487476
Step 1737 | loss: 3.040229320526123
Step 1737 | grad_norm: 3.1453497409820557
Step 1737 | learning_rate: 4.425176621708414e-05
Step 1737 | epoch: 1.6734104046242775
Step 1738 | loss: 3.6996004581451416
Step 1738 | grad_norm: 3.100965738296509
Step 1738 | learning_rate: 4.421965317919075e-05
Step 1738 | epoch: 1.674373795761079
Step 1739 | loss: 3.279254913330078
Step 1739 | grad_norm: 3.4645707607269287
Step 1739 | learning_rate: 4.418754014129737e-05
Step 1739 | epoch: 1.6753371868978806
Step 1740 | loss: 3.4293861389160156
Step 1740 | grad_norm: 2.7945640087127686
Step 1740 | learning_rate: 4.415542710340399e-05
Step 1740 | epoch: 1.6763005780346822
Step 1741 | loss: 3.5351226329803467
Step 1741 | grad_norm: 2.9870126247406006
Step 1741 | learning_rate: 4.41233140655106e-05
Step 1741 | epoch: 1.6772639691714835
Step 1742 | loss: 2.9677605628967285
Step 1742 | grad_norm: 2.2545251846313477
Step 1742 | learning_rate: 4.409120102761721e-05
Step 1742 | epoch: 1.6782273603082851
Step 1743 | loss: 3.149095058441162
Step 1743 | grad_norm: 3.6124038696289062
Step 1743 | learning_rate: 4.405908798972383e-05
Step 1743 | epoch: 1.6791907514450868
Step 1744 | loss: 3.873828172683716
Step 1744 | grad_norm: 3.321061372756958
Step 1744 | learning_rate: 4.4026974951830445e-05
Step 1744 | epoch: 1.6801541425818882
Step 1745 | loss: 3.7987022399902344
Step 1745 | grad_norm: 2.7730872631073
Step 1745 | learning_rate: 4.3994861913937066e-05
Step 1745 | epoch: 1.6811175337186897
Step 1746 | loss: 3.117372512817383
Step 1746 | grad_norm: 2.0342283248901367
Step 1746 | learning_rate: 4.396274887604367e-05
Step 1746 | epoch: 1.6820809248554913
Step 1747 | loss: 3.580976724624634
Step 1747 | grad_norm: 2.80804705619812
Step 1747 | learning_rate: 4.3930635838150294e-05
Step 1747 | epoch: 1.683044315992293
Step 1748 | loss: 3.51543927192688
Step 1748 | grad_norm: 2.9341721534729004
Step 1748 | learning_rate: 4.389852280025691e-05
Step 1748 | epoch: 1.6840077071290944
Step 1749 | loss: 3.0793492794036865
Step 1749 | grad_norm: 2.937061071395874
Step 1749 | learning_rate: 4.386640976236352e-05
Step 1749 | epoch: 1.6849710982658959
Step 1750 | loss: 3.9420814514160156
Step 1750 | grad_norm: 3.10082745552063
Step 1750 | learning_rate: 4.3834296724470136e-05
Step 1750 | epoch: 1.6859344894026975
Step 1751 | loss: 2.8229176998138428
Step 1751 | grad_norm: 2.274895429611206
Step 1751 | learning_rate: 4.380218368657675e-05
Step 1751 | epoch: 1.6868978805394992
Step 1752 | loss: 3.7740864753723145
Step 1752 | grad_norm: 3.138896942138672
Step 1752 | learning_rate: 4.377007064868337e-05
Step 1752 | epoch: 1.6878612716763006
Step 1753 | loss: 2.8798043727874756
Step 1753 | grad_norm: 1.9433104991912842
Step 1753 | learning_rate: 4.373795761078998e-05
Step 1753 | epoch: 1.688824662813102
Step 1754 | loss: 2.9588801860809326
Step 1754 | grad_norm: 2.3725666999816895
Step 1754 | learning_rate: 4.37058445728966e-05
Step 1754 | epoch: 1.6897880539499037
Step 1755 | loss: 3.342811107635498
Step 1755 | grad_norm: 2.660106658935547
Step 1755 | learning_rate: 4.3673731535003214e-05
Step 1755 | epoch: 1.6907514450867052
Step 1756 | loss: 2.9167778491973877
Step 1756 | grad_norm: 2.196235418319702
Step 1756 | learning_rate: 4.364161849710983e-05
Step 1756 | epoch: 1.6917148362235066
Step 1757 | loss: 3.595501661300659
Step 1757 | grad_norm: 2.5942800045013428
Step 1757 | learning_rate: 4.360950545921644e-05
Step 1757 | epoch: 1.6926782273603083
Step 1758 | loss: 2.976104974746704
Step 1758 | grad_norm: 2.8999764919281006
Step 1758 | learning_rate: 4.357739242132306e-05
Step 1758 | epoch: 1.69364161849711
Step 1759 | loss: 3.9853708744049072
Step 1759 | grad_norm: 2.8135132789611816
Step 1759 | learning_rate: 4.354527938342968e-05
Step 1759 | epoch: 1.6946050096339114
Step 1760 | loss: 3.609400510787964
Step 1760 | grad_norm: 3.364301919937134
Step 1760 | learning_rate: 4.3513166345536285e-05
Step 1760 | epoch: 1.6955684007707128
Step 1761 | loss: 3.0334908962249756
Step 1761 | grad_norm: 3.902402400970459
Step 1761 | learning_rate: 4.3481053307642906e-05
Step 1761 | epoch: 1.6965317919075145
Step 1762 | loss: 2.7972662448883057
Step 1762 | grad_norm: 2.7675461769104004
Step 1762 | learning_rate: 4.344894026974952e-05
Step 1762 | epoch: 1.6974951830443161
Step 1763 | loss: 3.3093833923339844
Step 1763 | grad_norm: 2.344897985458374
Step 1763 | learning_rate: 4.3416827231856134e-05
Step 1763 | epoch: 1.6984585741811176
Step 1764 | loss: 3.563054323196411
Step 1764 | grad_norm: 3.2203214168548584
Step 1764 | learning_rate: 4.338471419396275e-05
Step 1764 | epoch: 1.699421965317919
Step 1765 | loss: 2.942498207092285
Step 1765 | grad_norm: 2.245039939880371
Step 1765 | learning_rate: 4.335260115606937e-05
Step 1765 | epoch: 1.7003853564547207
Step 1766 | loss: 4.66654634475708
Step 1766 | grad_norm: 3.9456279277801514
Step 1766 | learning_rate: 4.3320488118175984e-05
Step 1766 | epoch: 1.701348747591522
Step 1767 | loss: 3.509878396987915
Step 1767 | grad_norm: 3.103106737136841
Step 1767 | learning_rate: 4.32883750802826e-05
Step 1767 | epoch: 1.7023121387283235
Step 1768 | loss: 2.325671434402466
Step 1768 | grad_norm: 2.332340955734253
Step 1768 | learning_rate: 4.325626204238921e-05
Step 1768 | epoch: 1.7032755298651252
Step 1769 | loss: 3.5559990406036377
Step 1769 | grad_norm: 2.46905255317688
Step 1769 | learning_rate: 4.3224149004495826e-05
Step 1769 | epoch: 1.7042389210019269
Step 1770 | loss: 2.908794641494751
Step 1770 | grad_norm: 2.6328001022338867
Step 1770 | learning_rate: 4.319203596660244e-05
Step 1770 | epoch: 1.7052023121387283
Step 1771 | loss: 3.642778158187866
Step 1771 | grad_norm: 2.3122141361236572
Step 1771 | learning_rate: 4.3159922928709055e-05
Step 1771 | epoch: 1.7061657032755297
Step 1772 | loss: 3.26080584526062
Step 1772 | grad_norm: 3.037806987762451
Step 1772 | learning_rate: 4.3127809890815676e-05
Step 1772 | epoch: 1.7071290944123314
Step 1773 | loss: 2.8882102966308594
Step 1773 | grad_norm: 3.208631992340088
Step 1773 | learning_rate: 4.309569685292229e-05
Step 1773 | epoch: 1.708092485549133
Step 1774 | loss: 3.438429832458496
Step 1774 | grad_norm: 2.4604759216308594
Step 1774 | learning_rate: 4.3063583815028904e-05
Step 1774 | epoch: 1.7090558766859345
Step 1775 | loss: 3.5421254634857178
Step 1775 | grad_norm: 3.0762953758239746
Step 1775 | learning_rate: 4.303147077713552e-05
Step 1775 | epoch: 1.710019267822736
Step 1776 | loss: 2.6878104209899902
Step 1776 | grad_norm: 2.5755045413970947
Step 1776 | learning_rate: 4.299935773924213e-05
Step 1776 | epoch: 1.7109826589595376
Step 1777 | loss: 3.026458501815796
Step 1777 | grad_norm: 3.61653733253479
Step 1777 | learning_rate: 4.2967244701348753e-05
Step 1777 | epoch: 1.7119460500963393
Step 1778 | loss: 2.690340995788574
Step 1778 | grad_norm: 2.7422990798950195
Step 1778 | learning_rate: 4.293513166345536e-05
Step 1778 | epoch: 1.7129094412331407
Step 1779 | loss: 3.1825311183929443
Step 1779 | grad_norm: 2.381626605987549
Step 1779 | learning_rate: 4.290301862556198e-05
Step 1779 | epoch: 1.7138728323699421
Step 1780 | loss: 3.3366875648498535
Step 1780 | grad_norm: 3.1011383533477783
Step 1780 | learning_rate: 4.2870905587668596e-05
Step 1780 | epoch: 1.7148362235067438
Step 1781 | loss: 3.2926950454711914
Step 1781 | grad_norm: 3.598508596420288
Step 1781 | learning_rate: 4.283879254977521e-05
Step 1781 | epoch: 1.7157996146435452
Step 1782 | loss: 3.275965690612793
Step 1782 | grad_norm: 2.406003475189209
Step 1782 | learning_rate: 4.2806679511881824e-05
Step 1782 | epoch: 1.7167630057803467
Step 1783 | loss: 3.3999688625335693
Step 1783 | grad_norm: 2.5567896366119385
Step 1783 | learning_rate: 4.2774566473988445e-05
Step 1783 | epoch: 1.7177263969171483
Step 1784 | loss: 3.428274393081665
Step 1784 | grad_norm: 3.2549262046813965
Step 1784 | learning_rate: 4.274245343609506e-05
Step 1784 | epoch: 1.71868978805395
Step 1785 | loss: 2.46174693107605
Step 1785 | grad_norm: 2.6736552715301514
Step 1785 | learning_rate: 4.2710340398201674e-05
Step 1785 | epoch: 1.7196531791907514
Step 1786 | loss: 3.379551410675049
Step 1786 | grad_norm: 2.6046438217163086
Step 1786 | learning_rate: 4.267822736030829e-05
Step 1786 | epoch: 1.7206165703275529
Step 1787 | loss: 3.2331159114837646
Step 1787 | grad_norm: 3.3422935009002686
Step 1787 | learning_rate: 4.26461143224149e-05
Step 1787 | epoch: 1.7215799614643545
Step 1788 | loss: 3.2252981662750244
Step 1788 | grad_norm: 2.3855788707733154
Step 1788 | learning_rate: 4.2614001284521516e-05
Step 1788 | epoch: 1.7225433526011562
Step 1789 | loss: 3.4056460857391357
Step 1789 | grad_norm: 3.097764253616333
Step 1789 | learning_rate: 4.258188824662813e-05
Step 1789 | epoch: 1.7235067437379576
Step 1790 | loss: 4.49647855758667
Step 1790 | grad_norm: 3.8500735759735107
Step 1790 | learning_rate: 4.254977520873475e-05
Step 1790 | epoch: 1.724470134874759
Step 1791 | loss: 2.502741813659668
Step 1791 | grad_norm: 3.1826727390289307
Step 1791 | learning_rate: 4.2517662170841366e-05
Step 1791 | epoch: 1.7254335260115607
Step 1792 | loss: 3.4884870052337646
Step 1792 | grad_norm: 3.3766539096832275
Step 1792 | learning_rate: 4.248554913294798e-05
Step 1792 | epoch: 1.7263969171483622
Step 1793 | loss: 3.791576623916626
Step 1793 | grad_norm: 3.3120009899139404
Step 1793 | learning_rate: 4.2453436095054594e-05
Step 1793 | epoch: 1.7273603082851636
Step 1794 | loss: 2.841641426086426
Step 1794 | grad_norm: 2.8473386764526367
Step 1794 | learning_rate: 4.242132305716121e-05
Step 1794 | epoch: 1.7283236994219653
Step 1795 | loss: 2.8871519565582275
Step 1795 | grad_norm: 2.453326463699341
Step 1795 | learning_rate: 4.238921001926782e-05
Step 1795 | epoch: 1.729287090558767
Step 1796 | loss: 3.531024932861328
Step 1796 | grad_norm: 3.740295171737671
Step 1796 | learning_rate: 4.2357096981374437e-05
Step 1796 | epoch: 1.7302504816955684
Step 1797 | loss: 3.240126132965088
Step 1797 | grad_norm: 2.5854332447052
Step 1797 | learning_rate: 4.232498394348106e-05
Step 1797 | epoch: 1.7312138728323698
Step 1798 | loss: 3.7898435592651367
Step 1798 | grad_norm: 3.37107515335083
Step 1798 | learning_rate: 4.229287090558767e-05
Step 1798 | epoch: 1.7321772639691715
Step 1799 | loss: 2.815458059310913
Step 1799 | grad_norm: 2.953232765197754
Step 1799 | learning_rate: 4.2260757867694286e-05
Step 1799 | epoch: 1.7331406551059731
Step 1800 | loss: 3.383817434310913
Step 1800 | grad_norm: 2.5570945739746094
Step 1800 | learning_rate: 4.22286448298009e-05
Step 1800 | epoch: 1.7341040462427746
Step 1801 | loss: 2.6227080821990967
Step 1801 | grad_norm: 2.487269163131714
Step 1801 | learning_rate: 4.2196531791907514e-05
Step 1801 | epoch: 1.735067437379576
Step 1802 | loss: 4.074926376342773
Step 1802 | grad_norm: 2.9812662601470947
Step 1802 | learning_rate: 4.2164418754014135e-05
Step 1802 | epoch: 1.7360308285163777
Step 1803 | loss: 2.855858087539673
Step 1803 | grad_norm: 2.7262027263641357
Step 1803 | learning_rate: 4.213230571612074e-05
Step 1803 | epoch: 1.7369942196531793
Step 1804 | loss: 3.2655818462371826
Step 1804 | grad_norm: 4.1488261222839355
Step 1804 | learning_rate: 4.2100192678227364e-05
Step 1804 | epoch: 1.7379576107899806
Step 1805 | loss: 3.319206953048706
Step 1805 | grad_norm: 2.667339563369751
Step 1805 | learning_rate: 4.206807964033398e-05
Step 1805 | epoch: 1.7389210019267822
Step 1806 | loss: 3.3511157035827637
Step 1806 | grad_norm: 3.1281721591949463
Step 1806 | learning_rate: 4.203596660244059e-05
Step 1806 | epoch: 1.739884393063584
Step 1807 | loss: 3.8812344074249268
Step 1807 | grad_norm: 3.3354697227478027
Step 1807 | learning_rate: 4.2003853564547206e-05
Step 1807 | epoch: 1.7408477842003853
Step 1808 | loss: 3.7202725410461426
Step 1808 | grad_norm: 3.6715803146362305
Step 1808 | learning_rate: 4.197174052665383e-05
Step 1808 | epoch: 1.7418111753371868
Step 1809 | loss: 3.702772855758667
Step 1809 | grad_norm: 2.780154228210449
Step 1809 | learning_rate: 4.193962748876044e-05
Step 1809 | epoch: 1.7427745664739884
Step 1810 | loss: 3.320216655731201
Step 1810 | grad_norm: 2.5081751346588135
Step 1810 | learning_rate: 4.1907514450867055e-05
Step 1810 | epoch: 1.74373795761079
Step 1811 | loss: 3.7142138481140137
Step 1811 | grad_norm: 2.646418809890747
Step 1811 | learning_rate: 4.187540141297367e-05
Step 1811 | epoch: 1.7447013487475915
Step 1812 | loss: 3.026726722717285
Step 1812 | grad_norm: 2.7517011165618896
Step 1812 | learning_rate: 4.1843288375080284e-05
Step 1812 | epoch: 1.745664739884393
Step 1813 | loss: 4.453882694244385
Step 1813 | grad_norm: 2.9170444011688232
Step 1813 | learning_rate: 4.18111753371869e-05
Step 1813 | epoch: 1.7466281310211946
Step 1814 | loss: 3.7743306159973145
Step 1814 | grad_norm: 2.759915590286255
Step 1814 | learning_rate: 4.177906229929351e-05
Step 1814 | epoch: 1.7475915221579963
Step 1815 | loss: 3.549490451812744
Step 1815 | grad_norm: 3.3356847763061523
Step 1815 | learning_rate: 4.174694926140013e-05
Step 1815 | epoch: 1.7485549132947977
Step 1816 | loss: 2.9625613689422607
Step 1816 | grad_norm: 2.4073686599731445
Step 1816 | learning_rate: 4.171483622350675e-05
Step 1816 | epoch: 1.7495183044315992
Step 1817 | loss: 3.335857629776001
Step 1817 | grad_norm: 2.5623836517333984
Step 1817 | learning_rate: 4.168272318561336e-05
Step 1817 | epoch: 1.7504816955684008
Step 1818 | loss: 2.451319932937622
Step 1818 | grad_norm: 2.1990609169006348
Step 1818 | learning_rate: 4.1650610147719976e-05
Step 1818 | epoch: 1.7514450867052023
Step 1819 | loss: 3.287705659866333
Step 1819 | grad_norm: 3.0530002117156982
Step 1819 | learning_rate: 4.161849710982659e-05
Step 1819 | epoch: 1.7524084778420037
Step 1820 | loss: 3.03322172164917
Step 1820 | grad_norm: 3.0166451930999756
Step 1820 | learning_rate: 4.1586384071933204e-05
Step 1820 | epoch: 1.7533718689788054
Step 1821 | loss: 3.784163236618042
Step 1821 | grad_norm: 3.325402021408081
Step 1821 | learning_rate: 4.155427103403982e-05
Step 1821 | epoch: 1.754335260115607
Step 1822 | loss: 3.5032763481140137
Step 1822 | grad_norm: 3.8441693782806396
Step 1822 | learning_rate: 4.152215799614644e-05
Step 1822 | epoch: 1.7552986512524085
Step 1823 | loss: 3.056363582611084
Step 1823 | grad_norm: 3.002176284790039
Step 1823 | learning_rate: 4.1490044958253053e-05
Step 1823 | epoch: 1.75626204238921
Step 1824 | loss: 3.7194008827209473
Step 1824 | grad_norm: 2.75190806388855
Step 1824 | learning_rate: 4.145793192035967e-05
Step 1824 | epoch: 1.7572254335260116
Step 1825 | loss: 3.5702250003814697
Step 1825 | grad_norm: 2.7394096851348877
Step 1825 | learning_rate: 4.142581888246628e-05
Step 1825 | epoch: 1.7581888246628132
Step 1826 | loss: 3.155184030532837
Step 1826 | grad_norm: 3.2359392642974854
Step 1826 | learning_rate: 4.13937058445729e-05
Step 1826 | epoch: 1.7591522157996147
Step 1827 | loss: 3.185419797897339
Step 1827 | grad_norm: 2.9200375080108643
Step 1827 | learning_rate: 4.136159280667952e-05
Step 1827 | epoch: 1.760115606936416
Step 1828 | loss: 3.108750104904175
Step 1828 | grad_norm: 3.277777671813965
Step 1828 | learning_rate: 4.132947976878613e-05
Step 1828 | epoch: 1.7610789980732178
Step 1829 | loss: 3.6690621376037598
Step 1829 | grad_norm: 2.9691262245178223
Step 1829 | learning_rate: 4.1297366730892745e-05
Step 1829 | epoch: 1.7620423892100194
Step 1830 | loss: 3.953024387359619
Step 1830 | grad_norm: 3.5061020851135254
Step 1830 | learning_rate: 4.126525369299936e-05
Step 1830 | epoch: 1.7630057803468207
Step 1831 | loss: 3.4984805583953857
Step 1831 | grad_norm: 2.832296371459961
Step 1831 | learning_rate: 4.1233140655105974e-05
Step 1831 | epoch: 1.7639691714836223
Step 1832 | loss: 2.971320390701294
Step 1832 | grad_norm: 2.487257719039917
Step 1832 | learning_rate: 4.120102761721259e-05
Step 1832 | epoch: 1.764932562620424
Step 1833 | loss: 3.102776288986206
Step 1833 | grad_norm: 2.7258412837982178
Step 1833 | learning_rate: 4.116891457931921e-05
Step 1833 | epoch: 1.7658959537572254
Step 1834 | loss: 3.4972240924835205
Step 1834 | grad_norm: 3.550205707550049
Step 1834 | learning_rate: 4.113680154142582e-05
Step 1834 | epoch: 1.7668593448940269
Step 1835 | loss: 3.7917463779449463
Step 1835 | grad_norm: 3.1377503871917725
Step 1835 | learning_rate: 4.110468850353244e-05
Step 1835 | epoch: 1.7678227360308285
Step 1836 | loss: 2.783336877822876
Step 1836 | grad_norm: 3.1488637924194336
Step 1836 | learning_rate: 4.107257546563905e-05
Step 1836 | epoch: 1.7687861271676302
Step 1837 | loss: 2.8880057334899902
Step 1837 | grad_norm: 2.87028169631958
Step 1837 | learning_rate: 4.1040462427745666e-05
Step 1837 | epoch: 1.7697495183044316
Step 1838 | loss: 4.152893543243408
Step 1838 | grad_norm: 3.057671308517456
Step 1838 | learning_rate: 4.100834938985228e-05
Step 1838 | epoch: 1.770712909441233
Step 1839 | loss: 2.8436014652252197
Step 1839 | grad_norm: 2.6629245281219482
Step 1839 | learning_rate: 4.0976236351958894e-05
Step 1839 | epoch: 1.7716763005780347
Step 1840 | loss: 3.5297553539276123
Step 1840 | grad_norm: 4.7093048095703125
Step 1840 | learning_rate: 4.0944123314065515e-05
Step 1840 | epoch: 1.7726396917148364
Step 1841 | loss: 3.3857343196868896
Step 1841 | grad_norm: 3.215759038925171
Step 1841 | learning_rate: 4.091201027617213e-05
Step 1841 | epoch: 1.7736030828516378
Step 1842 | loss: 4.004676818847656
Step 1842 | grad_norm: 3.0476772785186768
Step 1842 | learning_rate: 4.087989723827874e-05
Step 1842 | epoch: 1.7745664739884393
Step 1843 | loss: 2.8929126262664795
Step 1843 | grad_norm: 2.4824042320251465
Step 1843 | learning_rate: 4.084778420038536e-05
Step 1843 | epoch: 1.775529865125241
Step 1844 | loss: 2.318418025970459
Step 1844 | grad_norm: 2.18532657623291
Step 1844 | learning_rate: 4.081567116249198e-05
Step 1844 | epoch: 1.7764932562620424
Step 1845 | loss: 3.6916279792785645
Step 1845 | grad_norm: 4.141098976135254
Step 1845 | learning_rate: 4.0783558124598586e-05
Step 1845 | epoch: 1.7774566473988438
Step 1846 | loss: 2.9297447204589844
Step 1846 | grad_norm: 3.1449739933013916
Step 1846 | learning_rate: 4.07514450867052e-05
Step 1846 | epoch: 1.7784200385356455
Step 1847 | loss: 3.003509521484375
Step 1847 | grad_norm: 2.6016080379486084
Step 1847 | learning_rate: 4.071933204881182e-05
Step 1847 | epoch: 1.7793834296724471
Step 1848 | loss: 3.716006278991699
Step 1848 | grad_norm: 2.833185911178589
Step 1848 | learning_rate: 4.0687219010918435e-05
Step 1848 | epoch: 1.7803468208092486
Step 1849 | loss: 2.7649314403533936
Step 1849 | grad_norm: 2.576263904571533
Step 1849 | learning_rate: 4.065510597302505e-05
Step 1849 | epoch: 1.78131021194605
Step 1850 | loss: 3.391950845718384
Step 1850 | grad_norm: 2.6129539012908936
Step 1850 | learning_rate: 4.0622992935131664e-05
Step 1850 | epoch: 1.7822736030828517
Step 1851 | loss: 3.842907667160034
Step 1851 | grad_norm: 2.7718594074249268
Step 1851 | learning_rate: 4.0590879897238285e-05
Step 1851 | epoch: 1.7832369942196533
Step 1852 | loss: 2.772867441177368
Step 1852 | grad_norm: 2.389569044113159
Step 1852 | learning_rate: 4.055876685934489e-05
Step 1852 | epoch: 1.7842003853564548
Step 1853 | loss: 3.566945791244507
Step 1853 | grad_norm: 3.511507511138916
Step 1853 | learning_rate: 4.052665382145151e-05
Step 1853 | epoch: 1.7851637764932562
Step 1854 | loss: 3.041963577270508
Step 1854 | grad_norm: 2.730797529220581
Step 1854 | learning_rate: 4.049454078355813e-05
Step 1854 | epoch: 1.7861271676300579
Step 1855 | loss: 3.494004964828491
Step 1855 | grad_norm: 2.8842532634735107
Step 1855 | learning_rate: 4.046242774566474e-05
Step 1855 | epoch: 1.7870905587668593
Step 1856 | loss: 3.1617438793182373
Step 1856 | grad_norm: 3.2417125701904297
Step 1856 | learning_rate: 4.0430314707771356e-05
Step 1856 | epoch: 1.7880539499036607
Step 1857 | loss: 3.4944987297058105
Step 1857 | grad_norm: 2.670444965362549
Step 1857 | learning_rate: 4.039820166987797e-05
Step 1857 | epoch: 1.7890173410404624
Step 1858 | loss: 3.3879683017730713
Step 1858 | grad_norm: 2.6894800662994385
Step 1858 | learning_rate: 4.036608863198459e-05
Step 1858 | epoch: 1.789980732177264
Step 1859 | loss: 3.235825777053833
Step 1859 | grad_norm: 2.812006711959839
Step 1859 | learning_rate: 4.0333975594091205e-05
Step 1859 | epoch: 1.7909441233140655
Step 1860 | loss: 3.9642794132232666
Step 1860 | grad_norm: 2.6339547634124756
Step 1860 | learning_rate: 4.030186255619782e-05
Step 1860 | epoch: 1.791907514450867
Step 1861 | loss: 3.1638166904449463
Step 1861 | grad_norm: 2.2952182292938232
Step 1861 | learning_rate: 4.026974951830443e-05
Step 1861 | epoch: 1.7928709055876686
Step 1862 | loss: 3.264195442199707
Step 1862 | grad_norm: 2.4884142875671387
Step 1862 | learning_rate: 4.023763648041105e-05
Step 1862 | epoch: 1.7938342967244703
Step 1863 | loss: 3.4018125534057617
Step 1863 | grad_norm: 3.0064847469329834
Step 1863 | learning_rate: 4.020552344251766e-05
Step 1863 | epoch: 1.7947976878612717
Step 1864 | loss: 2.9158504009246826
Step 1864 | grad_norm: 2.998589038848877
Step 1864 | learning_rate: 4.0173410404624276e-05
Step 1864 | epoch: 1.7957610789980731
Step 1865 | loss: 3.327580690383911
Step 1865 | grad_norm: 2.623793125152588
Step 1865 | learning_rate: 4.01412973667309e-05
Step 1865 | epoch: 1.7967244701348748
Step 1866 | loss: 3.6461381912231445
Step 1866 | grad_norm: 2.9744887351989746
Step 1866 | learning_rate: 4.010918432883751e-05
Step 1866 | epoch: 1.7976878612716765
Step 1867 | loss: 2.5158164501190186
Step 1867 | grad_norm: 2.4720585346221924
Step 1867 | learning_rate: 4.0077071290944125e-05
Step 1867 | epoch: 1.798651252408478
Step 1868 | loss: 3.0714428424835205
Step 1868 | grad_norm: 2.346268653869629
Step 1868 | learning_rate: 4.004495825305074e-05
Step 1868 | epoch: 1.7996146435452793
Step 1869 | loss: 3.2988686561584473
Step 1869 | grad_norm: 3.5091848373413086
Step 1869 | learning_rate: 4.001284521515736e-05
Step 1869 | epoch: 1.800578034682081
Step 1870 | loss: 3.0152885913848877
Step 1870 | grad_norm: 2.6180758476257324
Step 1870 | learning_rate: 3.998073217726397e-05
Step 1870 | epoch: 1.8015414258188824
Step 1871 | loss: 3.618922472000122
Step 1871 | grad_norm: 2.832280397415161
Step 1871 | learning_rate: 3.994861913937059e-05
Step 1871 | epoch: 1.8025048169556839
Step 1872 | loss: 3.225550889968872
Step 1872 | grad_norm: 2.7787632942199707
Step 1872 | learning_rate: 3.99165061014772e-05
Step 1872 | epoch: 1.8034682080924855
Step 1873 | loss: 3.4601619243621826
Step 1873 | grad_norm: 2.904789924621582
Step 1873 | learning_rate: 3.988439306358382e-05
Step 1873 | epoch: 1.8044315992292872
Step 1874 | loss: 3.185427665710449
Step 1874 | grad_norm: 2.62705397605896
Step 1874 | learning_rate: 3.985228002569043e-05
Step 1874 | epoch: 1.8053949903660886
Step 1875 | loss: 3.427917718887329
Step 1875 | grad_norm: 2.9274168014526367
Step 1875 | learning_rate: 3.9820166987797045e-05
Step 1875 | epoch: 1.80635838150289
Step 1876 | loss: 3.590701103210449
Step 1876 | grad_norm: 3.1634764671325684
Step 1876 | learning_rate: 3.9788053949903666e-05
Step 1876 | epoch: 1.8073217726396917
Step 1877 | loss: 3.9983057975769043
Step 1877 | grad_norm: 4.085692882537842
Step 1877 | learning_rate: 3.9755940912010274e-05
Step 1877 | epoch: 1.8082851637764934
Step 1878 | loss: 3.596503496170044
Step 1878 | grad_norm: 2.834606170654297
Step 1878 | learning_rate: 3.9723827874116895e-05
Step 1878 | epoch: 1.8092485549132948
Step 1879 | loss: 3.2166943550109863
Step 1879 | grad_norm: 3.2415077686309814
Step 1879 | learning_rate: 3.969171483622351e-05
Step 1879 | epoch: 1.8102119460500963
Step 1880 | loss: 3.9946281909942627
Step 1880 | grad_norm: 3.7865684032440186
Step 1880 | learning_rate: 3.965960179833012e-05
Step 1880 | epoch: 1.811175337186898
Step 1881 | loss: 3.4432637691497803
Step 1881 | grad_norm: 3.5758888721466064
Step 1881 | learning_rate: 3.962748876043674e-05
Step 1881 | epoch: 1.8121387283236994
Step 1882 | loss: 3.612271785736084
Step 1882 | grad_norm: 3.1799848079681396
Step 1882 | learning_rate: 3.959537572254335e-05
Step 1882 | epoch: 1.8131021194605008
Step 1883 | loss: 3.592973470687866
Step 1883 | grad_norm: 3.392745018005371
Step 1883 | learning_rate: 3.956326268464997e-05
Step 1883 | epoch: 1.8140655105973025
Step 1884 | loss: 3.425670623779297
Step 1884 | grad_norm: 3.601349115371704
Step 1884 | learning_rate: 3.953114964675659e-05
Step 1884 | epoch: 1.8150289017341041
Step 1885 | loss: 2.38777494430542
Step 1885 | grad_norm: 2.0534720420837402
Step 1885 | learning_rate: 3.94990366088632e-05
Step 1885 | epoch: 1.8159922928709056
Step 1886 | loss: 2.318514585494995
Step 1886 | grad_norm: 2.221782922744751
Step 1886 | learning_rate: 3.9466923570969815e-05
Step 1886 | epoch: 1.816955684007707
Step 1887 | loss: 3.700815200805664
Step 1887 | grad_norm: 2.8456740379333496
Step 1887 | learning_rate: 3.9434810533076436e-05
Step 1887 | epoch: 1.8179190751445087
Step 1888 | loss: 3.8765134811401367
Step 1888 | grad_norm: 2.363245725631714
Step 1888 | learning_rate: 3.9402697495183043e-05
Step 1888 | epoch: 1.8188824662813103
Step 1889 | loss: 3.7319998741149902
Step 1889 | grad_norm: 3.0083343982696533
Step 1889 | learning_rate: 3.937058445728966e-05
Step 1889 | epoch: 1.8198458574181118
Step 1890 | loss: 3.1435434818267822
Step 1890 | grad_norm: 2.639390468597412
Step 1890 | learning_rate: 3.933847141939628e-05
Step 1890 | epoch: 1.8208092485549132
Step 1891 | loss: 3.5246803760528564
Step 1891 | grad_norm: 2.9076786041259766
Step 1891 | learning_rate: 3.930635838150289e-05
Step 1891 | epoch: 1.8217726396917149
Step 1892 | loss: 3.4222958087921143
Step 1892 | grad_norm: 3.3648035526275635
Step 1892 | learning_rate: 3.927424534360951e-05
Step 1892 | epoch: 1.8227360308285165
Step 1893 | loss: 2.3056976795196533
Step 1893 | grad_norm: 2.689957857131958
Step 1893 | learning_rate: 3.924213230571612e-05
Step 1893 | epoch: 1.8236994219653178
Step 1894 | loss: 3.3135311603546143
Step 1894 | grad_norm: 3.1112000942230225
Step 1894 | learning_rate: 3.921001926782274e-05
Step 1894 | epoch: 1.8246628131021194
Step 1895 | loss: 3.389890670776367
Step 1895 | grad_norm: 3.2066538333892822
Step 1895 | learning_rate: 3.917790622992935e-05
Step 1895 | epoch: 1.825626204238921
Step 1896 | loss: 2.675185203552246
Step 1896 | grad_norm: 2.6187565326690674
Step 1896 | learning_rate: 3.914579319203597e-05
Step 1896 | epoch: 1.8265895953757225
Step 1897 | loss: 3.1100029945373535
Step 1897 | grad_norm: 2.8442628383636475
Step 1897 | learning_rate: 3.9113680154142585e-05
Step 1897 | epoch: 1.827552986512524
Step 1898 | loss: 3.2955167293548584
Step 1898 | grad_norm: 2.7949562072753906
Step 1898 | learning_rate: 3.90815671162492e-05
Step 1898 | epoch: 1.8285163776493256
Step 1899 | loss: 3.5198357105255127
Step 1899 | grad_norm: 3.16487979888916
Step 1899 | learning_rate: 3.904945407835581e-05
Step 1899 | epoch: 1.8294797687861273
Step 1900 | loss: 2.686230182647705
Step 1900 | grad_norm: 3.0448970794677734
Step 1900 | learning_rate: 3.901734104046243e-05
Step 1900 | epoch: 1.8304431599229287
Step 1901 | loss: 3.726334571838379
Step 1901 | grad_norm: 3.0534908771514893
Step 1901 | learning_rate: 3.898522800256905e-05
Step 1901 | epoch: 1.8314065510597302
Step 1902 | loss: 3.603119134902954
Step 1902 | grad_norm: 3.3022680282592773
Step 1902 | learning_rate: 3.8953114964675656e-05
Step 1902 | epoch: 1.8323699421965318
Step 1903 | loss: 2.824462890625
Step 1903 | grad_norm: 3.495849609375
Step 1903 | learning_rate: 3.8921001926782277e-05
Step 1903 | epoch: 1.8333333333333335
Step 1904 | loss: 2.9467151165008545
Step 1904 | grad_norm: 2.717419147491455
Step 1904 | learning_rate: 3.888888888888889e-05
Step 1904 | epoch: 1.834296724470135
Step 1905 | loss: 3.22541880607605
Step 1905 | grad_norm: 2.8495125770568848
Step 1905 | learning_rate: 3.8856775850995505e-05
Step 1905 | epoch: 1.8352601156069364
Step 1906 | loss: 4.028773307800293
Step 1906 | grad_norm: 3.4702792167663574
Step 1906 | learning_rate: 3.882466281310212e-05
Step 1906 | epoch: 1.836223506743738
Step 1907 | loss: 2.94953989982605
Step 1907 | grad_norm: 3.261476755142212
Step 1907 | learning_rate: 3.879254977520873e-05
Step 1907 | epoch: 1.8371868978805395
Step 1908 | loss: 3.2513368129730225
Step 1908 | grad_norm: 3.2784583568573
Step 1908 | learning_rate: 3.8760436737315354e-05
Step 1908 | epoch: 1.838150289017341
Step 1909 | loss: 3.6476283073425293
Step 1909 | grad_norm: 3.3987669944763184
Step 1909 | learning_rate: 3.872832369942196e-05
Step 1909 | epoch: 1.8391136801541426
Step 1910 | loss: 3.177443027496338
Step 1910 | grad_norm: 2.521986484527588
Step 1910 | learning_rate: 3.869621066152858e-05
Step 1910 | epoch: 1.8400770712909442
Step 1911 | loss: 3.219162702560425
Step 1911 | grad_norm: 2.600156307220459
Step 1911 | learning_rate: 3.86640976236352e-05
Step 1911 | epoch: 1.8410404624277457
Step 1912 | loss: 2.8758065700531006
Step 1912 | grad_norm: 2.861685276031494
Step 1912 | learning_rate: 3.863198458574182e-05
Step 1912 | epoch: 1.842003853564547
Step 1913 | loss: 2.94811749458313
Step 1913 | grad_norm: 3.0028398036956787
Step 1913 | learning_rate: 3.8599871547848425e-05
Step 1913 | epoch: 1.8429672447013488
Step 1914 | loss: 4.046254634857178
Step 1914 | grad_norm: 3.148184299468994
Step 1914 | learning_rate: 3.8567758509955046e-05
Step 1914 | epoch: 1.8439306358381504
Step 1915 | loss: 2.8404619693756104
Step 1915 | grad_norm: 2.5258116722106934
Step 1915 | learning_rate: 3.853564547206166e-05
Step 1915 | epoch: 1.8448940269749519
Step 1916 | loss: 3.1391074657440186
Step 1916 | grad_norm: 2.9668807983398438
Step 1916 | learning_rate: 3.8503532434168275e-05
Step 1916 | epoch: 1.8458574181117533
Step 1917 | loss: 2.7049121856689453
Step 1917 | grad_norm: 3.809962511062622
Step 1917 | learning_rate: 3.847141939627489e-05
Step 1917 | epoch: 1.846820809248555
Step 1918 | loss: 3.168287992477417
Step 1918 | grad_norm: 3.380225658416748
Step 1918 | learning_rate: 3.84393063583815e-05
Step 1918 | epoch: 1.8477842003853564
Step 1919 | loss: 2.7778127193450928
Step 1919 | grad_norm: 2.9526784420013428
Step 1919 | learning_rate: 3.8407193320488124e-05
Step 1919 | epoch: 1.8487475915221578
Step 1920 | loss: 2.817286729812622
Step 1920 | grad_norm: 2.1251096725463867
Step 1920 | learning_rate: 3.837508028259473e-05
Step 1920 | epoch: 1.8497109826589595
Step 1921 | loss: 3.0573508739471436
Step 1921 | grad_norm: 2.6417348384857178
Step 1921 | learning_rate: 3.834296724470135e-05
Step 1921 | epoch: 1.8506743737957612
Step 1922 | loss: 3.7834694385528564
Step 1922 | grad_norm: 2.92962908744812
Step 1922 | learning_rate: 3.8310854206807966e-05
Step 1922 | epoch: 1.8516377649325626
Step 1923 | loss: 3.1188406944274902
Step 1923 | grad_norm: 3.0135295391082764
Step 1923 | learning_rate: 3.827874116891458e-05
Step 1923 | epoch: 1.852601156069364
Step 1924 | loss: 3.0620250701904297
Step 1924 | grad_norm: 2.6527328491210938
Step 1924 | learning_rate: 3.8246628131021195e-05
Step 1924 | epoch: 1.8535645472061657
Step 1925 | loss: 4.231483459472656
Step 1925 | grad_norm: 4.338186740875244
Step 1925 | learning_rate: 3.821451509312781e-05
Step 1925 | epoch: 1.8545279383429674
Step 1926 | loss: 2.9399423599243164
Step 1926 | grad_norm: 2.730689764022827
Step 1926 | learning_rate: 3.818240205523443e-05
Step 1926 | epoch: 1.8554913294797688
Step 1927 | loss: 2.904019832611084
Step 1927 | grad_norm: 2.7778866291046143
Step 1927 | learning_rate: 3.815028901734104e-05
Step 1927 | epoch: 1.8564547206165702
Step 1928 | loss: 2.304107666015625
Step 1928 | grad_norm: 2.8954217433929443
Step 1928 | learning_rate: 3.811817597944766e-05
Step 1928 | epoch: 1.857418111753372
Step 1929 | loss: 2.742793083190918
Step 1929 | grad_norm: 2.424565076828003
Step 1929 | learning_rate: 3.808606294155427e-05
Step 1929 | epoch: 1.8583815028901736
Step 1930 | loss: 3.2565066814422607
Step 1930 | grad_norm: 2.4874112606048584
Step 1930 | learning_rate: 3.8053949903660894e-05
Step 1930 | epoch: 1.859344894026975
Step 1931 | loss: 2.781466484069824
Step 1931 | grad_norm: 2.5037357807159424
Step 1931 | learning_rate: 3.80218368657675e-05
Step 1931 | epoch: 1.8603082851637764
Step 1932 | loss: 3.676596164703369
Step 1932 | grad_norm: 3.38425874710083
Step 1932 | learning_rate: 3.798972382787412e-05
Step 1932 | epoch: 1.861271676300578
Step 1933 | loss: 3.441664934158325
Step 1933 | grad_norm: 4.101507186889648
Step 1933 | learning_rate: 3.7957610789980736e-05
Step 1933 | epoch: 1.8622350674373795
Step 1934 | loss: 2.6413989067077637
Step 1934 | grad_norm: 2.289057731628418
Step 1934 | learning_rate: 3.7925497752087343e-05
Step 1934 | epoch: 1.863198458574181
Step 1935 | loss: 2.988478899002075
Step 1935 | grad_norm: 2.5942649841308594
Step 1935 | learning_rate: 3.7893384714193964e-05
Step 1935 | epoch: 1.8641618497109826
Step 1936 | loss: 3.0186288356781006
Step 1936 | grad_norm: 2.757119655609131
Step 1936 | learning_rate: 3.786127167630058e-05
Step 1936 | epoch: 1.8651252408477843
Step 1937 | loss: 2.9445364475250244
Step 1937 | grad_norm: 2.686758041381836
Step 1937 | learning_rate: 3.78291586384072e-05
Step 1937 | epoch: 1.8660886319845857
Step 1938 | loss: 3.449599504470825
Step 1938 | grad_norm: 2.9325029850006104
Step 1938 | learning_rate: 3.779704560051381e-05
Step 1938 | epoch: 1.8670520231213872
Step 1939 | loss: 3.6719372272491455
Step 1939 | grad_norm: 2.7969443798065186
Step 1939 | learning_rate: 3.776493256262043e-05
Step 1939 | epoch: 1.8680154142581888
Step 1940 | loss: 3.3106110095977783
Step 1940 | grad_norm: 3.239086866378784
Step 1940 | learning_rate: 3.773281952472704e-05
Step 1940 | epoch: 1.8689788053949905
Step 1941 | loss: 3.51483154296875
Step 1941 | grad_norm: 3.671678304672241
Step 1941 | learning_rate: 3.7700706486833656e-05
Step 1941 | epoch: 1.869942196531792
Step 1942 | loss: 3.154142141342163
Step 1942 | grad_norm: 2.811640501022339
Step 1942 | learning_rate: 3.766859344894027e-05
Step 1942 | epoch: 1.8709055876685934
Step 1943 | loss: 3.05837345123291
Step 1943 | grad_norm: 3.1371777057647705
Step 1943 | learning_rate: 3.7636480411046885e-05
Step 1943 | epoch: 1.871868978805395
Step 1944 | loss: 4.084536075592041
Step 1944 | grad_norm: 3.826615810394287
Step 1944 | learning_rate: 3.7604367373153506e-05
Step 1944 | epoch: 1.8728323699421965
Step 1945 | loss: 4.070153713226318
Step 1945 | grad_norm: 3.47393798828125
Step 1945 | learning_rate: 3.757225433526011e-05
Step 1945 | epoch: 1.873795761078998
Step 1946 | loss: 2.975755214691162
Step 1946 | grad_norm: 2.838792562484741
Step 1946 | learning_rate: 3.7540141297366734e-05
Step 1946 | epoch: 1.8747591522157996
Step 1947 | loss: 3.373173952102661
Step 1947 | grad_norm: 2.813349962234497
Step 1947 | learning_rate: 3.750802825947335e-05
Step 1947 | epoch: 1.8757225433526012
Step 1948 | loss: 2.81118106842041
Step 1948 | grad_norm: 2.571484088897705
Step 1948 | learning_rate: 3.747591522157996e-05
Step 1948 | epoch: 1.8766859344894027
Step 1949 | loss: 2.51279354095459
Step 1949 | grad_norm: 3.19187068939209
Step 1949 | learning_rate: 3.744380218368658e-05
Step 1949 | epoch: 1.8776493256262041
Step 1950 | loss: 3.406960964202881
Step 1950 | grad_norm: 2.6485214233398438
Step 1950 | learning_rate: 3.741168914579319e-05
Step 1950 | epoch: 1.8786127167630058
Step 1951 | loss: 3.63840651512146
Step 1951 | grad_norm: 2.662135124206543
Step 1951 | learning_rate: 3.737957610789981e-05
Step 1951 | epoch: 1.8795761078998074
Step 1952 | loss: 2.952579975128174
Step 1952 | grad_norm: 2.904118776321411
Step 1952 | learning_rate: 3.734746307000642e-05
Step 1952 | epoch: 1.8805394990366089
Step 1953 | loss: 3.296924352645874
Step 1953 | grad_norm: 2.606745719909668
Step 1953 | learning_rate: 3.731535003211304e-05
Step 1953 | epoch: 1.8815028901734103
Step 1954 | loss: 3.862304449081421
Step 1954 | grad_norm: 2.6709694862365723
Step 1954 | learning_rate: 3.7283236994219654e-05
Step 1954 | epoch: 1.882466281310212
Step 1955 | loss: 2.753058910369873
Step 1955 | grad_norm: 2.627354621887207
Step 1955 | learning_rate: 3.7251123956326275e-05
Step 1955 | epoch: 1.8834296724470136
Step 1956 | loss: 2.9741363525390625
Step 1956 | grad_norm: 3.0440218448638916
Step 1956 | learning_rate: 3.721901091843288e-05
Step 1956 | epoch: 1.8843930635838149
Step 1957 | loss: 3.067199468612671
Step 1957 | grad_norm: 3.352419376373291
Step 1957 | learning_rate: 3.7186897880539504e-05
Step 1957 | epoch: 1.8853564547206165
Step 1958 | loss: 2.7388510704040527
Step 1958 | grad_norm: 3.0077648162841797
Step 1958 | learning_rate: 3.715478484264612e-05
Step 1958 | epoch: 1.8863198458574182
Step 1959 | loss: 3.4545340538024902
Step 1959 | grad_norm: 3.570866107940674
Step 1959 | learning_rate: 3.712267180475273e-05
Step 1959 | epoch: 1.8872832369942196
Step 1960 | loss: 3.5753936767578125
Step 1960 | grad_norm: 3.1708216667175293
Step 1960 | learning_rate: 3.7090558766859346e-05
Step 1960 | epoch: 1.888246628131021
Step 1961 | loss: 3.596170425415039
Step 1961 | grad_norm: 3.10245418548584
Step 1961 | learning_rate: 3.705844572896596e-05
Step 1961 | epoch: 1.8892100192678227
Step 1962 | loss: 2.880932569503784
Step 1962 | grad_norm: 4.457372188568115
Step 1962 | learning_rate: 3.702633269107258e-05
Step 1962 | epoch: 1.8901734104046244
Step 1963 | loss: 2.9285857677459717
Step 1963 | grad_norm: 3.122821569442749
Step 1963 | learning_rate: 3.699421965317919e-05
Step 1963 | epoch: 1.8911368015414258
Step 1964 | loss: 3.3857173919677734
Step 1964 | grad_norm: 4.714576721191406
Step 1964 | learning_rate: 3.696210661528581e-05
Step 1964 | epoch: 1.8921001926782273
Step 1965 | loss: 2.7728939056396484
Step 1965 | grad_norm: 2.388827085494995
Step 1965 | learning_rate: 3.6929993577392424e-05
Step 1965 | epoch: 1.893063583815029
Step 1966 | loss: 3.148043394088745
Step 1966 | grad_norm: 3.038789749145508
Step 1966 | learning_rate: 3.689788053949904e-05
Step 1966 | epoch: 1.8940269749518306
Step 1967 | loss: 3.3287081718444824
Step 1967 | grad_norm: 2.913684606552124
Step 1967 | learning_rate: 3.686576750160565e-05
Step 1967 | epoch: 1.894990366088632
Step 1968 | loss: 3.264822006225586
Step 1968 | grad_norm: 3.2559659481048584
Step 1968 | learning_rate: 3.6833654463712267e-05
Step 1968 | epoch: 1.8959537572254335
Step 1969 | loss: 2.8900840282440186
Step 1969 | grad_norm: 2.435833215713501
Step 1969 | learning_rate: 3.680154142581889e-05
Step 1969 | epoch: 1.8969171483622351
Step 1970 | loss: 2.8760383129119873
Step 1970 | grad_norm: 2.5824854373931885
Step 1970 | learning_rate: 3.6769428387925495e-05
Step 1970 | epoch: 1.8978805394990366
Step 1971 | loss: 2.6725614070892334
Step 1971 | grad_norm: 2.7338008880615234
Step 1971 | learning_rate: 3.6737315350032116e-05
Step 1971 | epoch: 1.898843930635838
Step 1972 | loss: 3.5398833751678467
Step 1972 | grad_norm: 2.4848859310150146
Step 1972 | learning_rate: 3.670520231213873e-05
Step 1972 | epoch: 1.8998073217726397
Step 1973 | loss: 2.8705077171325684
Step 1973 | grad_norm: 2.413749933242798
Step 1973 | learning_rate: 3.667308927424535e-05
Step 1973 | epoch: 1.9007707129094413
Step 1974 | loss: 3.0237603187561035
Step 1974 | grad_norm: 2.9286506175994873
Step 1974 | learning_rate: 3.664097623635196e-05
Step 1974 | epoch: 1.9017341040462428
Step 1975 | loss: 2.8645050525665283
Step 1975 | grad_norm: 3.0315728187561035
Step 1975 | learning_rate: 3.660886319845858e-05
Step 1975 | epoch: 1.9026974951830442
Step 1976 | loss: 2.5472211837768555
Step 1976 | grad_norm: 2.3736374378204346
Step 1976 | learning_rate: 3.6576750160565194e-05
Step 1976 | epoch: 1.9036608863198459
Step 1977 | loss: 2.9612982273101807
Step 1977 | grad_norm: 2.7148749828338623
Step 1977 | learning_rate: 3.65446371226718e-05
Step 1977 | epoch: 1.9046242774566475
Step 1978 | loss: 3.662421464920044
Step 1978 | grad_norm: 2.751990795135498
Step 1978 | learning_rate: 3.651252408477842e-05
Step 1978 | epoch: 1.905587668593449
Step 1979 | loss: 2.558520555496216
Step 1979 | grad_norm: 2.3896851539611816
Step 1979 | learning_rate: 3.6480411046885036e-05
Step 1979 | epoch: 1.9065510597302504
Step 1980 | loss: 3.3827950954437256
Step 1980 | grad_norm: 3.065295934677124
Step 1980 | learning_rate: 3.644829800899166e-05
Step 1980 | epoch: 1.907514450867052
Step 1981 | loss: 2.749180555343628
Step 1981 | grad_norm: 3.1675822734832764
Step 1981 | learning_rate: 3.6416184971098265e-05
Step 1981 | epoch: 1.9084778420038537
Step 1982 | loss: 3.831068277359009
Step 1982 | grad_norm: 3.1425373554229736
Step 1982 | learning_rate: 3.6384071933204885e-05
Step 1982 | epoch: 1.909441233140655
Step 1983 | loss: 3.7164306640625
Step 1983 | grad_norm: 8.955008506774902
Step 1983 | learning_rate: 3.63519588953115e-05
Step 1983 | epoch: 1.9104046242774566
Step 1984 | loss: 3.260331153869629
Step 1984 | grad_norm: 2.762144088745117
Step 1984 | learning_rate: 3.6319845857418114e-05
Step 1984 | epoch: 1.9113680154142583
Step 1985 | loss: 2.985356569290161
Step 1985 | grad_norm: 6.631584167480469
Step 1985 | learning_rate: 3.628773281952473e-05
Step 1985 | epoch: 1.9123314065510597
Step 1986 | loss: 3.784832715988159
Step 1986 | grad_norm: 3.301851749420166
Step 1986 | learning_rate: 3.625561978163134e-05
Step 1986 | epoch: 1.9132947976878611
Step 1987 | loss: 3.640022039413452
Step 1987 | grad_norm: 2.4745254516601562
Step 1987 | learning_rate: 3.622350674373796e-05
Step 1987 | epoch: 1.9142581888246628
Step 1988 | loss: 3.505453109741211
Step 1988 | grad_norm: 3.052021026611328
Step 1988 | learning_rate: 3.619139370584457e-05
Step 1988 | epoch: 1.9152215799614645
Step 1989 | loss: 3.4162609577178955
Step 1989 | grad_norm: 3.0029799938201904
Step 1989 | learning_rate: 3.615928066795119e-05
Step 1989 | epoch: 1.916184971098266
Step 1990 | loss: 3.524169683456421
Step 1990 | grad_norm: 3.1380317211151123
Step 1990 | learning_rate: 3.6127167630057806e-05
Step 1990 | epoch: 1.9171483622350673
Step 1991 | loss: 3.23496150970459
Step 1991 | grad_norm: 3.0196685791015625
Step 1991 | learning_rate: 3.609505459216442e-05
Step 1991 | epoch: 1.918111753371869
Step 1992 | loss: 3.5338425636291504
Step 1992 | grad_norm: 3.222804069519043
Step 1992 | learning_rate: 3.6062941554271034e-05
Step 1992 | epoch: 1.9190751445086707
Step 1993 | loss: 3.430025100708008
Step 1993 | grad_norm: 2.622549533843994
Step 1993 | learning_rate: 3.603082851637765e-05
Step 1993 | epoch: 1.920038535645472
Step 1994 | loss: 3.1165313720703125
Step 1994 | grad_norm: 3.0710973739624023
Step 1994 | learning_rate: 3.599871547848427e-05
Step 1994 | epoch: 1.9210019267822736
Step 1995 | loss: 2.74090838432312
Step 1995 | grad_norm: 2.7413740158081055
Step 1995 | learning_rate: 3.596660244059088e-05
Step 1995 | epoch: 1.9219653179190752
Step 1996 | loss: 3.300034761428833
Step 1996 | grad_norm: 2.5458333492279053
Step 1996 | learning_rate: 3.59344894026975e-05
Step 1996 | epoch: 1.9229287090558767
Step 1997 | loss: 3.7614214420318604
Step 1997 | grad_norm: 3.362950086593628
Step 1997 | learning_rate: 3.590237636480411e-05
Step 1997 | epoch: 1.923892100192678
Step 1998 | loss: 3.2756619453430176
Step 1998 | grad_norm: 2.9068474769592285
Step 1998 | learning_rate: 3.587026332691073e-05
Step 1998 | epoch: 1.9248554913294798
Step 1999 | loss: 3.1772420406341553
Step 1999 | grad_norm: 2.540353536605835
Step 1999 | learning_rate: 3.583815028901734e-05
Step 1999 | epoch: 1.9258188824662814
Step 2000 | loss: 3.5345020294189453
Step 2000 | grad_norm: 2.7274694442749023
Step 2000 | learning_rate: 3.580603725112396e-05
Step 2000 | epoch: 1.9267822736030829
Step 2001 | loss: 3.5991039276123047
Step 2001 | grad_norm: 2.9223058223724365
Step 2001 | learning_rate: 3.5773924213230575e-05
Step 2001 | epoch: 1.9277456647398843
Step 2002 | loss: 2.9571056365966797
Step 2002 | grad_norm: 2.390119791030884
Step 2002 | learning_rate: 3.574181117533719e-05
Step 2002 | epoch: 1.928709055876686
Step 2003 | loss: 3.231874942779541
Step 2003 | grad_norm: 3.0555331707000732
Step 2003 | learning_rate: 3.5709698137443804e-05
Step 2003 | epoch: 1.9296724470134876
Step 2004 | loss: 4.1504130363464355
Step 2004 | grad_norm: 3.1409080028533936
Step 2004 | learning_rate: 3.567758509955042e-05
Step 2004 | epoch: 1.930635838150289
Step 2005 | loss: 3.3013405799865723
Step 2005 | grad_norm: 2.9858627319335938
Step 2005 | learning_rate: 3.564547206165704e-05
Step 2005 | epoch: 1.9315992292870905
Step 2006 | loss: 2.8697986602783203
Step 2006 | grad_norm: 2.7975122928619385
Step 2006 | learning_rate: 3.5613359023763646e-05
Step 2006 | epoch: 1.9325626204238922
Step 2007 | loss: 4.161795139312744
Step 2007 | grad_norm: 2.6249771118164062
Step 2007 | learning_rate: 3.558124598587027e-05
Step 2007 | epoch: 1.9335260115606936
Step 2008 | loss: 3.0293054580688477
Step 2008 | grad_norm: 2.347900152206421
Step 2008 | learning_rate: 3.554913294797688e-05
Step 2008 | epoch: 1.934489402697495
Step 2009 | loss: 2.678471803665161
Step 2009 | grad_norm: 2.9937050342559814
Step 2009 | learning_rate: 3.5517019910083496e-05
Step 2009 | epoch: 1.9354527938342967
Step 2010 | loss: 2.7787365913391113
Step 2010 | grad_norm: 2.60783052444458
Step 2010 | learning_rate: 3.548490687219011e-05
Step 2010 | epoch: 1.9364161849710984
Step 2011 | loss: 3.130688428878784
Step 2011 | grad_norm: 2.4763686656951904
Step 2011 | learning_rate: 3.5452793834296724e-05
Step 2011 | epoch: 1.9373795761078998
Step 2012 | loss: 3.002687454223633
Step 2012 | grad_norm: 2.9360063076019287
Step 2012 | learning_rate: 3.5420680796403345e-05
Step 2012 | epoch: 1.9383429672447012
Step 2013 | loss: 3.5567264556884766
Step 2013 | grad_norm: 2.5056443214416504
Step 2013 | learning_rate: 3.538856775850995e-05
Step 2013 | epoch: 1.939306358381503
Step 2014 | loss: 3.5999839305877686
Step 2014 | grad_norm: 3.3498237133026123
Step 2014 | learning_rate: 3.535645472061657e-05
Step 2014 | epoch: 1.9402697495183046
Step 2015 | loss: 3.6161696910858154
Step 2015 | grad_norm: 2.9382100105285645
Step 2015 | learning_rate: 3.532434168272319e-05
Step 2015 | epoch: 1.941233140655106
Step 2016 | loss: 3.2081458568573
Step 2016 | grad_norm: 2.393686532974243
Step 2016 | learning_rate: 3.52922286448298e-05
Step 2016 | epoch: 1.9421965317919074
Step 2017 | loss: 2.9252490997314453
Step 2017 | grad_norm: 3.198106288909912
Step 2017 | learning_rate: 3.5260115606936416e-05
Step 2017 | epoch: 1.943159922928709
Step 2018 | loss: 2.9357476234436035
Step 2018 | grad_norm: 2.500731945037842
Step 2018 | learning_rate: 3.522800256904304e-05
Step 2018 | epoch: 1.9441233140655108
Step 2019 | loss: 2.6602728366851807
Step 2019 | grad_norm: 2.597115993499756
Step 2019 | learning_rate: 3.519588953114965e-05
Step 2019 | epoch: 1.9450867052023122
Step 2020 | loss: 3.533090829849243
Step 2020 | grad_norm: 3.4233508110046387
Step 2020 | learning_rate: 3.5163776493256265e-05
Step 2020 | epoch: 1.9460500963391136
Step 2021 | loss: 2.9740397930145264
Step 2021 | grad_norm: 2.616987705230713
Step 2021 | learning_rate: 3.513166345536288e-05
Step 2021 | epoch: 1.9470134874759153
Step 2022 | loss: 2.9834399223327637
Step 2022 | grad_norm: 2.4862265586853027
Step 2022 | learning_rate: 3.5099550417469494e-05
Step 2022 | epoch: 1.9479768786127167
Step 2023 | loss: 2.923135757446289
Step 2023 | grad_norm: 2.9063310623168945
Step 2023 | learning_rate: 3.506743737957611e-05
Step 2023 | epoch: 1.9489402697495182
Step 2024 | loss: 3.219301700592041
Step 2024 | grad_norm: 3.3105337619781494
Step 2024 | learning_rate: 3.503532434168272e-05
Step 2024 | epoch: 1.9499036608863198
Step 2025 | loss: 2.9224753379821777
Step 2025 | grad_norm: 2.389437437057495
Step 2025 | learning_rate: 3.500321130378934e-05
Step 2025 | epoch: 1.9508670520231215
Step 2026 | loss: 4.074196815490723
Step 2026 | grad_norm: 3.8284595012664795
Step 2026 | learning_rate: 3.497109826589596e-05
Step 2026 | epoch: 1.951830443159923
Step 2027 | loss: 2.711226224899292
Step 2027 | grad_norm: 2.564124822616577
Step 2027 | learning_rate: 3.493898522800257e-05
Step 2027 | epoch: 1.9527938342967244
Step 2028 | loss: 3.6490414142608643
Step 2028 | grad_norm: 2.8395464420318604
Step 2028 | learning_rate: 3.4906872190109186e-05
Step 2028 | epoch: 1.953757225433526
Step 2029 | loss: 3.167510986328125
Step 2029 | grad_norm: 2.521184206008911
Step 2029 | learning_rate: 3.48747591522158e-05
Step 2029 | epoch: 1.9547206165703277
Step 2030 | loss: 3.2333030700683594
Step 2030 | grad_norm: 4.646958351135254
Step 2030 | learning_rate: 3.484264611432242e-05
Step 2030 | epoch: 1.9556840077071291
Step 2031 | loss: 4.010388374328613
Step 2031 | grad_norm: 4.220561504364014
Step 2031 | learning_rate: 3.481053307642903e-05
Step 2031 | epoch: 1.9566473988439306
Step 2032 | loss: 4.012979507446289
Step 2032 | grad_norm: 3.4098193645477295
Step 2032 | learning_rate: 3.477842003853565e-05
Step 2032 | epoch: 1.9576107899807322
Step 2033 | loss: 4.111667633056641
Step 2033 | grad_norm: 4.585643768310547
Step 2033 | learning_rate: 3.474630700064226e-05
Step 2033 | epoch: 1.9585741811175337
Step 2034 | loss: 4.084333896636963
Step 2034 | grad_norm: 3.4049458503723145
Step 2034 | learning_rate: 3.471419396274888e-05
Step 2034 | epoch: 1.9595375722543351
Step 2035 | loss: 3.4501216411590576
Step 2035 | grad_norm: 3.0878851413726807
Step 2035 | learning_rate: 3.468208092485549e-05
Step 2035 | epoch: 1.9605009633911368
Step 2036 | loss: 2.5078189373016357
Step 2036 | grad_norm: 3.080960988998413
Step 2036 | learning_rate: 3.4649967886962106e-05
Step 2036 | epoch: 1.9614643545279384
Step 2037 | loss: 2.817795991897583
Step 2037 | grad_norm: 2.4964492321014404
Step 2037 | learning_rate: 3.461785484906873e-05
Step 2037 | epoch: 1.9624277456647399
Step 2038 | loss: 3.1908774375915527
Step 2038 | grad_norm: 2.8556437492370605
Step 2038 | learning_rate: 3.4585741811175334e-05
Step 2038 | epoch: 1.9633911368015413
Step 2039 | loss: 2.9240171909332275
Step 2039 | grad_norm: 2.8896195888519287
Step 2039 | learning_rate: 3.4553628773281955e-05
Step 2039 | epoch: 1.964354527938343
Step 2040 | loss: 3.802337884902954
Step 2040 | grad_norm: 3.4448554515838623
Step 2040 | learning_rate: 3.452151573538857e-05
Step 2040 | epoch: 1.9653179190751446
Step 2041 | loss: 2.5814828872680664
Step 2041 | grad_norm: 2.220496416091919
Step 2041 | learning_rate: 3.4489402697495184e-05
Step 2041 | epoch: 1.966281310211946
Step 2042 | loss: 3.220828056335449
Step 2042 | grad_norm: 2.5500802993774414
Step 2042 | learning_rate: 3.44572896596018e-05
Step 2042 | epoch: 1.9672447013487475
Step 2043 | loss: 3.2440683841705322
Step 2043 | grad_norm: 2.4651687145233154
Step 2043 | learning_rate: 3.442517662170842e-05
Step 2043 | epoch: 1.9682080924855492
Step 2044 | loss: 4.075510501861572
Step 2044 | grad_norm: 3.3577423095703125
Step 2044 | learning_rate: 3.439306358381503e-05
Step 2044 | epoch: 1.9691714836223508
Step 2045 | loss: 3.66904878616333
Step 2045 | grad_norm: 3.5792148113250732
Step 2045 | learning_rate: 3.436095054592165e-05
Step 2045 | epoch: 1.970134874759152
Step 2046 | loss: 2.37618088722229
Step 2046 | grad_norm: 2.914562463760376
Step 2046 | learning_rate: 3.432883750802826e-05
Step 2046 | epoch: 1.9710982658959537
Step 2047 | loss: 2.8925232887268066
Step 2047 | grad_norm: 3.625559091567993
Step 2047 | learning_rate: 3.4296724470134875e-05
Step 2047 | epoch: 1.9720616570327554
Step 2048 | loss: 3.867375612258911
Step 2048 | grad_norm: 2.830669641494751
Step 2048 | learning_rate: 3.426461143224149e-05
Step 2048 | epoch: 1.9730250481695568
Step 2049 | loss: 3.3580756187438965
Step 2049 | grad_norm: 2.6647520065307617
Step 2049 | learning_rate: 3.4232498394348104e-05
Step 2049 | epoch: 1.9739884393063583
Step 2050 | loss: 3.751150369644165
Step 2050 | grad_norm: 2.91279673576355
Step 2050 | learning_rate: 3.4200385356454725e-05
Step 2050 | epoch: 1.97495183044316
Step 2051 | loss: 3.1196377277374268
Step 2051 | grad_norm: 3.1015493869781494
Step 2051 | learning_rate: 3.416827231856134e-05
Step 2051 | epoch: 1.9759152215799616
Step 2052 | loss: 3.4383461475372314
Step 2052 | grad_norm: 3.477409839630127
Step 2052 | learning_rate: 3.413615928066795e-05
Step 2052 | epoch: 1.976878612716763
Step 2053 | loss: 4.149459362030029
Step 2053 | grad_norm: 2.930704116821289
Step 2053 | learning_rate: 3.410404624277457e-05
Step 2053 | epoch: 1.9778420038535645
Step 2054 | loss: 3.587357997894287
Step 2054 | grad_norm: 4.037230014801025
Step 2054 | learning_rate: 3.407193320488118e-05
Step 2054 | epoch: 1.9788053949903661
Step 2055 | loss: 3.04872727394104
Step 2055 | grad_norm: 2.936171770095825
Step 2055 | learning_rate: 3.40398201669878e-05
Step 2055 | epoch: 1.9797687861271678
Step 2056 | loss: 2.9221417903900146
Step 2056 | grad_norm: 2.3362324237823486
Step 2056 | learning_rate: 3.400770712909441e-05
Step 2056 | epoch: 1.9807321772639692
Step 2057 | loss: 3.2170281410217285
Step 2057 | grad_norm: 3.1240687370300293
Step 2057 | learning_rate: 3.397559409120103e-05
Step 2057 | epoch: 1.9816955684007707
Step 2058 | loss: 2.611339807510376
Step 2058 | grad_norm: 2.075147867202759
Step 2058 | learning_rate: 3.3943481053307645e-05
Step 2058 | epoch: 1.9826589595375723
Step 2059 | loss: 3.3745808601379395
Step 2059 | grad_norm: 2.662841796875
Step 2059 | learning_rate: 3.391136801541426e-05
Step 2059 | epoch: 1.9836223506743738
Step 2060 | loss: 3.816927433013916
Step 2060 | grad_norm: 3.2602620124816895
Step 2060 | learning_rate: 3.3879254977520873e-05
Step 2060 | epoch: 1.9845857418111752
Step 2061 | loss: 3.1023457050323486
Step 2061 | grad_norm: 2.7498879432678223
Step 2061 | learning_rate: 3.3847141939627494e-05
Step 2061 | epoch: 1.9855491329479769
Step 2062 | loss: 3.1982719898223877
Step 2062 | grad_norm: 2.940476179122925
Step 2062 | learning_rate: 3.381502890173411e-05
Step 2062 | epoch: 1.9865125240847785
Step 2063 | loss: 3.2056171894073486
Step 2063 | grad_norm: 3.5272786617279053
Step 2063 | learning_rate: 3.378291586384072e-05
Step 2063 | epoch: 1.98747591522158
Step 2064 | loss: 3.2113037109375
Step 2064 | grad_norm: 2.865201234817505
Step 2064 | learning_rate: 3.375080282594734e-05
Step 2064 | epoch: 1.9884393063583814
Step 2065 | loss: 2.5628890991210938
Step 2065 | grad_norm: 2.2214324474334717
Step 2065 | learning_rate: 3.371868978805395e-05
Step 2065 | epoch: 1.989402697495183
Step 2066 | loss: 3.08905029296875
Step 2066 | grad_norm: 4.812770366668701
Step 2066 | learning_rate: 3.3686576750160565e-05
Step 2066 | epoch: 1.9903660886319847
Step 2067 | loss: 2.537853479385376
Step 2067 | grad_norm: 2.599127769470215
Step 2067 | learning_rate: 3.365446371226718e-05
Step 2067 | epoch: 1.9913294797687862
Step 2068 | loss: 3.1742374897003174
Step 2068 | grad_norm: 3.303208589553833
Step 2068 | learning_rate: 3.36223506743738e-05
Step 2068 | epoch: 1.9922928709055876
Step 2069 | loss: 2.9644951820373535
Step 2069 | grad_norm: 2.7019765377044678
Step 2069 | learning_rate: 3.3590237636480415e-05
Step 2069 | epoch: 1.9932562620423893
Step 2070 | loss: 3.128269910812378
Step 2070 | grad_norm: 3.3806118965148926
Step 2070 | learning_rate: 3.355812459858703e-05
Step 2070 | epoch: 1.9942196531791907
Step 2071 | loss: 3.1400046348571777
Step 2071 | grad_norm: 3.2239038944244385
Step 2071 | learning_rate: 3.352601156069364e-05
Step 2071 | epoch: 1.9951830443159921
Step 2072 | loss: 3.4920601844787598
Step 2072 | grad_norm: 3.3856029510498047
Step 2072 | learning_rate: 3.349389852280026e-05
Step 2072 | epoch: 1.9961464354527938
Step 2073 | loss: 3.8307507038116455
Step 2073 | grad_norm: 3.512256145477295
Step 2073 | learning_rate: 3.346178548490687e-05
Step 2073 | epoch: 1.9971098265895955
Step 2074 | loss: 2.9179961681365967
Step 2074 | grad_norm: 2.6495556831359863
Step 2074 | learning_rate: 3.3429672447013486e-05
Step 2074 | epoch: 1.998073217726397
Step 2075 | loss: 3.573551654815674
Step 2075 | grad_norm: 2.897761344909668
Step 2075 | learning_rate: 3.3397559409120107e-05
Step 2075 | epoch: 1.9990366088631983
Step 2076 | loss: 4.077561378479004
Step 2076 | grad_norm: 4.604119777679443
Step 2076 | learning_rate: 3.336544637122672e-05
Step 2076 | epoch: 2.0
Step 2077 | loss: 3.391728639602661
Step 2077 | grad_norm: 3.053678512573242
Step 2077 | learning_rate: 3.3333333333333335e-05
Step 2077 | epoch: 2.0009633911368017
Step 2078 | loss: 3.7313835620880127
Step 2078 | grad_norm: 2.5404040813446045
Step 2078 | learning_rate: 3.330122029543995e-05
Step 2078 | epoch: 2.001926782273603
Step 2079 | loss: 3.5024054050445557
Step 2079 | grad_norm: 3.0875611305236816
Step 2079 | learning_rate: 3.326910725754657e-05
Step 2079 | epoch: 2.0028901734104045
Step 2080 | loss: 2.953740358352661
Step 2080 | grad_norm: 2.588135004043579
Step 2080 | learning_rate: 3.323699421965318e-05
Step 2080 | epoch: 2.003853564547206
Step 2081 | loss: 3.9721713066101074
Step 2081 | grad_norm: 3.8018953800201416
Step 2081 | learning_rate: 3.320488118175979e-05
Step 2081 | epoch: 2.004816955684008
Step 2082 | loss: 2.851280450820923
Step 2082 | grad_norm: 2.490729570388794
Step 2082 | learning_rate: 3.317276814386641e-05
Step 2082 | epoch: 2.005780346820809
Step 2083 | loss: 3.0187950134277344
Step 2083 | grad_norm: 3.1668894290924072
Step 2083 | learning_rate: 3.314065510597303e-05
Step 2083 | epoch: 2.0067437379576107
Step 2084 | loss: 3.1401870250701904
Step 2084 | grad_norm: 3.0236871242523193
Step 2084 | learning_rate: 3.310854206807964e-05
Step 2084 | epoch: 2.0077071290944124
Step 2085 | loss: 2.798081874847412
Step 2085 | grad_norm: 2.545262098312378
Step 2085 | learning_rate: 3.3076429030186255e-05
Step 2085 | epoch: 2.008670520231214
Step 2086 | loss: 3.4497411251068115
Step 2086 | grad_norm: 3.069931745529175
Step 2086 | learning_rate: 3.3044315992292876e-05
Step 2086 | epoch: 2.0096339113680153
Step 2087 | loss: 2.7150397300720215
Step 2087 | grad_norm: 2.774909257888794
Step 2087 | learning_rate: 3.301220295439949e-05
Step 2087 | epoch: 2.010597302504817
Step 2088 | loss: 2.7458319664001465
Step 2088 | grad_norm: 3.383049488067627
Step 2088 | learning_rate: 3.2980089916506105e-05
Step 2088 | epoch: 2.0115606936416186
Step 2089 | loss: 3.1523988246917725
Step 2089 | grad_norm: 3.18833065032959
Step 2089 | learning_rate: 3.294797687861272e-05
Step 2089 | epoch: 2.01252408477842
Step 2090 | loss: 2.9983954429626465
Step 2090 | grad_norm: 2.242385149002075
Step 2090 | learning_rate: 3.291586384071933e-05
Step 2090 | epoch: 2.0134874759152215
Step 2091 | loss: 3.638705253601074
Step 2091 | grad_norm: 2.817387104034424
Step 2091 | learning_rate: 3.288375080282595e-05
Step 2091 | epoch: 2.014450867052023
Step 2092 | loss: 3.038360118865967
Step 2092 | grad_norm: 3.4113268852233887
Step 2092 | learning_rate: 3.285163776493256e-05
Step 2092 | epoch: 2.015414258188825
Step 2093 | loss: 3.465158462524414
Step 2093 | grad_norm: 2.5856075286865234
Step 2093 | learning_rate: 3.281952472703918e-05
Step 2093 | epoch: 2.016377649325626
Step 2094 | loss: 3.9138824939727783
Step 2094 | grad_norm: 2.6301846504211426
Step 2094 | learning_rate: 3.2787411689145796e-05
Step 2094 | epoch: 2.0173410404624277
Step 2095 | loss: 3.1153512001037598
Step 2095 | grad_norm: 2.7112717628479004
Step 2095 | learning_rate: 3.275529865125241e-05
Step 2095 | epoch: 2.0183044315992293
Step 2096 | loss: 4.370720386505127
Step 2096 | grad_norm: 4.099565029144287
Step 2096 | learning_rate: 3.2723185613359025e-05
Step 2096 | epoch: 2.019267822736031
Step 2097 | loss: 2.7593271732330322
Step 2097 | grad_norm: 2.953556776046753
Step 2097 | learning_rate: 3.269107257546564e-05
Step 2097 | epoch: 2.020231213872832
Step 2098 | loss: 3.1841063499450684
Step 2098 | grad_norm: 3.132307529449463
Step 2098 | learning_rate: 3.265895953757225e-05
Step 2098 | epoch: 2.021194605009634
Step 2099 | loss: 3.085472345352173
Step 2099 | grad_norm: 2.702824592590332
Step 2099 | learning_rate: 3.262684649967887e-05
Step 2099 | epoch: 2.0221579961464355
Step 2100 | loss: 3.511815309524536
Step 2100 | grad_norm: 2.3218789100646973
Step 2100 | learning_rate: 3.259473346178549e-05
Step 2100 | epoch: 2.023121387283237
Step 2101 | loss: 3.6745758056640625
Step 2101 | grad_norm: 4.471165657043457
Step 2101 | learning_rate: 3.25626204238921e-05
Step 2101 | epoch: 2.0240847784200384
Step 2102 | loss: 3.7884345054626465
Step 2102 | grad_norm: 2.8342103958129883
Step 2102 | learning_rate: 3.253050738599872e-05
Step 2102 | epoch: 2.02504816955684
Step 2103 | loss: 3.5586299896240234
Step 2103 | grad_norm: 2.9593236446380615
Step 2103 | learning_rate: 3.249839434810533e-05
Step 2103 | epoch: 2.0260115606936417
Step 2104 | loss: 3.4243152141571045
Step 2104 | grad_norm: 2.747338056564331
Step 2104 | learning_rate: 3.246628131021195e-05
Step 2104 | epoch: 2.026974951830443
Step 2105 | loss: 3.6202869415283203
Step 2105 | grad_norm: 2.9238500595092773
Step 2105 | learning_rate: 3.243416827231856e-05
Step 2105 | epoch: 2.0279383429672446
Step 2106 | loss: 3.223977565765381
Step 2106 | grad_norm: 2.267042398452759
Step 2106 | learning_rate: 3.240205523442518e-05
Step 2106 | epoch: 2.0289017341040463
Step 2107 | loss: 3.046821117401123
Step 2107 | grad_norm: 2.647800922393799
Step 2107 | learning_rate: 3.2369942196531794e-05
Step 2107 | epoch: 2.029865125240848
Step 2108 | loss: 3.224863052368164
Step 2108 | grad_norm: 3.3248541355133057
Step 2108 | learning_rate: 3.233782915863841e-05
Step 2108 | epoch: 2.030828516377649
Step 2109 | loss: 2.754267692565918
Step 2109 | grad_norm: 2.700711727142334
Step 2109 | learning_rate: 3.230571612074502e-05
Step 2109 | epoch: 2.031791907514451
Step 2110 | loss: 3.0974297523498535
Step 2110 | grad_norm: 2.5889806747436523
Step 2110 | learning_rate: 3.227360308285164e-05
Step 2110 | epoch: 2.0327552986512525
Step 2111 | loss: 3.008983850479126
Step 2111 | grad_norm: 2.4627158641815186
Step 2111 | learning_rate: 3.224149004495826e-05
Step 2111 | epoch: 2.033718689788054
Step 2112 | loss: 3.1352856159210205
Step 2112 | grad_norm: 2.57419753074646
Step 2112 | learning_rate: 3.220937700706487e-05
Step 2112 | epoch: 2.0346820809248554
Step 2113 | loss: 2.9278409481048584
Step 2113 | grad_norm: 3.168792724609375
Step 2113 | learning_rate: 3.2177263969171486e-05
Step 2113 | epoch: 2.035645472061657
Step 2114 | loss: 3.5509679317474365
Step 2114 | grad_norm: 3.459000587463379
Step 2114 | learning_rate: 3.21451509312781e-05
Step 2114 | epoch: 2.0366088631984587
Step 2115 | loss: 4.214995861053467
Step 2115 | grad_norm: 3.186610460281372
Step 2115 | learning_rate: 3.2113037893384715e-05
Step 2115 | epoch: 2.03757225433526
Step 2116 | loss: 3.062899589538574
Step 2116 | grad_norm: 3.0074732303619385
Step 2116 | learning_rate: 3.208092485549133e-05
Step 2116 | epoch: 2.0385356454720616
Step 2117 | loss: 3.9157490730285645
Step 2117 | grad_norm: 2.7402853965759277
Step 2117 | learning_rate: 3.204881181759794e-05
Step 2117 | epoch: 2.0394990366088632
Step 2118 | loss: 3.752506732940674
Step 2118 | grad_norm: 3.0851290225982666
Step 2118 | learning_rate: 3.2016698779704564e-05
Step 2118 | epoch: 2.040462427745665
Step 2119 | loss: 3.601233720779419
Step 2119 | grad_norm: 2.663832902908325
Step 2119 | learning_rate: 3.198458574181118e-05
Step 2119 | epoch: 2.041425818882466
Step 2120 | loss: 4.292822360992432
Step 2120 | grad_norm: 3.290787935256958
Step 2120 | learning_rate: 3.195247270391779e-05
Step 2120 | epoch: 2.0423892100192678
Step 2121 | loss: 3.616691827774048
Step 2121 | grad_norm: 2.8421382904052734
Step 2121 | learning_rate: 3.192035966602441e-05
Step 2121 | epoch: 2.0433526011560694
Step 2122 | loss: 3.2848267555236816
Step 2122 | grad_norm: 2.352102756500244
Step 2122 | learning_rate: 3.188824662813103e-05
Step 2122 | epoch: 2.044315992292871
Step 2123 | loss: 3.0027599334716797
Step 2123 | grad_norm: 2.665419340133667
Step 2123 | learning_rate: 3.1856133590237635e-05
Step 2123 | epoch: 2.0452793834296723
Step 2124 | loss: 3.434366226196289
Step 2124 | grad_norm: 2.9427685737609863
Step 2124 | learning_rate: 3.182402055234425e-05
Step 2124 | epoch: 2.046242774566474
Step 2125 | loss: 3.152658462524414
Step 2125 | grad_norm: 3.232590436935425
Step 2125 | learning_rate: 3.179190751445087e-05
Step 2125 | epoch: 2.0472061657032756
Step 2126 | loss: 3.5187747478485107
Step 2126 | grad_norm: 3.4151558876037598
Step 2126 | learning_rate: 3.1759794476557484e-05
Step 2126 | epoch: 2.0481695568400773
Step 2127 | loss: 3.041445016860962
Step 2127 | grad_norm: 2.749255895614624
Step 2127 | learning_rate: 3.17276814386641e-05
Step 2127 | epoch: 2.0491329479768785
Step 2128 | loss: 2.919491767883301
Step 2128 | grad_norm: 3.0820517539978027
Step 2128 | learning_rate: 3.169556840077071e-05
Step 2128 | epoch: 2.05009633911368
Step 2129 | loss: 3.0996310710906982
Step 2129 | grad_norm: 2.8584132194519043
Step 2129 | learning_rate: 3.1663455362877334e-05
Step 2129 | epoch: 2.051059730250482
Step 2130 | loss: 4.445666313171387
Step 2130 | grad_norm: 3.906937837600708
Step 2130 | learning_rate: 3.163134232498394e-05
Step 2130 | epoch: 2.052023121387283
Step 2131 | loss: 3.276639699935913
Step 2131 | grad_norm: 2.9104199409484863
Step 2131 | learning_rate: 3.159922928709056e-05
Step 2131 | epoch: 2.0529865125240847
Step 2132 | loss: 3.964527130126953
Step 2132 | grad_norm: 3.7277934551239014
Step 2132 | learning_rate: 3.1567116249197176e-05
Step 2132 | epoch: 2.0539499036608864
Step 2133 | loss: 3.0646884441375732
Step 2133 | grad_norm: 2.550229072570801
Step 2133 | learning_rate: 3.153500321130379e-05
Step 2133 | epoch: 2.054913294797688
Step 2134 | loss: 3.840787172317505
Step 2134 | grad_norm: 2.946528196334839
Step 2134 | learning_rate: 3.1502890173410405e-05
Step 2134 | epoch: 2.0558766859344892
Step 2135 | loss: 3.3801393508911133
Step 2135 | grad_norm: 3.4684362411499023
Step 2135 | learning_rate: 3.147077713551702e-05
Step 2135 | epoch: 2.056840077071291
Step 2136 | loss: 4.767577648162842
Step 2136 | grad_norm: 4.544127464294434
Step 2136 | learning_rate: 3.143866409762364e-05
Step 2136 | epoch: 2.0578034682080926
Step 2137 | loss: 3.122053384780884
Step 2137 | grad_norm: 3.7154042720794678
Step 2137 | learning_rate: 3.140655105973025e-05
Step 2137 | epoch: 2.0587668593448942
Step 2138 | loss: 4.16591739654541
Step 2138 | grad_norm: 3.6318202018737793
Step 2138 | learning_rate: 3.137443802183687e-05
Step 2138 | epoch: 2.0597302504816954
Step 2139 | loss: 2.802457571029663
Step 2139 | grad_norm: 2.3745875358581543
Step 2139 | learning_rate: 3.134232498394348e-05
Step 2139 | epoch: 2.060693641618497
Step 2140 | loss: 3.908022165298462
Step 2140 | grad_norm: 5.1400299072265625
Step 2140 | learning_rate: 3.1310211946050097e-05
Step 2140 | epoch: 2.0616570327552988
Step 2141 | loss: 3.0642130374908447
Step 2141 | grad_norm: 2.107043504714966
Step 2141 | learning_rate: 3.127809890815671e-05
Step 2141 | epoch: 2.0626204238921
Step 2142 | loss: 3.177123785018921
Step 2142 | grad_norm: 2.827730178833008
Step 2142 | learning_rate: 3.1245985870263325e-05
Step 2142 | epoch: 2.0635838150289016
Step 2143 | loss: 3.7631399631500244
Step 2143 | grad_norm: 2.8577935695648193
Step 2143 | learning_rate: 3.1213872832369946e-05
Step 2143 | epoch: 2.0645472061657033
Step 2144 | loss: 3.1615278720855713
Step 2144 | grad_norm: 2.853391408920288
Step 2144 | learning_rate: 3.118175979447656e-05
Step 2144 | epoch: 2.065510597302505
Step 2145 | loss: 4.6057538986206055
Step 2145 | grad_norm: 2.910609483718872
Step 2145 | learning_rate: 3.1149646756583174e-05
Step 2145 | epoch: 2.066473988439306
Step 2146 | loss: 2.9384446144104004
Step 2146 | grad_norm: 2.3997631072998047
Step 2146 | learning_rate: 3.111753371868979e-05
Step 2146 | epoch: 2.067437379576108
Step 2147 | loss: 3.424560308456421
Step 2147 | grad_norm: 3.2755892276763916
Step 2147 | learning_rate: 3.108542068079641e-05
Step 2147 | epoch: 2.0684007707129095
Step 2148 | loss: 3.972054958343506
Step 2148 | grad_norm: 2.9434239864349365
Step 2148 | learning_rate: 3.105330764290302e-05
Step 2148 | epoch: 2.069364161849711
Step 2149 | loss: 3.739438533782959
Step 2149 | grad_norm: 2.845226764678955
Step 2149 | learning_rate: 3.102119460500964e-05
Step 2149 | epoch: 2.0703275529865124
Step 2150 | loss: 3.3384621143341064
Step 2150 | grad_norm: 3.0716865062713623
Step 2150 | learning_rate: 3.098908156711625e-05
Step 2150 | epoch: 2.071290944123314
Step 2151 | loss: 3.416266918182373
Step 2151 | grad_norm: 3.894364833831787
Step 2151 | learning_rate: 3.0956968529222866e-05
Step 2151 | epoch: 2.0722543352601157
Step 2152 | loss: 3.1422202587127686
Step 2152 | grad_norm: 3.14227032661438
Step 2152 | learning_rate: 3.092485549132948e-05
Step 2152 | epoch: 2.0732177263969174
Step 2153 | loss: 3.5705573558807373
Step 2153 | grad_norm: 2.7687714099884033
Step 2153 | learning_rate: 3.0892742453436094e-05
Step 2153 | epoch: 2.0741811175337186
Step 2154 | loss: 3.223259210586548
Step 2154 | grad_norm: 2.3795642852783203
Step 2154 | learning_rate: 3.0860629415542715e-05
Step 2154 | epoch: 2.0751445086705202
Step 2155 | loss: 4.167737007141113
Step 2155 | grad_norm: 3.458463668823242
Step 2155 | learning_rate: 3.082851637764932e-05
Step 2155 | epoch: 2.076107899807322
Step 2156 | loss: 3.5908806324005127
Step 2156 | grad_norm: 3.1932332515716553
Step 2156 | learning_rate: 3.0796403339755944e-05
Step 2156 | epoch: 2.077071290944123
Step 2157 | loss: 2.6531929969787598
Step 2157 | grad_norm: 2.4715864658355713
Step 2157 | learning_rate: 3.076429030186256e-05
Step 2157 | epoch: 2.078034682080925
Step 2158 | loss: 3.0484907627105713
Step 2158 | grad_norm: 2.5003252029418945
Step 2158 | learning_rate: 3.073217726396917e-05
Step 2158 | epoch: 2.0789980732177264
Step 2159 | loss: 3.163109064102173
Step 2159 | grad_norm: 2.404344320297241
Step 2159 | learning_rate: 3.0700064226075786e-05
Step 2159 | epoch: 2.079961464354528
Step 2160 | loss: 2.733686923980713
Step 2160 | grad_norm: 2.623607873916626
Step 2160 | learning_rate: 3.06679511881824e-05
Step 2160 | epoch: 2.0809248554913293
Step 2161 | loss: 3.801875114440918
Step 2161 | grad_norm: 2.776355504989624
Step 2161 | learning_rate: 3.063583815028902e-05
Step 2161 | epoch: 2.081888246628131
Step 2162 | loss: 3.214226007461548
Step 2162 | grad_norm: 2.4557957649230957
Step 2162 | learning_rate: 3.060372511239563e-05
Step 2162 | epoch: 2.0828516377649327
Step 2163 | loss: 3.4932861328125
Step 2163 | grad_norm: 2.6872119903564453
Step 2163 | learning_rate: 3.057161207450225e-05
Step 2163 | epoch: 2.0838150289017343
Step 2164 | loss: 4.048421382904053
Step 2164 | grad_norm: 5.330965518951416
Step 2164 | learning_rate: 3.0539499036608864e-05
Step 2164 | epoch: 2.0847784200385355
Step 2165 | loss: 3.261404514312744
Step 2165 | grad_norm: 3.294368028640747
Step 2165 | learning_rate: 3.050738599871548e-05
Step 2165 | epoch: 2.085741811175337
Step 2166 | loss: 3.882112741470337
Step 2166 | grad_norm: 3.4158053398132324
Step 2166 | learning_rate: 3.0475272960822092e-05
Step 2166 | epoch: 2.086705202312139
Step 2167 | loss: 3.7451212406158447
Step 2167 | grad_norm: 2.7302916049957275
Step 2167 | learning_rate: 3.044315992292871e-05
Step 2167 | epoch: 2.08766859344894
Step 2168 | loss: 4.365445613861084
Step 2168 | grad_norm: 2.7726073265075684
Step 2168 | learning_rate: 3.0411046885035328e-05
Step 2168 | epoch: 2.0886319845857417
Step 2169 | loss: 2.9625084400177
Step 2169 | grad_norm: 3.7544422149658203
Step 2169 | learning_rate: 3.0378933847141945e-05
Step 2169 | epoch: 2.0895953757225434
Step 2170 | loss: 2.563021659851074
Step 2170 | grad_norm: 2.2717018127441406
Step 2170 | learning_rate: 3.0346820809248556e-05
Step 2170 | epoch: 2.090558766859345
Step 2171 | loss: 3.170398473739624
Step 2171 | grad_norm: 2.8914794921875
Step 2171 | learning_rate: 3.0314707771355174e-05
Step 2171 | epoch: 2.0915221579961463
Step 2172 | loss: 3.1609609127044678
Step 2172 | grad_norm: 2.7804484367370605
Step 2172 | learning_rate: 3.0282594733461788e-05
Step 2172 | epoch: 2.092485549132948
Step 2173 | loss: 4.250236511230469
Step 2173 | grad_norm: 3.741180896759033
Step 2173 | learning_rate: 3.02504816955684e-05
Step 2173 | epoch: 2.0934489402697496
Step 2174 | loss: 4.282559394836426
Step 2174 | grad_norm: 3.390472888946533
Step 2174 | learning_rate: 3.0218368657675016e-05
Step 2174 | epoch: 2.0944123314065513
Step 2175 | loss: 3.5011792182922363
Step 2175 | grad_norm: 2.7495296001434326
Step 2175 | learning_rate: 3.0186255619781634e-05
Step 2175 | epoch: 2.0953757225433525
Step 2176 | loss: 2.4440059661865234
Step 2176 | grad_norm: 2.9172990322113037
Step 2176 | learning_rate: 3.015414258188825e-05
Step 2176 | epoch: 2.096339113680154
Step 2177 | loss: 2.730731248855591
Step 2177 | grad_norm: 2.6979377269744873
Step 2177 | learning_rate: 3.0122029543994862e-05
Step 2177 | epoch: 2.097302504816956
Step 2178 | loss: 2.9913063049316406
Step 2178 | grad_norm: 3.323577404022217
Step 2178 | learning_rate: 3.008991650610148e-05
Step 2178 | epoch: 2.098265895953757
Step 2179 | loss: 2.643069267272949
Step 2179 | grad_norm: 2.1017932891845703
Step 2179 | learning_rate: 3.0057803468208097e-05
Step 2179 | epoch: 2.0992292870905587
Step 2180 | loss: 2.7628579139709473
Step 2180 | grad_norm: 2.9932613372802734
Step 2180 | learning_rate: 3.0025690430314708e-05
Step 2180 | epoch: 2.1001926782273603
Step 2181 | loss: 3.963641405105591
Step 2181 | grad_norm: 3.3832311630249023
Step 2181 | learning_rate: 2.9993577392421322e-05
Step 2181 | epoch: 2.101156069364162
Step 2182 | loss: 3.5923259258270264
Step 2182 | grad_norm: 3.367537021636963
Step 2182 | learning_rate: 2.996146435452794e-05
Step 2182 | epoch: 2.102119460500963
Step 2183 | loss: 2.413529396057129
Step 2183 | grad_norm: 2.5883712768554688
Step 2183 | learning_rate: 2.9929351316634557e-05
Step 2183 | epoch: 2.103082851637765
Step 2184 | loss: 3.3861396312713623
Step 2184 | grad_norm: 5.799437046051025
Step 2184 | learning_rate: 2.9897238278741168e-05
Step 2184 | epoch: 2.1040462427745665
Step 2185 | loss: 2.832484722137451
Step 2185 | grad_norm: 3.041457414627075
Step 2185 | learning_rate: 2.9865125240847786e-05
Step 2185 | epoch: 2.105009633911368
Step 2186 | loss: 3.278475046157837
Step 2186 | grad_norm: 2.989989995956421
Step 2186 | learning_rate: 2.9833012202954403e-05
Step 2186 | epoch: 2.1059730250481694
Step 2187 | loss: 3.4912569522857666
Step 2187 | grad_norm: 3.2993626594543457
Step 2187 | learning_rate: 2.9800899165061014e-05
Step 2187 | epoch: 2.106936416184971
Step 2188 | loss: 2.541677951812744
Step 2188 | grad_norm: 2.854179620742798
Step 2188 | learning_rate: 2.9768786127167632e-05
Step 2188 | epoch: 2.1078998073217727
Step 2189 | loss: 3.111320734024048
Step 2189 | grad_norm: 2.505758762359619
Step 2189 | learning_rate: 2.9736673089274246e-05
Step 2189 | epoch: 2.1088631984585744
Step 2190 | loss: 3.239920139312744
Step 2190 | grad_norm: 3.450514078140259
Step 2190 | learning_rate: 2.9704560051380863e-05
Step 2190 | epoch: 2.1098265895953756
Step 2191 | loss: 2.5337679386138916
Step 2191 | grad_norm: 3.523794174194336
Step 2191 | learning_rate: 2.9672447013487474e-05
Step 2191 | epoch: 2.1107899807321773
Step 2192 | loss: 2.2792153358459473
Step 2192 | grad_norm: 2.049273729324341
Step 2192 | learning_rate: 2.9640333975594092e-05
Step 2192 | epoch: 2.111753371868979
Step 2193 | loss: 3.773757219314575
Step 2193 | grad_norm: 3.742309331893921
Step 2193 | learning_rate: 2.960822093770071e-05
Step 2193 | epoch: 2.11271676300578
Step 2194 | loss: 3.7408149242401123
Step 2194 | grad_norm: 3.0849709510803223
Step 2194 | learning_rate: 2.957610789980732e-05
Step 2194 | epoch: 2.113680154142582
Step 2195 | loss: 2.874384880065918
Step 2195 | grad_norm: 3.3598577976226807
Step 2195 | learning_rate: 2.9543994861913938e-05
Step 2195 | epoch: 2.1146435452793835
Step 2196 | loss: 3.1788060665130615
Step 2196 | grad_norm: 2.8931992053985596
Step 2196 | learning_rate: 2.9511881824020555e-05
Step 2196 | epoch: 2.115606936416185
Step 2197 | loss: 3.3663833141326904
Step 2197 | grad_norm: 3.248969078063965
Step 2197 | learning_rate: 2.947976878612717e-05
Step 2197 | epoch: 2.1165703275529864
Step 2198 | loss: 2.4319052696228027
Step 2198 | grad_norm: 2.725757122039795
Step 2198 | learning_rate: 2.9447655748233784e-05
Step 2198 | epoch: 2.117533718689788
Step 2199 | loss: 3.2873289585113525
Step 2199 | grad_norm: 2.7406067848205566
Step 2199 | learning_rate: 2.9415542710340398e-05
Step 2199 | epoch: 2.1184971098265897
Step 2200 | loss: 2.833073854446411
Step 2200 | grad_norm: 2.735955238342285
Step 2200 | learning_rate: 2.9383429672447016e-05
Step 2200 | epoch: 2.1194605009633913
Step 2201 | loss: 4.126884460449219
Step 2201 | grad_norm: 4.419632911682129
Step 2201 | learning_rate: 2.9351316634553633e-05
Step 2201 | epoch: 2.1204238921001926
Step 2202 | loss: 3.1013052463531494
Step 2202 | grad_norm: 2.8669304847717285
Step 2202 | learning_rate: 2.9319203596660244e-05
Step 2202 | epoch: 2.121387283236994
Step 2203 | loss: 3.099437952041626
Step 2203 | grad_norm: 2.790865182876587
Step 2203 | learning_rate: 2.928709055876686e-05
Step 2203 | epoch: 2.122350674373796
Step 2204 | loss: 3.210453748703003
Step 2204 | grad_norm: 3.4635813236236572
Step 2204 | learning_rate: 2.925497752087348e-05
Step 2204 | epoch: 2.123314065510597
Step 2205 | loss: 2.4210846424102783
Step 2205 | grad_norm: 2.215197801589966
Step 2205 | learning_rate: 2.922286448298009e-05
Step 2205 | epoch: 2.1242774566473988
Step 2206 | loss: 3.0555317401885986
Step 2206 | grad_norm: 2.9863312244415283
Step 2206 | learning_rate: 2.9190751445086707e-05
Step 2206 | epoch: 2.1252408477842004
Step 2207 | loss: 3.3801655769348145
Step 2207 | grad_norm: 2.677175760269165
Step 2207 | learning_rate: 2.915863840719332e-05
Step 2207 | epoch: 2.126204238921002
Step 2208 | loss: 2.7299156188964844
Step 2208 | grad_norm: 3.773435592651367
Step 2208 | learning_rate: 2.912652536929994e-05
Step 2208 | epoch: 2.1271676300578033
Step 2209 | loss: 2.914327621459961
Step 2209 | grad_norm: 2.956048011779785
Step 2209 | learning_rate: 2.909441233140655e-05
Step 2209 | epoch: 2.128131021194605
Step 2210 | loss: 3.629775047302246
Step 2210 | grad_norm: 3.156433343887329
Step 2210 | learning_rate: 2.9062299293513168e-05
Step 2210 | epoch: 2.1290944123314066
Step 2211 | loss: 3.4669582843780518
Step 2211 | grad_norm: 3.419654607772827
Step 2211 | learning_rate: 2.9030186255619785e-05
Step 2211 | epoch: 2.1300578034682083
Step 2212 | loss: 4.073101997375488
Step 2212 | grad_norm: 3.749110698699951
Step 2212 | learning_rate: 2.8998073217726396e-05
Step 2212 | epoch: 2.1310211946050095
Step 2213 | loss: 3.2774224281311035
Step 2213 | grad_norm: 3.336308717727661
Step 2213 | learning_rate: 2.8965960179833014e-05
Step 2213 | epoch: 2.131984585741811
Step 2214 | loss: 3.7734317779541016
Step 2214 | grad_norm: 4.224903583526611
Step 2214 | learning_rate: 2.893384714193963e-05
Step 2214 | epoch: 2.132947976878613
Step 2215 | loss: 2.4991655349731445
Step 2215 | grad_norm: 2.3163695335388184
Step 2215 | learning_rate: 2.8901734104046245e-05
Step 2215 | epoch: 2.133911368015414
Step 2216 | loss: 3.4037322998046875
Step 2216 | grad_norm: 3.603173017501831
Step 2216 | learning_rate: 2.8869621066152856e-05
Step 2216 | epoch: 2.1348747591522157
Step 2217 | loss: 3.0032901763916016
Step 2217 | grad_norm: 2.7710578441619873
Step 2217 | learning_rate: 2.8837508028259474e-05
Step 2217 | epoch: 2.1358381502890174
Step 2218 | loss: 2.984455108642578
Step 2218 | grad_norm: 3.1698484420776367
Step 2218 | learning_rate: 2.880539499036609e-05
Step 2218 | epoch: 2.136801541425819
Step 2219 | loss: 2.589296579360962
Step 2219 | grad_norm: 3.089306354522705
Step 2219 | learning_rate: 2.8773281952472702e-05
Step 2219 | epoch: 2.1377649325626202
Step 2220 | loss: 2.8557662963867188
Step 2220 | grad_norm: 2.479006767272949
Step 2220 | learning_rate: 2.874116891457932e-05
Step 2220 | epoch: 2.138728323699422
Step 2221 | loss: 3.771599769592285
Step 2221 | grad_norm: 3.0618557929992676
Step 2221 | learning_rate: 2.8709055876685937e-05
Step 2221 | epoch: 2.1396917148362236
Step 2222 | loss: 3.1517906188964844
Step 2222 | grad_norm: 4.222952842712402
Step 2222 | learning_rate: 2.8676942838792555e-05
Step 2222 | epoch: 2.140655105973025
Step 2223 | loss: 3.564333438873291
Step 2223 | grad_norm: 2.91754150390625
Step 2223 | learning_rate: 2.8644829800899166e-05
Step 2223 | epoch: 2.1416184971098264
Step 2224 | loss: 2.928222894668579
Step 2224 | grad_norm: 2.450843334197998
Step 2224 | learning_rate: 2.861271676300578e-05
Step 2224 | epoch: 2.142581888246628
Step 2225 | loss: 3.056720495223999
Step 2225 | grad_norm: 3.2410614490509033
Step 2225 | learning_rate: 2.8580603725112397e-05
Step 2225 | epoch: 2.1435452793834298
Step 2226 | loss: 2.5667550563812256
Step 2226 | grad_norm: 3.5931832790374756
Step 2226 | learning_rate: 2.8548490687219015e-05
Step 2226 | epoch: 2.1445086705202314
Step 2227 | loss: 3.674858808517456
Step 2227 | grad_norm: 2.8254928588867188
Step 2227 | learning_rate: 2.8516377649325626e-05
Step 2227 | epoch: 2.1454720616570326
Step 2228 | loss: 3.457858085632324
Step 2228 | grad_norm: 4.356595039367676
Step 2228 | learning_rate: 2.8484264611432243e-05
Step 2228 | epoch: 2.1464354527938343
Step 2229 | loss: 3.4323618412017822
Step 2229 | grad_norm: 3.3384687900543213
Step 2229 | learning_rate: 2.845215157353886e-05
Step 2229 | epoch: 2.147398843930636
Step 2230 | loss: 3.0427370071411133
Step 2230 | grad_norm: 2.1526498794555664
Step 2230 | learning_rate: 2.842003853564547e-05
Step 2230 | epoch: 2.148362235067437
Step 2231 | loss: 3.753805637359619
Step 2231 | grad_norm: 3.5781431198120117
Step 2231 | learning_rate: 2.838792549775209e-05
Step 2231 | epoch: 2.149325626204239
Step 2232 | loss: 3.787703514099121
Step 2232 | grad_norm: 3.234118700027466
Step 2232 | learning_rate: 2.8355812459858703e-05
Step 2232 | epoch: 2.1502890173410405
Step 2233 | loss: 3.450620651245117
Step 2233 | grad_norm: 3.958798408508301
Step 2233 | learning_rate: 2.832369942196532e-05
Step 2233 | epoch: 2.151252408477842
Step 2234 | loss: 2.6507880687713623
Step 2234 | grad_norm: 2.8004043102264404
Step 2234 | learning_rate: 2.8291586384071932e-05
Step 2234 | epoch: 2.1522157996146434
Step 2235 | loss: 3.573026180267334
Step 2235 | grad_norm: 3.254906177520752
Step 2235 | learning_rate: 2.825947334617855e-05
Step 2235 | epoch: 2.153179190751445
Step 2236 | loss: 2.731194257736206
Step 2236 | grad_norm: 2.7235021591186523
Step 2236 | learning_rate: 2.8227360308285167e-05
Step 2236 | epoch: 2.1541425818882467
Step 2237 | loss: 3.267242431640625
Step 2237 | grad_norm: 2.7978227138519287
Step 2237 | learning_rate: 2.8195247270391778e-05
Step 2237 | epoch: 2.1551059730250484
Step 2238 | loss: 3.585052728652954
Step 2238 | grad_norm: 4.965991973876953
Step 2238 | learning_rate: 2.8163134232498395e-05
Step 2238 | epoch: 2.1560693641618496
Step 2239 | loss: 3.4791252613067627
Step 2239 | grad_norm: 2.859008550643921
Step 2239 | learning_rate: 2.8131021194605013e-05
Step 2239 | epoch: 2.1570327552986512
Step 2240 | loss: 3.4642293453216553
Step 2240 | grad_norm: 2.633291721343994
Step 2240 | learning_rate: 2.8098908156711627e-05
Step 2240 | epoch: 2.157996146435453
Step 2241 | loss: 3.326793670654297
Step 2241 | grad_norm: 2.49987530708313
Step 2241 | learning_rate: 2.806679511881824e-05
Step 2241 | epoch: 2.1589595375722546
Step 2242 | loss: 2.826935291290283
Step 2242 | grad_norm: 2.784111738204956
Step 2242 | learning_rate: 2.8034682080924855e-05
Step 2242 | epoch: 2.159922928709056
Step 2243 | loss: 2.7537524700164795
Step 2243 | grad_norm: 3.5260605812072754
Step 2243 | learning_rate: 2.8002569043031473e-05
Step 2243 | epoch: 2.1608863198458574
Step 2244 | loss: 3.101045846939087
Step 2244 | grad_norm: 2.9006173610687256
Step 2244 | learning_rate: 2.7970456005138084e-05
Step 2244 | epoch: 2.161849710982659
Step 2245 | loss: 2.9247443675994873
Step 2245 | grad_norm: 3.219444513320923
Step 2245 | learning_rate: 2.79383429672447e-05
Step 2245 | epoch: 2.1628131021194603
Step 2246 | loss: 3.129495143890381
Step 2246 | grad_norm: 3.290182113647461
Step 2246 | learning_rate: 2.790622992935132e-05
Step 2246 | epoch: 2.163776493256262
Step 2247 | loss: 3.6149001121520996
Step 2247 | grad_norm: 5.456209659576416
Step 2247 | learning_rate: 2.7874116891457937e-05
Step 2247 | epoch: 2.1647398843930636
Step 2248 | loss: 3.0508384704589844
Step 2248 | grad_norm: 2.4555132389068604
Step 2248 | learning_rate: 2.7842003853564547e-05
Step 2248 | epoch: 2.1657032755298653
Step 2249 | loss: 3.0715956687927246
Step 2249 | grad_norm: 3.0303001403808594
Step 2249 | learning_rate: 2.7809890815671165e-05
Step 2249 | epoch: 2.1666666666666665
Step 2250 | loss: 3.8717215061187744
Step 2250 | grad_norm: 3.321340322494507
Step 2250 | learning_rate: 2.777777777777778e-05
Step 2250 | epoch: 2.167630057803468
Step 2251 | loss: 3.1112256050109863
Step 2251 | grad_norm: 3.649125576019287
Step 2251 | learning_rate: 2.7745664739884393e-05
Step 2251 | epoch: 2.16859344894027
Step 2252 | loss: 3.429506778717041
Step 2252 | grad_norm: 2.706249952316284
Step 2252 | learning_rate: 2.7713551701991007e-05
Step 2252 | epoch: 2.169556840077071
Step 2253 | loss: 2.958272933959961
Step 2253 | grad_norm: 2.7979507446289062
Step 2253 | learning_rate: 2.7681438664097625e-05
Step 2253 | epoch: 2.1705202312138727
Step 2254 | loss: 3.1138103008270264
Step 2254 | grad_norm: 2.4369683265686035
Step 2254 | learning_rate: 2.7649325626204243e-05
Step 2254 | epoch: 2.1714836223506744
Step 2255 | loss: 2.8459739685058594
Step 2255 | grad_norm: 2.152963638305664
Step 2255 | learning_rate: 2.7617212588310853e-05
Step 2255 | epoch: 2.172447013487476
Step 2256 | loss: 3.822711944580078
Step 2256 | grad_norm: 3.428433656692505
Step 2256 | learning_rate: 2.758509955041747e-05
Step 2256 | epoch: 2.1734104046242773
Step 2257 | loss: 3.2978646755218506
Step 2257 | grad_norm: 2.6147310733795166
Step 2257 | learning_rate: 2.755298651252409e-05
Step 2257 | epoch: 2.174373795761079
Step 2258 | loss: 2.7191715240478516
Step 2258 | grad_norm: 2.734632730484009
Step 2258 | learning_rate: 2.7520873474630703e-05
Step 2258 | epoch: 2.1753371868978806
Step 2259 | loss: 3.0599310398101807
Step 2259 | grad_norm: 2.559108257293701
Step 2259 | learning_rate: 2.7488760436737317e-05
Step 2259 | epoch: 2.1763005780346822
Step 2260 | loss: 3.927978992462158
Step 2260 | grad_norm: 3.2304558753967285
Step 2260 | learning_rate: 2.745664739884393e-05
Step 2260 | epoch: 2.1772639691714835
Step 2261 | loss: 2.7890121936798096
Step 2261 | grad_norm: 2.6520557403564453
Step 2261 | learning_rate: 2.742453436095055e-05
Step 2261 | epoch: 2.178227360308285
Step 2262 | loss: 3.2442753314971924
Step 2262 | grad_norm: 2.451923131942749
Step 2262 | learning_rate: 2.739242132305716e-05
Step 2262 | epoch: 2.179190751445087
Step 2263 | loss: 3.1986637115478516
Step 2263 | grad_norm: 3.768908977508545
Step 2263 | learning_rate: 2.7360308285163777e-05
Step 2263 | epoch: 2.1801541425818884
Step 2264 | loss: 3.3986382484436035
Step 2264 | grad_norm: 2.8015544414520264
Step 2264 | learning_rate: 2.7328195247270395e-05
Step 2264 | epoch: 2.1811175337186897
Step 2265 | loss: 3.518859624862671
Step 2265 | grad_norm: 4.2547287940979
Step 2265 | learning_rate: 2.7296082209377012e-05
Step 2265 | epoch: 2.1820809248554913
Step 2266 | loss: 2.6203675270080566
Step 2266 | grad_norm: 3.3921215534210205
Step 2266 | learning_rate: 2.7263969171483623e-05
Step 2266 | epoch: 2.183044315992293
Step 2267 | loss: 3.3740267753601074
Step 2267 | grad_norm: 3.076047420501709
Step 2267 | learning_rate: 2.7231856133590237e-05
Step 2267 | epoch: 2.184007707129094
Step 2268 | loss: 3.733245611190796
Step 2268 | grad_norm: 2.863849639892578
Step 2268 | learning_rate: 2.7199743095696855e-05
Step 2268 | epoch: 2.184971098265896
Step 2269 | loss: 2.464895486831665
Step 2269 | grad_norm: 2.961310386657715
Step 2269 | learning_rate: 2.7167630057803466e-05
Step 2269 | epoch: 2.1859344894026975
Step 2270 | loss: 3.693500518798828
Step 2270 | grad_norm: 3.17212176322937
Step 2270 | learning_rate: 2.7135517019910083e-05
Step 2270 | epoch: 2.186897880539499
Step 2271 | loss: 3.4808108806610107
Step 2271 | grad_norm: 2.384401321411133
Step 2271 | learning_rate: 2.71034039820167e-05
Step 2271 | epoch: 2.1878612716763004
Step 2272 | loss: 2.9843151569366455
Step 2272 | grad_norm: 2.629324197769165
Step 2272 | learning_rate: 2.707129094412332e-05
Step 2272 | epoch: 2.188824662813102
Step 2273 | loss: 3.229081153869629
Step 2273 | grad_norm: 2.2632715702056885
Step 2273 | learning_rate: 2.703917790622993e-05
Step 2273 | epoch: 2.1897880539499037
Step 2274 | loss: 3.428391695022583
Step 2274 | grad_norm: 2.6532278060913086
Step 2274 | learning_rate: 2.7007064868336547e-05
Step 2274 | epoch: 2.1907514450867054
Step 2275 | loss: 3.8776841163635254
Step 2275 | grad_norm: 3.2313921451568604
Step 2275 | learning_rate: 2.697495183044316e-05
Step 2275 | epoch: 2.1917148362235066
Step 2276 | loss: 3.028581142425537
Step 2276 | grad_norm: 2.577683210372925
Step 2276 | learning_rate: 2.6942838792549775e-05
Step 2276 | epoch: 2.1926782273603083
Step 2277 | loss: 4.2573418617248535
Step 2277 | grad_norm: 3.293783664703369
Step 2277 | learning_rate: 2.691072575465639e-05
Step 2277 | epoch: 2.19364161849711
Step 2278 | loss: 4.135391712188721
Step 2278 | grad_norm: 3.232933521270752
Step 2278 | learning_rate: 2.6878612716763007e-05
Step 2278 | epoch: 2.1946050096339116
Step 2279 | loss: 2.5165293216705322
Step 2279 | grad_norm: 2.8202223777770996
Step 2279 | learning_rate: 2.6846499678869624e-05
Step 2279 | epoch: 2.195568400770713
Step 2280 | loss: 2.7624852657318115
Step 2280 | grad_norm: 3.0761613845825195
Step 2280 | learning_rate: 2.6814386640976235e-05
Step 2280 | epoch: 2.1965317919075145
Step 2281 | loss: 3.201016902923584
Step 2281 | grad_norm: 4.217084884643555
Step 2281 | learning_rate: 2.6782273603082853e-05
Step 2281 | epoch: 2.197495183044316
Step 2282 | loss: 4.325711250305176
Step 2282 | grad_norm: 3.0555002689361572
Step 2282 | learning_rate: 2.675016056518947e-05
Step 2282 | epoch: 2.1984585741811173
Step 2283 | loss: 3.0684900283813477
Step 2283 | grad_norm: 2.897359848022461
Step 2283 | learning_rate: 2.6718047527296085e-05
Step 2283 | epoch: 2.199421965317919
Step 2284 | loss: 3.038792371749878
Step 2284 | grad_norm: 3.1258718967437744
Step 2284 | learning_rate: 2.66859344894027e-05
Step 2284 | epoch: 2.2003853564547207
Step 2285 | loss: 3.285693407058716
Step 2285 | grad_norm: 3.083203077316284
Step 2285 | learning_rate: 2.6653821451509313e-05
Step 2285 | epoch: 2.2013487475915223
Step 2286 | loss: 2.803191661834717
Step 2286 | grad_norm: 2.6828410625457764
Step 2286 | learning_rate: 2.662170841361593e-05
Step 2286 | epoch: 2.2023121387283235
Step 2287 | loss: 3.085787773132324
Step 2287 | grad_norm: 2.6398673057556152
Step 2287 | learning_rate: 2.658959537572254e-05
Step 2287 | epoch: 2.203275529865125
Step 2288 | loss: 3.0058302879333496
Step 2288 | grad_norm: 2.8526904582977295
Step 2288 | learning_rate: 2.655748233782916e-05
Step 2288 | epoch: 2.204238921001927
Step 2289 | loss: 3.012951612472534
Step 2289 | grad_norm: 3.2954413890838623
Step 2289 | learning_rate: 2.6525369299935776e-05
Step 2289 | epoch: 2.2052023121387285
Step 2290 | loss: 3.7519032955169678
Step 2290 | grad_norm: 3.1482625007629395
Step 2290 | learning_rate: 2.6493256262042394e-05
Step 2290 | epoch: 2.2061657032755297
Step 2291 | loss: 2.575612783432007
Step 2291 | grad_norm: 3.4302079677581787
Step 2291 | learning_rate: 2.6461143224149005e-05
Step 2291 | epoch: 2.2071290944123314
Step 2292 | loss: 3.9516358375549316
Step 2292 | grad_norm: 3.2204322814941406
Step 2292 | learning_rate: 2.6429030186255622e-05
Step 2292 | epoch: 2.208092485549133
Step 2293 | loss: 3.0047945976257324
Step 2293 | grad_norm: 2.732926845550537
Step 2293 | learning_rate: 2.6396917148362237e-05
Step 2293 | epoch: 2.2090558766859343
Step 2294 | loss: 2.9254274368286133
Step 2294 | grad_norm: 3.2445473670959473
Step 2294 | learning_rate: 2.636480411046885e-05
Step 2294 | epoch: 2.210019267822736
Step 2295 | loss: 2.8448870182037354
Step 2295 | grad_norm: 3.004112720489502
Step 2295 | learning_rate: 2.6332691072575465e-05
Step 2295 | epoch: 2.2109826589595376
Step 2296 | loss: 3.6233623027801514
Step 2296 | grad_norm: 3.2167272567749023
Step 2296 | learning_rate: 2.6300578034682083e-05
Step 2296 | epoch: 2.2119460500963393
Step 2297 | loss: 3.2571213245391846
Step 2297 | grad_norm: 3.13946795463562
Step 2297 | learning_rate: 2.62684649967887e-05
Step 2297 | epoch: 2.2129094412331405
Step 2298 | loss: 3.0527868270874023
Step 2298 | grad_norm: 2.7972419261932373
Step 2298 | learning_rate: 2.623635195889531e-05
Step 2298 | epoch: 2.213872832369942
Step 2299 | loss: 3.1700456142425537
Step 2299 | grad_norm: 2.677015542984009
Step 2299 | learning_rate: 2.620423892100193e-05
Step 2299 | epoch: 2.214836223506744
Step 2300 | loss: 3.757504463195801
Step 2300 | grad_norm: 4.906383514404297
Step 2300 | learning_rate: 2.6172125883108546e-05
Step 2300 | epoch: 2.2157996146435455
Step 2301 | loss: 2.8607702255249023
Step 2301 | grad_norm: 2.3301405906677246
Step 2301 | learning_rate: 2.6140012845215157e-05
Step 2301 | epoch: 2.2167630057803467
Step 2302 | loss: 2.903134822845459
Step 2302 | grad_norm: 2.8430685997009277
Step 2302 | learning_rate: 2.6107899807321774e-05
Step 2302 | epoch: 2.2177263969171483
Step 2303 | loss: 3.6598594188690186
Step 2303 | grad_norm: 2.6871018409729004
Step 2303 | learning_rate: 2.607578676942839e-05
Step 2303 | epoch: 2.21868978805395
Step 2304 | loss: 2.9109926223754883
Step 2304 | grad_norm: 2.142280101776123
Step 2304 | learning_rate: 2.6043673731535006e-05
Step 2304 | epoch: 2.2196531791907512
Step 2305 | loss: 3.1932284832000732
Step 2305 | grad_norm: 2.7311317920684814
Step 2305 | learning_rate: 2.6011560693641617e-05
Step 2305 | epoch: 2.220616570327553
Step 2306 | loss: 3.350515842437744
Step 2306 | grad_norm: 2.6970338821411133
Step 2306 | learning_rate: 2.5979447655748235e-05
Step 2306 | epoch: 2.2215799614643545
Step 2307 | loss: 3.4901647567749023
Step 2307 | grad_norm: 2.5051891803741455
Step 2307 | learning_rate: 2.5947334617854852e-05
Step 2307 | epoch: 2.222543352601156
Step 2308 | loss: 2.7507920265197754
Step 2308 | grad_norm: 2.939401626586914
Step 2308 | learning_rate: 2.5915221579961463e-05
Step 2308 | epoch: 2.2235067437379574
Step 2309 | loss: 3.272986650466919
Step 2309 | grad_norm: 3.4002201557159424
Step 2309 | learning_rate: 2.588310854206808e-05
Step 2309 | epoch: 2.224470134874759
Step 2310 | loss: 3.829011917114258
Step 2310 | grad_norm: 3.0520131587982178
Step 2310 | learning_rate: 2.5850995504174698e-05
Step 2310 | epoch: 2.2254335260115607
Step 2311 | loss: 3.9764108657836914
Step 2311 | grad_norm: 4.099773406982422
Step 2311 | learning_rate: 2.5818882466281312e-05
Step 2311 | epoch: 2.2263969171483624
Step 2312 | loss: 3.1025631427764893
Step 2312 | grad_norm: 2.293095827102661
Step 2312 | learning_rate: 2.5786769428387923e-05
Step 2312 | epoch: 2.2273603082851636
Step 2313 | loss: 2.814236879348755
Step 2313 | grad_norm: 2.743134021759033
Step 2313 | learning_rate: 2.575465639049454e-05
Step 2313 | epoch: 2.2283236994219653
Step 2314 | loss: 3.832735300064087
Step 2314 | grad_norm: 3.730055332183838
Step 2314 | learning_rate: 2.5722543352601158e-05
Step 2314 | epoch: 2.229287090558767
Step 2315 | loss: 4.037609577178955
Step 2315 | grad_norm: 3.0428659915924072
Step 2315 | learning_rate: 2.5690430314707776e-05
Step 2315 | epoch: 2.2302504816955686
Step 2316 | loss: 3.0372893810272217
Step 2316 | grad_norm: 3.1433374881744385
Step 2316 | learning_rate: 2.5658317276814387e-05
Step 2316 | epoch: 2.23121387283237
Step 2317 | loss: 3.638303756713867
Step 2317 | grad_norm: 2.968778371810913
Step 2317 | learning_rate: 2.5626204238921004e-05
Step 2317 | epoch: 2.2321772639691715
Step 2318 | loss: 3.6493773460388184
Step 2318 | grad_norm: 3.0752310752868652
Step 2318 | learning_rate: 2.5594091201027622e-05
Step 2318 | epoch: 2.233140655105973
Step 2319 | loss: 3.4944255352020264
Step 2319 | grad_norm: 2.9611971378326416
Step 2319 | learning_rate: 2.5561978163134233e-05
Step 2319 | epoch: 2.2341040462427744
Step 2320 | loss: 3.404386281967163
Step 2320 | grad_norm: 2.8281311988830566
Step 2320 | learning_rate: 2.5529865125240847e-05
Step 2320 | epoch: 2.235067437379576
Step 2321 | loss: 3.33491587638855
Step 2321 | grad_norm: 3.5203847885131836
Step 2321 | learning_rate: 2.5497752087347464e-05
Step 2321 | epoch: 2.2360308285163777
Step 2322 | loss: 3.4422268867492676
Step 2322 | grad_norm: 3.497755527496338
Step 2322 | learning_rate: 2.5465639049454082e-05
Step 2322 | epoch: 2.2369942196531793
Step 2323 | loss: 3.845004081726074
Step 2323 | grad_norm: 3.160013437271118
Step 2323 | learning_rate: 2.5433526011560693e-05
Step 2323 | epoch: 2.2379576107899806
Step 2324 | loss: 3.0860235691070557
Step 2324 | grad_norm: 2.5069127082824707
Step 2324 | learning_rate: 2.540141297366731e-05
Step 2324 | epoch: 2.2389210019267822
Step 2325 | loss: 3.92168927192688
Step 2325 | grad_norm: 2.7215168476104736
Step 2325 | learning_rate: 2.5369299935773928e-05
Step 2325 | epoch: 2.239884393063584
Step 2326 | loss: 2.972060203552246
Step 2326 | grad_norm: 3.0353097915649414
Step 2326 | learning_rate: 2.533718689788054e-05
Step 2326 | epoch: 2.2408477842003856
Step 2327 | loss: 3.6324002742767334
Step 2327 | grad_norm: 3.0294342041015625
Step 2327 | learning_rate: 2.5305073859987156e-05
Step 2327 | epoch: 2.2418111753371868
Step 2328 | loss: 3.8721580505371094
Step 2328 | grad_norm: 2.9041407108306885
Step 2328 | learning_rate: 2.527296082209377e-05
Step 2328 | epoch: 2.2427745664739884
Step 2329 | loss: 2.986781120300293
Step 2329 | grad_norm: 3.148623466491699
Step 2329 | learning_rate: 2.5240847784200388e-05
Step 2329 | epoch: 2.24373795761079
Step 2330 | loss: 3.0520920753479004
Step 2330 | grad_norm: 2.757864475250244
Step 2330 | learning_rate: 2.5208734746307e-05
Step 2330 | epoch: 2.2447013487475918
Step 2331 | loss: 2.907711982727051
Step 2331 | grad_norm: 2.7242069244384766
Step 2331 | learning_rate: 2.5176621708413616e-05
Step 2331 | epoch: 2.245664739884393
Step 2332 | loss: 2.9683427810668945
Step 2332 | grad_norm: 3.6535186767578125
Step 2332 | learning_rate: 2.5144508670520234e-05
Step 2332 | epoch: 2.2466281310211946
Step 2333 | loss: 4.242371559143066
Step 2333 | grad_norm: 3.5916545391082764
Step 2333 | learning_rate: 2.5112395632626845e-05
Step 2333 | epoch: 2.2475915221579963
Step 2334 | loss: 3.3706347942352295
Step 2334 | grad_norm: 2.7210428714752197
Step 2334 | learning_rate: 2.5080282594733462e-05
Step 2334 | epoch: 2.2485549132947975
Step 2335 | loss: 3.205751895904541
Step 2335 | grad_norm: 2.335891008377075
Step 2335 | learning_rate: 2.504816955684008e-05
Step 2335 | epoch: 2.249518304431599
Step 2336 | loss: 3.6206424236297607
Step 2336 | grad_norm: 3.2038938999176025
Step 2336 | learning_rate: 2.5016056518946694e-05
Step 2336 | epoch: 2.250481695568401
Step 2337 | loss: 3.545830249786377
Step 2337 | grad_norm: 4.573442459106445
Step 2337 | learning_rate: 2.4983943481053308e-05
Step 2337 | epoch: 2.2514450867052025
Step 2338 | loss: 3.4901721477508545
Step 2338 | grad_norm: 3.086122512817383
Step 2338 | learning_rate: 2.4951830443159922e-05
Step 2338 | epoch: 2.2524084778420037
Step 2339 | loss: 3.038911819458008
Step 2339 | grad_norm: 3.26057505607605
Step 2339 | learning_rate: 2.491971740526654e-05
Step 2339 | epoch: 2.2533718689788054
Step 2340 | loss: 3.7119479179382324
Step 2340 | grad_norm: 3.540865182876587
Step 2340 | learning_rate: 2.4887604367373154e-05
Step 2340 | epoch: 2.254335260115607
Step 2341 | loss: 3.152177572250366
Step 2341 | grad_norm: 2.54613995552063
Step 2341 | learning_rate: 2.485549132947977e-05
Step 2341 | epoch: 2.2552986512524082
Step 2342 | loss: 3.1851553916931152
Step 2342 | grad_norm: 3.9502601623535156
Step 2342 | learning_rate: 2.4823378291586386e-05
Step 2342 | epoch: 2.25626204238921
Step 2343 | loss: 4.367134094238281
Step 2343 | grad_norm: 3.3385002613067627
Step 2343 | learning_rate: 2.4791265253693e-05
Step 2343 | epoch: 2.2572254335260116
Step 2344 | loss: 2.804718017578125
Step 2344 | grad_norm: 3.052737236022949
Step 2344 | learning_rate: 2.4759152215799618e-05
Step 2344 | epoch: 2.2581888246628132
Step 2345 | loss: 3.2882633209228516
Step 2345 | grad_norm: 3.5968289375305176
Step 2345 | learning_rate: 2.4727039177906232e-05
Step 2345 | epoch: 2.2591522157996144
Step 2346 | loss: 3.4400858879089355
Step 2346 | grad_norm: 2.8934926986694336
Step 2346 | learning_rate: 2.4694926140012846e-05
Step 2346 | epoch: 2.260115606936416
Step 2347 | loss: 2.9181411266326904
Step 2347 | grad_norm: 3.3723206520080566
Step 2347 | learning_rate: 2.466281310211946e-05
Step 2347 | epoch: 2.2610789980732178
Step 2348 | loss: 3.767523765563965
Step 2348 | grad_norm: 3.2399184703826904
Step 2348 | learning_rate: 2.4630700064226078e-05
Step 2348 | epoch: 2.2620423892100194
Step 2349 | loss: 3.8269097805023193
Step 2349 | grad_norm: 3.4739439487457275
Step 2349 | learning_rate: 2.4598587026332692e-05
Step 2349 | epoch: 2.2630057803468207
Step 2350 | loss: 2.7993247509002686
Step 2350 | grad_norm: 2.917001485824585
Step 2350 | learning_rate: 2.4566473988439306e-05
Step 2350 | epoch: 2.2639691714836223
Step 2351 | loss: 3.6094048023223877
Step 2351 | grad_norm: 3.0756168365478516
Step 2351 | learning_rate: 2.4534360950545924e-05
Step 2351 | epoch: 2.264932562620424
Step 2352 | loss: 3.288074254989624
Step 2352 | grad_norm: 2.687687873840332
Step 2352 | learning_rate: 2.4502247912652538e-05
Step 2352 | epoch: 2.2658959537572256
Step 2353 | loss: 3.655146598815918
Step 2353 | grad_norm: 3.634556770324707
Step 2353 | learning_rate: 2.4470134874759156e-05
Step 2353 | epoch: 2.266859344894027
Step 2354 | loss: 2.876955270767212
Step 2354 | grad_norm: 2.776594400405884
Step 2354 | learning_rate: 2.443802183686577e-05
Step 2354 | epoch: 2.2678227360308285
Step 2355 | loss: 3.04742169380188
Step 2355 | grad_norm: 3.1335558891296387
Step 2355 | learning_rate: 2.4405908798972384e-05
Step 2355 | epoch: 2.26878612716763
Step 2356 | loss: 3.4522504806518555
Step 2356 | grad_norm: 2.8163795471191406
Step 2356 | learning_rate: 2.4373795761078998e-05
Step 2356 | epoch: 2.2697495183044314
Step 2357 | loss: 2.8934106826782227
Step 2357 | grad_norm: 3.115504264831543
Step 2357 | learning_rate: 2.4341682723185612e-05
Step 2357 | epoch: 2.270712909441233
Step 2358 | loss: 4.283005237579346
Step 2358 | grad_norm: 4.144738674163818
Step 2358 | learning_rate: 2.430956968529223e-05
Step 2358 | epoch: 2.2716763005780347
Step 2359 | loss: 2.66292667388916
Step 2359 | grad_norm: 3.168011426925659
Step 2359 | learning_rate: 2.4277456647398844e-05
Step 2359 | epoch: 2.2726396917148364
Step 2360 | loss: 2.916506052017212
Step 2360 | grad_norm: 2.9543986320495605
Step 2360 | learning_rate: 2.4245343609505462e-05
Step 2360 | epoch: 2.2736030828516376
Step 2361 | loss: 3.192988157272339
Step 2361 | grad_norm: 4.167932033538818
Step 2361 | learning_rate: 2.4213230571612076e-05
Step 2361 | epoch: 2.2745664739884393
Step 2362 | loss: 3.4141714572906494
Step 2362 | grad_norm: 3.0235257148742676
Step 2362 | learning_rate: 2.4181117533718693e-05
Step 2362 | epoch: 2.275529865125241
Step 2363 | loss: 3.45444655418396
Step 2363 | grad_norm: 2.127626419067383
Step 2363 | learning_rate: 2.4149004495825304e-05
Step 2363 | epoch: 2.2764932562620426
Step 2364 | loss: 4.842234134674072
Step 2364 | grad_norm: 4.639107704162598
Step 2364 | learning_rate: 2.4116891457931922e-05
Step 2364 | epoch: 2.277456647398844
Step 2365 | loss: 3.4465949535369873
Step 2365 | grad_norm: 2.6402833461761475
Step 2365 | learning_rate: 2.4084778420038536e-05
Step 2365 | epoch: 2.2784200385356455
Step 2366 | loss: 3.3924150466918945
Step 2366 | grad_norm: 3.0358269214630127
Step 2366 | learning_rate: 2.405266538214515e-05
Step 2366 | epoch: 2.279383429672447
Step 2367 | loss: 2.748605728149414
Step 2367 | grad_norm: 2.46445894241333
Step 2367 | learning_rate: 2.4020552344251768e-05
Step 2367 | epoch: 2.2803468208092488
Step 2368 | loss: 3.0766208171844482
Step 2368 | grad_norm: 3.5212435722351074
Step 2368 | learning_rate: 2.3988439306358382e-05
Step 2368 | epoch: 2.28131021194605
Step 2369 | loss: 2.996375560760498
Step 2369 | grad_norm: 4.244745254516602
Step 2369 | learning_rate: 2.3956326268465e-05
Step 2369 | epoch: 2.2822736030828517
Step 2370 | loss: 3.0470101833343506
Step 2370 | grad_norm: 3.9421494007110596
Step 2370 | learning_rate: 2.3924213230571614e-05
Step 2370 | epoch: 2.2832369942196533
Step 2371 | loss: 2.9613935947418213
Step 2371 | grad_norm: 3.2207062244415283
Step 2371 | learning_rate: 2.3892100192678228e-05
Step 2371 | epoch: 2.2842003853564545
Step 2372 | loss: 3.4573445320129395
Step 2372 | grad_norm: 3.3830950260162354
Step 2372 | learning_rate: 2.3859987154784842e-05
Step 2372 | epoch: 2.285163776493256
Step 2373 | loss: 3.139848470687866
Step 2373 | grad_norm: 3.1096863746643066
Step 2373 | learning_rate: 2.382787411689146e-05
Step 2373 | epoch: 2.286127167630058
Step 2374 | loss: 3.3466272354125977
Step 2374 | grad_norm: 3.5941641330718994
Step 2374 | learning_rate: 2.3795761078998074e-05
Step 2374 | epoch: 2.2870905587668595
Step 2375 | loss: 2.7474348545074463
Step 2375 | grad_norm: 2.304572820663452
Step 2375 | learning_rate: 2.3763648041104688e-05
Step 2375 | epoch: 2.2880539499036607
Step 2376 | loss: 3.07043194770813
Step 2376 | grad_norm: 3.0001704692840576
Step 2376 | learning_rate: 2.3731535003211306e-05
Step 2376 | epoch: 2.2890173410404624
Step 2377 | loss: 3.794530153274536
Step 2377 | grad_norm: 3.2043416500091553
Step 2377 | learning_rate: 2.369942196531792e-05
Step 2377 | epoch: 2.289980732177264
Step 2378 | loss: 3.432123899459839
Step 2378 | grad_norm: 3.0219879150390625
Step 2378 | learning_rate: 2.3667308927424537e-05
Step 2378 | epoch: 2.2909441233140653
Step 2379 | loss: 3.2337212562561035
Step 2379 | grad_norm: 2.382801055908203
Step 2379 | learning_rate: 2.363519588953115e-05
Step 2379 | epoch: 2.291907514450867
Step 2380 | loss: 2.906684160232544
Step 2380 | grad_norm: 3.1542677879333496
Step 2380 | learning_rate: 2.3603082851637766e-05
Step 2380 | epoch: 2.2928709055876686
Step 2381 | loss: 3.1078460216522217
Step 2381 | grad_norm: 2.9319798946380615
Step 2381 | learning_rate: 2.357096981374438e-05
Step 2381 | epoch: 2.2938342967244703
Step 2382 | loss: 3.1789259910583496
Step 2382 | grad_norm: 3.1645212173461914
Step 2382 | learning_rate: 2.3538856775850994e-05
Step 2382 | epoch: 2.294797687861272
Step 2383 | loss: 2.939061164855957
Step 2383 | grad_norm: 3.2572922706604004
Step 2383 | learning_rate: 2.3506743737957612e-05
Step 2383 | epoch: 2.295761078998073
Step 2384 | loss: 2.8453047275543213
Step 2384 | grad_norm: 3.0600943565368652
Step 2384 | learning_rate: 2.3474630700064226e-05
Step 2384 | epoch: 2.296724470134875
Step 2385 | loss: 3.642263412475586
Step 2385 | grad_norm: 3.2892820835113525
Step 2385 | learning_rate: 2.3442517662170844e-05
Step 2385 | epoch: 2.2976878612716765
Step 2386 | loss: 3.7609703540802
Step 2386 | grad_norm: 3.079465389251709
Step 2386 | learning_rate: 2.3410404624277458e-05
Step 2386 | epoch: 2.2986512524084777
Step 2387 | loss: 3.4307310581207275
Step 2387 | grad_norm: 3.2959883213043213
Step 2387 | learning_rate: 2.3378291586384075e-05
Step 2387 | epoch: 2.2996146435452793
Step 2388 | loss: 4.319120407104492
Step 2388 | grad_norm: 3.635580062866211
Step 2388 | learning_rate: 2.334617854849069e-05
Step 2388 | epoch: 2.300578034682081
Step 2389 | loss: 3.465754270553589
Step 2389 | grad_norm: 3.62086820602417
Step 2389 | learning_rate: 2.3314065510597304e-05
Step 2389 | epoch: 2.3015414258188827
Step 2390 | loss: 3.041983127593994
Step 2390 | grad_norm: 3.8022048473358154
Step 2390 | learning_rate: 2.3281952472703918e-05
Step 2390 | epoch: 2.302504816955684
Step 2391 | loss: 2.3978331089019775
Step 2391 | grad_norm: 2.923109292984009
Step 2391 | learning_rate: 2.3249839434810532e-05
Step 2391 | epoch: 2.3034682080924855
Step 2392 | loss: 3.375869035720825
Step 2392 | grad_norm: 3.220531463623047
Step 2392 | learning_rate: 2.321772639691715e-05
Step 2392 | epoch: 2.304431599229287
Step 2393 | loss: 3.5775909423828125
Step 2393 | grad_norm: 3.679351806640625
Step 2393 | learning_rate: 2.3185613359023764e-05
Step 2393 | epoch: 2.3053949903660884
Step 2394 | loss: 3.320610523223877
Step 2394 | grad_norm: 3.4727210998535156
Step 2394 | learning_rate: 2.315350032113038e-05
Step 2394 | epoch: 2.30635838150289
Step 2395 | loss: 2.8109209537506104
Step 2395 | grad_norm: 3.043290376663208
Step 2395 | learning_rate: 2.3121387283236996e-05
Step 2395 | epoch: 2.3073217726396917
Step 2396 | loss: 2.9885735511779785
Step 2396 | grad_norm: 3.2035071849823
Step 2396 | learning_rate: 2.3089274245343613e-05
Step 2396 | epoch: 2.3082851637764934
Step 2397 | loss: 3.0277984142303467
Step 2397 | grad_norm: 2.3569655418395996
Step 2397 | learning_rate: 2.3057161207450227e-05
Step 2397 | epoch: 2.3092485549132946
Step 2398 | loss: 4.078911781311035
Step 2398 | grad_norm: 2.8262836933135986
Step 2398 | learning_rate: 2.302504816955684e-05
Step 2398 | epoch: 2.3102119460500963
Step 2399 | loss: 3.7148220539093018
Step 2399 | grad_norm: 3.3096084594726562
Step 2399 | learning_rate: 2.2992935131663456e-05
Step 2399 | epoch: 2.311175337186898
Step 2400 | loss: 2.9658589363098145
Step 2400 | grad_norm: 2.4429209232330322
Step 2400 | learning_rate: 2.296082209377007e-05
Step 2400 | epoch: 2.3121387283236996
Step 2401 | loss: 3.1766293048858643
Step 2401 | grad_norm: 2.9795608520507812
Step 2401 | learning_rate: 2.2928709055876687e-05
Step 2401 | epoch: 2.313102119460501
Step 2402 | loss: 3.0039381980895996
Step 2402 | grad_norm: 2.8916985988616943
Step 2402 | learning_rate: 2.28965960179833e-05
Step 2402 | epoch: 2.3140655105973025
Step 2403 | loss: 3.6868579387664795
Step 2403 | grad_norm: 3.431380033493042
Step 2403 | learning_rate: 2.286448298008992e-05
Step 2403 | epoch: 2.315028901734104
Step 2404 | loss: 3.037976026535034
Step 2404 | grad_norm: 2.670438051223755
Step 2404 | learning_rate: 2.2832369942196533e-05
Step 2404 | epoch: 2.315992292870906
Step 2405 | loss: 3.2906076908111572
Step 2405 | grad_norm: 3.5446550846099854
Step 2405 | learning_rate: 2.280025690430315e-05
Step 2405 | epoch: 2.316955684007707
Step 2406 | loss: 2.9297118186950684
Step 2406 | grad_norm: 2.804687976837158
Step 2406 | learning_rate: 2.2768143866409765e-05
Step 2406 | epoch: 2.3179190751445087
Step 2407 | loss: 2.900723695755005
Step 2407 | grad_norm: 3.2344319820404053
Step 2407 | learning_rate: 2.2736030828516376e-05
Step 2407 | epoch: 2.3188824662813103
Step 2408 | loss: 3.0966458320617676
Step 2408 | grad_norm: 2.69805645942688
Step 2408 | learning_rate: 2.2703917790622994e-05
Step 2408 | epoch: 2.3198458574181116
Step 2409 | loss: 3.6124517917633057
Step 2409 | grad_norm: 2.853745698928833
Step 2409 | learning_rate: 2.2671804752729608e-05
Step 2409 | epoch: 2.320809248554913
Step 2410 | loss: 3.5385642051696777
Step 2410 | grad_norm: 3.0139670372009277
Step 2410 | learning_rate: 2.2639691714836225e-05
Step 2410 | epoch: 2.321772639691715
Step 2411 | loss: 3.931467294692993
Step 2411 | grad_norm: 2.4768245220184326
Step 2411 | learning_rate: 2.260757867694284e-05
Step 2411 | epoch: 2.3227360308285165
Step 2412 | loss: 2.7775306701660156
Step 2412 | grad_norm: 2.845538854598999
Step 2412 | learning_rate: 2.2575465639049457e-05
Step 2412 | epoch: 2.3236994219653178
Step 2413 | loss: 3.6917614936828613
Step 2413 | grad_norm: 3.4768118858337402
Step 2413 | learning_rate: 2.254335260115607e-05
Step 2413 | epoch: 2.3246628131021194
Step 2414 | loss: 3.4144132137298584
Step 2414 | grad_norm: 3.2565364837646484
Step 2414 | learning_rate: 2.2511239563262685e-05
Step 2414 | epoch: 2.325626204238921
Step 2415 | loss: 2.906336545944214
Step 2415 | grad_norm: 3.005016803741455
Step 2415 | learning_rate: 2.24791265253693e-05
Step 2415 | epoch: 2.3265895953757223
Step 2416 | loss: 3.987793207168579
Step 2416 | grad_norm: 3.5934653282165527
Step 2416 | learning_rate: 2.2447013487475914e-05
Step 2416 | epoch: 2.327552986512524
Step 2417 | loss: 3.10807204246521
Step 2417 | grad_norm: 2.4882705211639404
Step 2417 | learning_rate: 2.241490044958253e-05
Step 2417 | epoch: 2.3285163776493256
Step 2418 | loss: 4.2407073974609375
Step 2418 | grad_norm: 4.072305679321289
Step 2418 | learning_rate: 2.2382787411689146e-05
Step 2418 | epoch: 2.3294797687861273
Step 2419 | loss: 2.9402239322662354
Step 2419 | grad_norm: 2.5765604972839355
Step 2419 | learning_rate: 2.2350674373795763e-05
Step 2419 | epoch: 2.330443159922929
Step 2420 | loss: 2.3413853645324707
Step 2420 | grad_norm: 3.155212879180908
Step 2420 | learning_rate: 2.2318561335902377e-05
Step 2420 | epoch: 2.33140655105973
Step 2421 | loss: 2.5857291221618652
Step 2421 | grad_norm: 2.5102198123931885
Step 2421 | learning_rate: 2.2286448298008995e-05
Step 2421 | epoch: 2.332369942196532
Step 2422 | loss: 3.1966443061828613
Step 2422 | grad_norm: 3.1485226154327393
Step 2422 | learning_rate: 2.225433526011561e-05
Step 2422 | epoch: 2.3333333333333335
Step 2423 | loss: 2.98219633102417
Step 2423 | grad_norm: 2.7916669845581055
Step 2423 | learning_rate: 2.2222222222222223e-05
Step 2423 | epoch: 2.3342967244701347
Step 2424 | loss: 3.229499340057373
Step 2424 | grad_norm: 3.1581766605377197
Step 2424 | learning_rate: 2.2190109184328837e-05
Step 2424 | epoch: 2.3352601156069364
Step 2425 | loss: 2.726271629333496
Step 2425 | grad_norm: 2.410512924194336
Step 2425 | learning_rate: 2.215799614643545e-05
Step 2425 | epoch: 2.336223506743738
Step 2426 | loss: 2.9950833320617676
Step 2426 | grad_norm: 2.758527994155884
Step 2426 | learning_rate: 2.212588310854207e-05
Step 2426 | epoch: 2.3371868978805397
Step 2427 | loss: 3.3870701789855957
Step 2427 | grad_norm: 4.065280914306641
Step 2427 | learning_rate: 2.2093770070648683e-05
Step 2427 | epoch: 2.338150289017341
Step 2428 | loss: 3.3079192638397217
Step 2428 | grad_norm: 3.137599229812622
Step 2428 | learning_rate: 2.20616570327553e-05
Step 2428 | epoch: 2.3391136801541426
Step 2429 | loss: 3.3563497066497803
Step 2429 | grad_norm: 3.143160343170166
Step 2429 | learning_rate: 2.2029543994861915e-05
Step 2429 | epoch: 2.340077071290944
Step 2430 | loss: 3.5996034145355225
Step 2430 | grad_norm: 3.064424753189087
Step 2430 | learning_rate: 2.1997430956968533e-05
Step 2430 | epoch: 2.3410404624277454
Step 2431 | loss: 3.0644354820251465
Step 2431 | grad_norm: 3.5149881839752197
Step 2431 | learning_rate: 2.1965317919075147e-05
Step 2431 | epoch: 2.342003853564547
Step 2432 | loss: 3.4565658569335938
Step 2432 | grad_norm: 3.252368450164795
Step 2432 | learning_rate: 2.193320488118176e-05
Step 2432 | epoch: 2.3429672447013488
Step 2433 | loss: 3.187899351119995
Step 2433 | grad_norm: 3.6959288120269775
Step 2433 | learning_rate: 2.1901091843288375e-05
Step 2433 | epoch: 2.3439306358381504
Step 2434 | loss: 4.2122673988342285
Step 2434 | grad_norm: 3.731978178024292
Step 2434 | learning_rate: 2.186897880539499e-05
Step 2434 | epoch: 2.3448940269749516
Step 2435 | loss: 3.258329391479492
Step 2435 | grad_norm: 2.7156786918640137
Step 2435 | learning_rate: 2.1836865767501607e-05
Step 2435 | epoch: 2.3458574181117533
Step 2436 | loss: 2.8456766605377197
Step 2436 | grad_norm: 2.208336114883423
Step 2436 | learning_rate: 2.180475272960822e-05
Step 2436 | epoch: 2.346820809248555
Step 2437 | loss: 3.02479887008667
Step 2437 | grad_norm: 2.423469305038452
Step 2437 | learning_rate: 2.177263969171484e-05
Step 2437 | epoch: 2.3477842003853566
Step 2438 | loss: 3.0026602745056152
Step 2438 | grad_norm: 3.0645663738250732
Step 2438 | learning_rate: 2.1740526653821453e-05
Step 2438 | epoch: 2.348747591522158
Step 2439 | loss: 3.1386611461639404
Step 2439 | grad_norm: 2.96374773979187
Step 2439 | learning_rate: 2.1708413615928067e-05
Step 2439 | epoch: 2.3497109826589595
Step 2440 | loss: 3.947624683380127
Step 2440 | grad_norm: 3.289644956588745
Step 2440 | learning_rate: 2.1676300578034685e-05
Step 2440 | epoch: 2.350674373795761
Step 2441 | loss: 3.1344830989837646
Step 2441 | grad_norm: 3.293436050415039
Step 2441 | learning_rate: 2.16441875401413e-05
Step 2441 | epoch: 2.351637764932563
Step 2442 | loss: 3.1813225746154785
Step 2442 | grad_norm: 2.756542682647705
Step 2442 | learning_rate: 2.1612074502247913e-05
Step 2442 | epoch: 2.352601156069364
Step 2443 | loss: 3.045320749282837
Step 2443 | grad_norm: 2.799731969833374
Step 2443 | learning_rate: 2.1579961464354527e-05
Step 2443 | epoch: 2.3535645472061657
Step 2444 | loss: 3.021908760070801
Step 2444 | grad_norm: 2.631218194961548
Step 2444 | learning_rate: 2.1547848426461145e-05
Step 2444 | epoch: 2.3545279383429674
Step 2445 | loss: 2.8111205101013184
Step 2445 | grad_norm: 1.991836428642273
Step 2445 | learning_rate: 2.151573538856776e-05
Step 2445 | epoch: 2.3554913294797686
Step 2446 | loss: 2.796790838241577
Step 2446 | grad_norm: 2.598825693130493
Step 2446 | learning_rate: 2.1483622350674377e-05
Step 2446 | epoch: 2.3564547206165702
Step 2447 | loss: 3.3802876472473145
Step 2447 | grad_norm: 2.8241026401519775
Step 2447 | learning_rate: 2.145150931278099e-05
Step 2447 | epoch: 2.357418111753372
Step 2448 | loss: 3.1502277851104736
Step 2448 | grad_norm: 3.5287160873413086
Step 2448 | learning_rate: 2.1419396274887605e-05
Step 2448 | epoch: 2.3583815028901736
Step 2449 | loss: 3.3299899101257324
Step 2449 | grad_norm: 3.576664447784424
Step 2449 | learning_rate: 2.1387283236994223e-05
Step 2449 | epoch: 2.359344894026975
Step 2450 | loss: 3.3456947803497314
Step 2450 | grad_norm: 2.7141408920288086
Step 2450 | learning_rate: 2.1355170199100837e-05
Step 2450 | epoch: 2.3603082851637764
Step 2451 | loss: 3.753774881362915
Step 2451 | grad_norm: 2.270569086074829
Step 2451 | learning_rate: 2.132305716120745e-05
Step 2451 | epoch: 2.361271676300578
Step 2452 | loss: 2.854677200317383
Step 2452 | grad_norm: 2.4571645259857178
Step 2452 | learning_rate: 2.1290944123314065e-05
Step 2452 | epoch: 2.3622350674373798
Step 2453 | loss: 2.8258960247039795
Step 2453 | grad_norm: 3.406491994857788
Step 2453 | learning_rate: 2.1258831085420683e-05
Step 2453 | epoch: 2.363198458574181
Step 2454 | loss: 3.577293634414673
Step 2454 | grad_norm: 2.930356740951538
Step 2454 | learning_rate: 2.1226718047527297e-05
Step 2454 | epoch: 2.3641618497109826
Step 2455 | loss: 3.389981746673584
Step 2455 | grad_norm: 2.972891092300415
Step 2455 | learning_rate: 2.119460500963391e-05
Step 2455 | epoch: 2.3651252408477843
Step 2456 | loss: 4.067976474761963
Step 2456 | grad_norm: 3.058560848236084
Step 2456 | learning_rate: 2.116249197174053e-05
Step 2456 | epoch: 2.366088631984586
Step 2457 | loss: 3.1001734733581543
Step 2457 | grad_norm: 2.885075092315674
Step 2457 | learning_rate: 2.1130378933847143e-05
Step 2457 | epoch: 2.367052023121387
Step 2458 | loss: 3.5239319801330566
Step 2458 | grad_norm: 2.532663345336914
Step 2458 | learning_rate: 2.1098265895953757e-05
Step 2458 | epoch: 2.368015414258189
Step 2459 | loss: 3.3928301334381104
Step 2459 | grad_norm: 2.868757963180542
Step 2459 | learning_rate: 2.106615285806037e-05
Step 2459 | epoch: 2.3689788053949905
Step 2460 | loss: 3.658567190170288
Step 2460 | grad_norm: 2.706851005554199
Step 2460 | learning_rate: 2.103403982016699e-05
Step 2460 | epoch: 2.3699421965317917
Step 2461 | loss: 4.2121477127075195
Step 2461 | grad_norm: 3.9056808948516846
Step 2461 | learning_rate: 2.1001926782273603e-05
Step 2461 | epoch: 2.3709055876685934
Step 2462 | loss: 3.6892881393432617
Step 2462 | grad_norm: 2.60810923576355
Step 2462 | learning_rate: 2.096981374438022e-05
Step 2462 | epoch: 2.371868978805395
Step 2463 | loss: 3.6429169178009033
Step 2463 | grad_norm: 4.604927062988281
Step 2463 | learning_rate: 2.0937700706486835e-05
Step 2463 | epoch: 2.3728323699421967
Step 2464 | loss: 3.734314203262329
Step 2464 | grad_norm: 3.4706215858459473
Step 2464 | learning_rate: 2.090558766859345e-05
Step 2464 | epoch: 2.373795761078998
Step 2465 | loss: 3.1587305068969727
Step 2465 | grad_norm: 2.868699789047241
Step 2465 | learning_rate: 2.0873474630700067e-05
Step 2465 | epoch: 2.3747591522157996
Step 2466 | loss: 2.338975191116333
Step 2466 | grad_norm: 2.1919021606445312
Step 2466 | learning_rate: 2.084136159280668e-05
Step 2466 | epoch: 2.3757225433526012
Step 2467 | loss: 3.8849971294403076
Step 2467 | grad_norm: 2.7332005500793457
Step 2467 | learning_rate: 2.0809248554913295e-05
Step 2467 | epoch: 2.3766859344894025
Step 2468 | loss: 2.7780869007110596
Step 2468 | grad_norm: 2.9518399238586426
Step 2468 | learning_rate: 2.077713551701991e-05
Step 2468 | epoch: 2.377649325626204
Step 2469 | loss: 3.018415927886963
Step 2469 | grad_norm: 3.0784237384796143
Step 2469 | learning_rate: 2.0745022479126527e-05
Step 2469 | epoch: 2.378612716763006
Step 2470 | loss: 3.3826520442962646
Step 2470 | grad_norm: 3.4535443782806396
Step 2470 | learning_rate: 2.071290944123314e-05
Step 2470 | epoch: 2.3795761078998074
Step 2471 | loss: 3.1229333877563477
Step 2471 | grad_norm: 3.3278698921203613
Step 2471 | learning_rate: 2.068079640333976e-05
Step 2471 | epoch: 2.380539499036609
Step 2472 | loss: 3.0200035572052
Step 2472 | grad_norm: 3.1003360748291016
Step 2472 | learning_rate: 2.0648683365446373e-05
Step 2472 | epoch: 2.3815028901734103
Step 2473 | loss: 4.42396354675293
Step 2473 | grad_norm: 3.442584991455078
Step 2473 | learning_rate: 2.0616570327552987e-05
Step 2473 | epoch: 2.382466281310212
Step 2474 | loss: 4.115331649780273
Step 2474 | grad_norm: 3.510162353515625
Step 2474 | learning_rate: 2.0584457289659604e-05
Step 2474 | epoch: 2.3834296724470136
Step 2475 | loss: 3.0658249855041504
Step 2475 | grad_norm: 2.358102321624756
Step 2475 | learning_rate: 2.055234425176622e-05
Step 2475 | epoch: 2.384393063583815
Step 2476 | loss: 3.1715710163116455
Step 2476 | grad_norm: 3.101732015609741
Step 2476 | learning_rate: 2.0520231213872833e-05
Step 2476 | epoch: 2.3853564547206165
Step 2477 | loss: 2.826416254043579
Step 2477 | grad_norm: 3.6566998958587646
Step 2477 | learning_rate: 2.0488118175979447e-05
Step 2477 | epoch: 2.386319845857418
Step 2478 | loss: 3.4088239669799805
Step 2478 | grad_norm: 3.5815372467041016
Step 2478 | learning_rate: 2.0456005138086065e-05
Step 2478 | epoch: 2.38728323699422
Step 2479 | loss: 2.5283148288726807
Step 2479 | grad_norm: 3.119713306427002
Step 2479 | learning_rate: 2.042389210019268e-05
Step 2479 | epoch: 2.388246628131021
Step 2480 | loss: 3.3601748943328857
Step 2480 | grad_norm: 2.9874472618103027
Step 2480 | learning_rate: 2.0391779062299293e-05
Step 2480 | epoch: 2.3892100192678227
Step 2481 | loss: 3.4736106395721436
Step 2481 | grad_norm: 2.995426893234253
Step 2481 | learning_rate: 2.035966602440591e-05
Step 2481 | epoch: 2.3901734104046244
Step 2482 | loss: 2.8010590076446533
Step 2482 | grad_norm: 2.6034531593322754
Step 2482 | learning_rate: 2.0327552986512525e-05
Step 2482 | epoch: 2.3911368015414256
Step 2483 | loss: 3.319303274154663
Step 2483 | grad_norm: 4.020198345184326
Step 2483 | learning_rate: 2.0295439948619142e-05
Step 2483 | epoch: 2.3921001926782273
Step 2484 | loss: 2.991806745529175
Step 2484 | grad_norm: 2.9189136028289795
Step 2484 | learning_rate: 2.0263326910725756e-05
Step 2484 | epoch: 2.393063583815029
Step 2485 | loss: 2.765138626098633
Step 2485 | grad_norm: 2.597212553024292
Step 2485 | learning_rate: 2.023121387283237e-05
Step 2485 | epoch: 2.3940269749518306
Step 2486 | loss: 2.456909418106079
Step 2486 | grad_norm: 2.6610167026519775
Step 2486 | learning_rate: 2.0199100834938985e-05
Step 2486 | epoch: 2.394990366088632
Step 2487 | loss: 3.2671749591827393
Step 2487 | grad_norm: 2.480043888092041
Step 2487 | learning_rate: 2.0166987797045602e-05
Step 2487 | epoch: 2.3959537572254335
Step 2488 | loss: 3.7367913722991943
Step 2488 | grad_norm: 3.3427464962005615
Step 2488 | learning_rate: 2.0134874759152217e-05
Step 2488 | epoch: 2.396917148362235
Step 2489 | loss: 3.6379940509796143
Step 2489 | grad_norm: 3.1969456672668457
Step 2489 | learning_rate: 2.010276172125883e-05
Step 2489 | epoch: 2.397880539499037
Step 2490 | loss: 3.036360740661621
Step 2490 | grad_norm: 2.4031596183776855
Step 2490 | learning_rate: 2.007064868336545e-05
Step 2490 | epoch: 2.398843930635838
Step 2491 | loss: 2.722825050354004
Step 2491 | grad_norm: 2.7760353088378906
Step 2491 | learning_rate: 2.0038535645472063e-05
Step 2491 | epoch: 2.3998073217726397
Step 2492 | loss: 4.696118354797363
Step 2492 | grad_norm: 4.398471832275391
Step 2492 | learning_rate: 2.000642260757868e-05
Step 2492 | epoch: 2.4007707129094413
Step 2493 | loss: 3.033493757247925
Step 2493 | grad_norm: 2.6677398681640625
Step 2493 | learning_rate: 1.9974309569685294e-05
Step 2493 | epoch: 2.401734104046243
Step 2494 | loss: 3.3114867210388184
Step 2494 | grad_norm: 2.4713737964630127
Step 2494 | learning_rate: 1.994219653179191e-05
Step 2494 | epoch: 2.402697495183044
Step 2495 | loss: 3.416334867477417
Step 2495 | grad_norm: 2.878019332885742
Step 2495 | learning_rate: 1.9910083493898523e-05
Step 2495 | epoch: 2.403660886319846
Step 2496 | loss: 3.2528247833251953
Step 2496 | grad_norm: 2.646277666091919
Step 2496 | learning_rate: 1.9877970456005137e-05
Step 2496 | epoch: 2.4046242774566475
Step 2497 | loss: 3.885557174682617
Step 2497 | grad_norm: 3.408371686935425
Step 2497 | learning_rate: 1.9845857418111754e-05
Step 2497 | epoch: 2.4055876685934487
Step 2498 | loss: 3.724195718765259
Step 2498 | grad_norm: 3.491114854812622
Step 2498 | learning_rate: 1.981374438021837e-05
Step 2498 | epoch: 2.4065510597302504
Step 2499 | loss: 3.1507019996643066
Step 2499 | grad_norm: 3.204146146774292
Step 2499 | learning_rate: 1.9781631342324986e-05
Step 2499 | epoch: 2.407514450867052
Step 2500 | loss: 2.945734977722168
Step 2500 | grad_norm: 2.6724865436553955
Step 2500 | learning_rate: 1.97495183044316e-05
Step 2500 | epoch: 2.4084778420038537
Step 2501 | loss: 3.1634864807128906
Step 2501 | grad_norm: 3.1242501735687256
Step 2501 | learning_rate: 1.9717405266538218e-05
Step 2501 | epoch: 2.409441233140655
Step 2502 | loss: 3.185849905014038
Step 2502 | grad_norm: 2.8619840145111084
Step 2502 | learning_rate: 1.968529222864483e-05
Step 2502 | epoch: 2.4104046242774566
Step 2503 | loss: 3.266554355621338
Step 2503 | grad_norm: 2.4796624183654785
Step 2503 | learning_rate: 1.9653179190751446e-05
Step 2503 | epoch: 2.4113680154142583
Step 2504 | loss: 3.6108551025390625
Step 2504 | grad_norm: 3.1444077491760254
Step 2504 | learning_rate: 1.962106615285806e-05
Step 2504 | epoch: 2.4123314065510595
Step 2505 | loss: 3.7457709312438965
Step 2505 | grad_norm: 4.323587894439697
Step 2505 | learning_rate: 1.9588953114964675e-05
Step 2505 | epoch: 2.413294797687861
Step 2506 | loss: 3.937077045440674
Step 2506 | grad_norm: 3.2309114933013916
Step 2506 | learning_rate: 1.9556840077071292e-05
Step 2506 | epoch: 2.414258188824663
Step 2507 | loss: 3.5760178565979004
Step 2507 | grad_norm: 4.250737190246582
Step 2507 | learning_rate: 1.9524727039177907e-05
Step 2507 | epoch: 2.4152215799614645
Step 2508 | loss: 2.942808151245117
Step 2508 | grad_norm: 2.8542368412017822
Step 2508 | learning_rate: 1.9492614001284524e-05
Step 2508 | epoch: 2.416184971098266
Step 2509 | loss: 3.100172996520996
Step 2509 | grad_norm: 2.9854419231414795
Step 2509 | learning_rate: 1.9460500963391138e-05
Step 2509 | epoch: 2.4171483622350673
Step 2510 | loss: 2.8143067359924316
Step 2510 | grad_norm: 2.510068655014038
Step 2510 | learning_rate: 1.9428387925497752e-05
Step 2510 | epoch: 2.418111753371869
Step 2511 | loss: 3.4265973567962646
Step 2511 | grad_norm: 3.4687631130218506
Step 2511 | learning_rate: 1.9396274887604367e-05
Step 2511 | epoch: 2.4190751445086707
Step 2512 | loss: 4.2343926429748535
Step 2512 | grad_norm: 3.8512866497039795
Step 2512 | learning_rate: 1.936416184971098e-05
Step 2512 | epoch: 2.420038535645472
Step 2513 | loss: 2.993943929672241
Step 2513 | grad_norm: 3.1001243591308594
Step 2513 | learning_rate: 1.93320488118176e-05
Step 2513 | epoch: 2.4210019267822736
Step 2514 | loss: 3.4146499633789062
Step 2514 | grad_norm: 3.5019776821136475
Step 2514 | learning_rate: 1.9299935773924213e-05
Step 2514 | epoch: 2.421965317919075
Step 2515 | loss: 2.6754112243652344
Step 2515 | grad_norm: 2.949371814727783
Step 2515 | learning_rate: 1.926782273603083e-05
Step 2515 | epoch: 2.422928709055877
Step 2516 | loss: 2.400073289871216
Step 2516 | grad_norm: 2.2878782749176025
Step 2516 | learning_rate: 1.9235709698137444e-05
Step 2516 | epoch: 2.423892100192678
Step 2517 | loss: 2.992753744125366
Step 2517 | grad_norm: 4.094299793243408
Step 2517 | learning_rate: 1.9203596660244062e-05
Step 2517 | epoch: 2.4248554913294798
Step 2518 | loss: 2.7433969974517822
Step 2518 | grad_norm: 3.5182347297668457
Step 2518 | learning_rate: 1.9171483622350676e-05
Step 2518 | epoch: 2.4258188824662814
Step 2519 | loss: 2.6711266040802
Step 2519 | grad_norm: 2.6746530532836914
Step 2519 | learning_rate: 1.913937058445729e-05
Step 2519 | epoch: 2.4267822736030826
Step 2520 | loss: 3.1824145317077637
Step 2520 | grad_norm: 3.4255127906799316
Step 2520 | learning_rate: 1.9107257546563905e-05
Step 2520 | epoch: 2.4277456647398843
Step 2521 | loss: 2.201812982559204
Step 2521 | grad_norm: 2.564822196960449
Step 2521 | learning_rate: 1.907514450867052e-05
Step 2521 | epoch: 2.428709055876686
Step 2522 | loss: 4.252840518951416
Step 2522 | grad_norm: 4.0023417472839355
Step 2522 | learning_rate: 1.9043031470777136e-05
Step 2522 | epoch: 2.4296724470134876
Step 2523 | loss: 3.4594476222991943
Step 2523 | grad_norm: 2.664611339569092
Step 2523 | learning_rate: 1.901091843288375e-05
Step 2523 | epoch: 2.430635838150289
Step 2524 | loss: 3.2635498046875
Step 2524 | grad_norm: 3.5288023948669434
Step 2524 | learning_rate: 1.8978805394990368e-05
Step 2524 | epoch: 2.4315992292870905
Step 2525 | loss: 3.3093907833099365
Step 2525 | grad_norm: 3.4351892471313477
Step 2525 | learning_rate: 1.8946692357096982e-05
Step 2525 | epoch: 2.432562620423892
Step 2526 | loss: 3.718613862991333
Step 2526 | grad_norm: 3.1545262336730957
Step 2526 | learning_rate: 1.89145793192036e-05
Step 2526 | epoch: 2.433526011560694
Step 2527 | loss: 2.924880027770996
Step 2527 | grad_norm: 2.6271989345550537
Step 2527 | learning_rate: 1.8882466281310214e-05
Step 2527 | epoch: 2.434489402697495
Step 2528 | loss: 2.899689197540283
Step 2528 | grad_norm: 3.922924041748047
Step 2528 | learning_rate: 1.8850353243416828e-05
Step 2528 | epoch: 2.4354527938342967
Step 2529 | loss: 3.5215554237365723
Step 2529 | grad_norm: 3.2656335830688477
Step 2529 | learning_rate: 1.8818240205523442e-05
Step 2529 | epoch: 2.4364161849710984
Step 2530 | loss: 4.005178928375244
Step 2530 | grad_norm: 3.725184202194214
Step 2530 | learning_rate: 1.8786127167630057e-05
Step 2530 | epoch: 2.4373795761079
Step 2531 | loss: 3.191805839538574
Step 2531 | grad_norm: 3.1385574340820312
Step 2531 | learning_rate: 1.8754014129736674e-05
Step 2531 | epoch: 2.4383429672447012
Step 2532 | loss: 2.993708610534668
Step 2532 | grad_norm: 2.9568660259246826
Step 2532 | learning_rate: 1.872190109184329e-05
Step 2532 | epoch: 2.439306358381503
Step 2533 | loss: 3.4595391750335693
Step 2533 | grad_norm: 2.8813748359680176
Step 2533 | learning_rate: 1.8689788053949906e-05
Step 2533 | epoch: 2.4402697495183046
Step 2534 | loss: 3.8898468017578125
Step 2534 | grad_norm: 3.048949718475342
Step 2534 | learning_rate: 1.865767501605652e-05
Step 2534 | epoch: 2.4412331406551058
Step 2535 | loss: 3.490028142929077
Step 2535 | grad_norm: 4.70913553237915
Step 2535 | learning_rate: 1.8625561978163138e-05
Step 2535 | epoch: 2.4421965317919074
Step 2536 | loss: 3.534207582473755
Step 2536 | grad_norm: 2.5036492347717285
Step 2536 | learning_rate: 1.8593448940269752e-05
Step 2536 | epoch: 2.443159922928709
Step 2537 | loss: 3.011493444442749
Step 2537 | grad_norm: 2.642345666885376
Step 2537 | learning_rate: 1.8561335902376366e-05
Step 2537 | epoch: 2.4441233140655108
Step 2538 | loss: 2.655872344970703
Step 2538 | grad_norm: 2.8051490783691406
Step 2538 | learning_rate: 1.852922286448298e-05
Step 2538 | epoch: 2.445086705202312
Step 2539 | loss: 3.8263628482818604
Step 2539 | grad_norm: 3.098690986633301
Step 2539 | learning_rate: 1.8497109826589594e-05
Step 2539 | epoch: 2.4460500963391136
Step 2540 | loss: 3.4264094829559326
Step 2540 | grad_norm: 3.043203592300415
Step 2540 | learning_rate: 1.8464996788696212e-05
Step 2540 | epoch: 2.4470134874759153
Step 2541 | loss: 3.5747506618499756
Step 2541 | grad_norm: 3.188890218734741
Step 2541 | learning_rate: 1.8432883750802826e-05
Step 2541 | epoch: 2.447976878612717
Step 2542 | loss: 3.9319326877593994
Step 2542 | grad_norm: 3.0213451385498047
Step 2542 | learning_rate: 1.8400770712909444e-05
Step 2542 | epoch: 2.448940269749518
Step 2543 | loss: 3.0502068996429443
Step 2543 | grad_norm: 2.246520757675171
Step 2543 | learning_rate: 1.8368657675016058e-05
Step 2543 | epoch: 2.44990366088632
Step 2544 | loss: 3.1328322887420654
Step 2544 | grad_norm: 3.3777425289154053
Step 2544 | learning_rate: 1.8336544637122676e-05
Step 2544 | epoch: 2.4508670520231215
Step 2545 | loss: 3.1454038619995117
Step 2545 | grad_norm: 2.75431752204895
Step 2545 | learning_rate: 1.830443159922929e-05
Step 2545 | epoch: 2.451830443159923
Step 2546 | loss: 3.5701744556427
Step 2546 | grad_norm: 3.025325298309326
Step 2546 | learning_rate: 1.82723185613359e-05
Step 2546 | epoch: 2.4527938342967244
Step 2547 | loss: 3.6678013801574707
Step 2547 | grad_norm: 4.550148963928223
Step 2547 | learning_rate: 1.8240205523442518e-05
Step 2547 | epoch: 2.453757225433526
Step 2548 | loss: 3.4369986057281494
Step 2548 | grad_norm: 4.2544474601745605
Step 2548 | learning_rate: 1.8208092485549132e-05
Step 2548 | epoch: 2.4547206165703277
Step 2549 | loss: 3.2741496562957764
Step 2549 | grad_norm: 2.957589626312256
Step 2549 | learning_rate: 1.817597944765575e-05
Step 2549 | epoch: 2.455684007707129
Step 2550 | loss: 2.737788200378418
Step 2550 | grad_norm: 3.3350000381469727
Step 2550 | learning_rate: 1.8143866409762364e-05
Step 2550 | epoch: 2.4566473988439306
Step 2551 | loss: 3.5616233348846436
Step 2551 | grad_norm: 2.549579620361328
Step 2551 | learning_rate: 1.811175337186898e-05
Step 2551 | epoch: 2.4576107899807322
Step 2552 | loss: 3.956756591796875
Step 2552 | grad_norm: 3.459129571914673
Step 2552 | learning_rate: 1.8079640333975596e-05
Step 2552 | epoch: 2.458574181117534
Step 2553 | loss: 3.3454411029815674
Step 2553 | grad_norm: 2.7208304405212402
Step 2553 | learning_rate: 1.804752729608221e-05
Step 2553 | epoch: 2.459537572254335
Step 2554 | loss: 3.977936029434204
Step 2554 | grad_norm: 3.743626356124878
Step 2554 | learning_rate: 1.8015414258188824e-05
Step 2554 | epoch: 2.4605009633911368
Step 2555 | loss: 4.142860412597656
Step 2555 | grad_norm: 4.250840663909912
Step 2555 | learning_rate: 1.798330122029544e-05
Step 2555 | epoch: 2.4614643545279384
Step 2556 | loss: 2.937997817993164
Step 2556 | grad_norm: 2.9184255599975586
Step 2556 | learning_rate: 1.7951188182402056e-05
Step 2556 | epoch: 2.4624277456647397
Step 2557 | loss: 3.354339122772217
Step 2557 | grad_norm: 3.5403172969818115
Step 2557 | learning_rate: 1.791907514450867e-05
Step 2557 | epoch: 2.4633911368015413
Step 2558 | loss: 3.043740749359131
Step 2558 | grad_norm: 3.3821842670440674
Step 2558 | learning_rate: 1.7886962106615288e-05
Step 2558 | epoch: 2.464354527938343
Step 2559 | loss: 3.0585198402404785
Step 2559 | grad_norm: 2.7929019927978516
Step 2559 | learning_rate: 1.7854849068721902e-05
Step 2559 | epoch: 2.4653179190751446
Step 2560 | loss: 3.4413700103759766
Step 2560 | grad_norm: 3.086918830871582
Step 2560 | learning_rate: 1.782273603082852e-05
Step 2560 | epoch: 2.4662813102119463
Step 2561 | loss: 3.2118008136749268
Step 2561 | grad_norm: 3.429731607437134
Step 2561 | learning_rate: 1.7790622992935134e-05
Step 2561 | epoch: 2.4672447013487475
Step 2562 | loss: 3.7406418323516846
Step 2562 | grad_norm: 3.3679566383361816
Step 2562 | learning_rate: 1.7758509955041748e-05
Step 2562 | epoch: 2.468208092485549
Step 2563 | loss: 2.628434658050537
Step 2563 | grad_norm: 2.8096601963043213
Step 2563 | learning_rate: 1.7726396917148362e-05
Step 2563 | epoch: 2.469171483622351
Step 2564 | loss: 3.664736747741699
Step 2564 | grad_norm: 3.4462263584136963
Step 2564 | learning_rate: 1.7694283879254976e-05
Step 2564 | epoch: 2.470134874759152
Step 2565 | loss: 3.4001901149749756
Step 2565 | grad_norm: 3.170081377029419
Step 2565 | learning_rate: 1.7662170841361594e-05
Step 2565 | epoch: 2.4710982658959537
Step 2566 | loss: 2.995135545730591
Step 2566 | grad_norm: 2.9684059619903564
Step 2566 | learning_rate: 1.7630057803468208e-05
Step 2566 | epoch: 2.4720616570327554
Step 2567 | loss: 3.2425928115844727
Step 2567 | grad_norm: 3.0015339851379395
Step 2567 | learning_rate: 1.7597944765574826e-05
Step 2567 | epoch: 2.473025048169557
Step 2568 | loss: 3.522108316421509
Step 2568 | grad_norm: 3.3207247257232666
Step 2568 | learning_rate: 1.756583172768144e-05
Step 2568 | epoch: 2.4739884393063583
Step 2569 | loss: 2.2625784873962402
Step 2569 | grad_norm: 2.656141996383667
Step 2569 | learning_rate: 1.7533718689788054e-05
Step 2569 | epoch: 2.47495183044316
Step 2570 | loss: 3.6195056438446045
Step 2570 | grad_norm: 3.100919246673584
Step 2570 | learning_rate: 1.750160565189467e-05
Step 2570 | epoch: 2.4759152215799616
Step 2571 | loss: 2.572429656982422
Step 2571 | grad_norm: 2.8237228393554688
Step 2571 | learning_rate: 1.7469492614001286e-05
Step 2571 | epoch: 2.476878612716763
Step 2572 | loss: 3.0241546630859375
Step 2572 | grad_norm: 3.0192668437957764
Step 2572 | learning_rate: 1.74373795761079e-05
Step 2572 | epoch: 2.4778420038535645
Step 2573 | loss: 3.0508925914764404
Step 2573 | grad_norm: 3.695202350616455
Step 2573 | learning_rate: 1.7405266538214514e-05
Step 2573 | epoch: 2.478805394990366
Step 2574 | loss: 2.5057053565979004
Step 2574 | grad_norm: 2.6643505096435547
Step 2574 | learning_rate: 1.737315350032113e-05
Step 2574 | epoch: 2.479768786127168
Step 2575 | loss: 2.902244806289673
Step 2575 | grad_norm: 3.4099011421203613
Step 2575 | learning_rate: 1.7341040462427746e-05
Step 2575 | epoch: 2.480732177263969
Step 2576 | loss: 3.0185341835021973
Step 2576 | grad_norm: 3.390683650970459
Step 2576 | learning_rate: 1.7308927424534363e-05
Step 2576 | epoch: 2.4816955684007707
Step 2577 | loss: 3.242493152618408
Step 2577 | grad_norm: 2.7418270111083984
Step 2577 | learning_rate: 1.7276814386640978e-05
Step 2577 | epoch: 2.4826589595375723
Step 2578 | loss: 3.2596800327301025
Step 2578 | grad_norm: 3.2665672302246094
Step 2578 | learning_rate: 1.7244701348747592e-05
Step 2578 | epoch: 2.483622350674374
Step 2579 | loss: 3.601644277572632
Step 2579 | grad_norm: 5.003283977508545
Step 2579 | learning_rate: 1.721258831085421e-05
Step 2579 | epoch: 2.484585741811175
Step 2580 | loss: 2.9622514247894287
Step 2580 | grad_norm: 2.49859881401062
Step 2580 | learning_rate: 1.7180475272960824e-05
Step 2580 | epoch: 2.485549132947977
Step 2581 | loss: 3.448728322982788
Step 2581 | grad_norm: 3.6029484272003174
Step 2581 | learning_rate: 1.7148362235067438e-05
Step 2581 | epoch: 2.4865125240847785
Step 2582 | loss: 3.5127055644989014
Step 2582 | grad_norm: 2.9058685302734375
Step 2582 | learning_rate: 1.7116249197174052e-05
Step 2582 | epoch: 2.48747591522158
Step 2583 | loss: 3.058669090270996
Step 2583 | grad_norm: 2.515547037124634
Step 2583 | learning_rate: 1.708413615928067e-05
Step 2583 | epoch: 2.4884393063583814
Step 2584 | loss: 3.6788365840911865
Step 2584 | grad_norm: 3.5661768913269043
Step 2584 | learning_rate: 1.7052023121387284e-05
Step 2584 | epoch: 2.489402697495183
Step 2585 | loss: 3.608767032623291
Step 2585 | grad_norm: 3.041489839553833
Step 2585 | learning_rate: 1.70199100834939e-05
Step 2585 | epoch: 2.4903660886319847
Step 2586 | loss: 3.734711170196533
Step 2586 | grad_norm: 3.004387617111206
Step 2586 | learning_rate: 1.6987797045600515e-05
Step 2586 | epoch: 2.491329479768786
Step 2587 | loss: 3.6899213790893555
Step 2587 | grad_norm: 3.3367700576782227
Step 2587 | learning_rate: 1.695568400770713e-05
Step 2587 | epoch: 2.4922928709055876
Step 2588 | loss: 3.3028695583343506
Step 2588 | grad_norm: 3.2292494773864746
Step 2588 | learning_rate: 1.6923570969813747e-05
Step 2588 | epoch: 2.4932562620423893
Step 2589 | loss: 3.0332369804382324
Step 2589 | grad_norm: 2.6906614303588867
Step 2589 | learning_rate: 1.689145793192036e-05
Step 2589 | epoch: 2.494219653179191
Step 2590 | loss: 4.488962650299072
Step 2590 | grad_norm: 3.670124053955078
Step 2590 | learning_rate: 1.6859344894026976e-05
Step 2590 | epoch: 2.495183044315992
Step 2591 | loss: 2.9755635261535645
Step 2591 | grad_norm: 3.4726948738098145
Step 2591 | learning_rate: 1.682723185613359e-05
Step 2591 | epoch: 2.496146435452794
Step 2592 | loss: 3.4280974864959717
Step 2592 | grad_norm: 2.5606205463409424
Step 2592 | learning_rate: 1.6795118818240207e-05
Step 2592 | epoch: 2.4971098265895955
Step 2593 | loss: 3.064239740371704
Step 2593 | grad_norm: 2.5863401889801025
Step 2593 | learning_rate: 1.676300578034682e-05
Step 2593 | epoch: 2.4980732177263967
Step 2594 | loss: 3.1637845039367676
Step 2594 | grad_norm: 3.066655397415161
Step 2594 | learning_rate: 1.6730892742453436e-05
Step 2594 | epoch: 2.4990366088631983
Step 2595 | loss: 3.2288923263549805
Step 2595 | grad_norm: 2.9986836910247803
Step 2595 | learning_rate: 1.6698779704560053e-05
Step 2595 | epoch: 2.5
Step 2596 | loss: 3.6744818687438965
Step 2596 | grad_norm: 4.198798656463623
Step 2596 | learning_rate: 1.6666666666666667e-05
Step 2596 | epoch: 2.5009633911368017
Step 2597 | loss: 3.282524347305298
Step 2597 | grad_norm: 3.022406578063965
Step 2597 | learning_rate: 1.6634553628773285e-05
Step 2597 | epoch: 2.5019267822736033
Step 2598 | loss: 3.0926647186279297
Step 2598 | grad_norm: 4.1873321533203125
Step 2598 | learning_rate: 1.6602440590879896e-05
Step 2598 | epoch: 2.5028901734104045
Step 2599 | loss: 4.236041069030762
Step 2599 | grad_norm: 4.092750549316406
Step 2599 | learning_rate: 1.6570327552986513e-05
Step 2599 | epoch: 2.503853564547206
Step 2600 | loss: 3.8395509719848633
Step 2600 | grad_norm: 3.0784807205200195
Step 2600 | learning_rate: 1.6538214515093128e-05
Step 2600 | epoch: 2.504816955684008
Step 2601 | loss: 3.168104887008667
Step 2601 | grad_norm: 2.6548874378204346
Step 2601 | learning_rate: 1.6506101477199745e-05
Step 2601 | epoch: 2.505780346820809
Step 2602 | loss: 2.9021642208099365
Step 2602 | grad_norm: 2.6773905754089355
Step 2602 | learning_rate: 1.647398843930636e-05
Step 2602 | epoch: 2.5067437379576107
Step 2603 | loss: 3.59263014793396
Step 2603 | grad_norm: 3.15557599067688
Step 2603 | learning_rate: 1.6441875401412974e-05
Step 2603 | epoch: 2.5077071290944124
Step 2604 | loss: 2.4714808464050293
Step 2604 | grad_norm: 2.6871049404144287
Step 2604 | learning_rate: 1.640976236351959e-05
Step 2604 | epoch: 2.508670520231214
Step 2605 | loss: 3.8559505939483643
Step 2605 | grad_norm: 2.799402952194214
Step 2605 | learning_rate: 1.6377649325626205e-05
Step 2605 | epoch: 2.5096339113680153
Step 2606 | loss: 3.21408748626709
Step 2606 | grad_norm: 3.6946001052856445
Step 2606 | learning_rate: 1.634553628773282e-05
Step 2606 | epoch: 2.510597302504817
Step 2607 | loss: 3.1259877681732178
Step 2607 | grad_norm: 2.583923101425171
Step 2607 | learning_rate: 1.6313423249839434e-05
Step 2607 | epoch: 2.5115606936416186
Step 2608 | loss: 3.683234214782715
Step 2608 | grad_norm: 4.20728874206543
Step 2608 | learning_rate: 1.628131021194605e-05
Step 2608 | epoch: 2.51252408477842
Step 2609 | loss: 3.540739059448242
Step 2609 | grad_norm: 4.4396586418151855
Step 2609 | learning_rate: 1.6249197174052665e-05
Step 2609 | epoch: 2.5134874759152215
Step 2610 | loss: 3.774177312850952
Step 2610 | grad_norm: 3.6841089725494385
Step 2610 | learning_rate: 1.621708413615928e-05
Step 2610 | epoch: 2.514450867052023
Step 2611 | loss: 3.2084248065948486
Step 2611 | grad_norm: 2.6529135704040527
Step 2611 | learning_rate: 1.6184971098265897e-05
Step 2611 | epoch: 2.515414258188825
Step 2612 | loss: 3.3201546669006348
Step 2612 | grad_norm: 3.381355047225952
Step 2612 | learning_rate: 1.615285806037251e-05
Step 2612 | epoch: 2.5163776493256265
Step 2613 | loss: 3.376958131790161
Step 2613 | grad_norm: 3.169628620147705
Step 2613 | learning_rate: 1.612074502247913e-05
Step 2613 | epoch: 2.5173410404624277
Step 2614 | loss: 3.372765064239502
Step 2614 | grad_norm: 3.1862030029296875
Step 2614 | learning_rate: 1.6088631984585743e-05
Step 2614 | epoch: 2.5183044315992293
Step 2615 | loss: 3.9243621826171875
Step 2615 | grad_norm: 10.021783828735352
Step 2615 | learning_rate: 1.6056518946692357e-05
Step 2615 | epoch: 2.5192678227360306
Step 2616 | loss: 3.3799431324005127
Step 2616 | grad_norm: 3.7555360794067383
Step 2616 | learning_rate: 1.602440590879897e-05
Step 2616 | epoch: 2.520231213872832
Step 2617 | loss: 2.7940828800201416
Step 2617 | grad_norm: 4.763221740722656
Step 2617 | learning_rate: 1.599229287090559e-05
Step 2617 | epoch: 2.521194605009634
Step 2618 | loss: 3.4700827598571777
Step 2618 | grad_norm: 3.121760606765747
Step 2618 | learning_rate: 1.5960179833012203e-05
Step 2618 | epoch: 2.5221579961464355
Step 2619 | loss: 2.993685245513916
Step 2619 | grad_norm: 3.091569423675537
Step 2619 | learning_rate: 1.5928066795118817e-05
Step 2619 | epoch: 2.523121387283237
Step 2620 | loss: 2.694211959838867
Step 2620 | grad_norm: 2.438445806503296
Step 2620 | learning_rate: 1.5895953757225435e-05
Step 2620 | epoch: 2.5240847784200384
Step 2621 | loss: 3.10713267326355
Step 2621 | grad_norm: 2.6819441318511963
Step 2621 | learning_rate: 1.586384071933205e-05
Step 2621 | epoch: 2.52504816955684
Step 2622 | loss: 3.236525535583496
Step 2622 | grad_norm: 2.5929393768310547
Step 2622 | learning_rate: 1.5831727681438667e-05
Step 2622 | epoch: 2.5260115606936417
Step 2623 | loss: 3.4049127101898193
Step 2623 | grad_norm: 3.211989641189575
Step 2623 | learning_rate: 1.579961464354528e-05
Step 2623 | epoch: 2.526974951830443
Step 2624 | loss: 3.721989154815674
Step 2624 | grad_norm: 3.1992835998535156
Step 2624 | learning_rate: 1.5767501605651895e-05
Step 2624 | epoch: 2.5279383429672446
Step 2625 | loss: 2.5722696781158447
Step 2625 | grad_norm: 2.7608213424682617
Step 2625 | learning_rate: 1.573538856775851e-05
Step 2625 | epoch: 2.5289017341040463
Step 2626 | loss: 3.4628496170043945
Step 2626 | grad_norm: 2.9297173023223877
Step 2626 | learning_rate: 1.5703275529865124e-05
Step 2626 | epoch: 2.529865125240848
Step 2627 | loss: 2.4829914569854736
Step 2627 | grad_norm: 2.275646686553955
Step 2627 | learning_rate: 1.567116249197174e-05
Step 2627 | epoch: 2.530828516377649
Step 2628 | loss: 3.3071606159210205
Step 2628 | grad_norm: 2.7239832878112793
Step 2628 | learning_rate: 1.5639049454078355e-05
Step 2628 | epoch: 2.531791907514451
Step 2629 | loss: 3.2140278816223145
Step 2629 | grad_norm: 4.9130048751831055
Step 2629 | learning_rate: 1.5606936416184973e-05
Step 2629 | epoch: 2.5327552986512525
Step 2630 | loss: 4.405842304229736
Step 2630 | grad_norm: 3.728834629058838
Step 2630 | learning_rate: 1.5574823378291587e-05
Step 2630 | epoch: 2.5337186897880537
Step 2631 | loss: 2.98893666267395
Step 2631 | grad_norm: 3.4114885330200195
Step 2631 | learning_rate: 1.5542710340398205e-05
Step 2631 | epoch: 2.5346820809248554
Step 2632 | loss: 3.534454345703125
Step 2632 | grad_norm: 3.546236991882324
Step 2632 | learning_rate: 1.551059730250482e-05
Step 2632 | epoch: 2.535645472061657
Step 2633 | loss: 3.177313804626465
Step 2633 | grad_norm: 2.8132848739624023
Step 2633 | learning_rate: 1.5478484264611433e-05
Step 2633 | epoch: 2.5366088631984587
Step 2634 | loss: 3.916485071182251
Step 2634 | grad_norm: 3.5423808097839355
Step 2634 | learning_rate: 1.5446371226718047e-05
Step 2634 | epoch: 2.5375722543352603
Step 2635 | loss: 3.169114589691162
Step 2635 | grad_norm: 2.4354190826416016
Step 2635 | learning_rate: 1.541425818882466e-05
Step 2635 | epoch: 2.5385356454720616
Step 2636 | loss: 3.963117837905884
Step 2636 | grad_norm: 3.3701260089874268
Step 2636 | learning_rate: 1.538214515093128e-05
Step 2636 | epoch: 2.5394990366088632
Step 2637 | loss: 2.789595365524292
Step 2637 | grad_norm: 2.2783877849578857
Step 2637 | learning_rate: 1.5350032113037893e-05
Step 2637 | epoch: 2.540462427745665
Step 2638 | loss: 3.584932565689087
Step 2638 | grad_norm: 2.324002265930176
Step 2638 | learning_rate: 1.531791907514451e-05
Step 2638 | epoch: 2.541425818882466
Step 2639 | loss: 2.764711380004883
Step 2639 | grad_norm: 3.1377668380737305
Step 2639 | learning_rate: 1.5285806037251125e-05
Step 2639 | epoch: 2.5423892100192678
Step 2640 | loss: 3.027355194091797
Step 2640 | grad_norm: 2.567972421646118
Step 2640 | learning_rate: 1.525369299935774e-05
Step 2640 | epoch: 2.5433526011560694
Step 2641 | loss: 3.3678359985351562
Step 2641 | grad_norm: 2.838900566101074
Step 2641 | learning_rate: 1.5221579961464355e-05
Step 2641 | epoch: 2.544315992292871
Step 2642 | loss: 2.881517171859741
Step 2642 | grad_norm: 2.658466339111328
Step 2642 | learning_rate: 1.5189466923570973e-05
Step 2642 | epoch: 2.5452793834296723
Step 2643 | loss: 2.497124433517456
Step 2643 | grad_norm: 2.685718297958374
Step 2643 | learning_rate: 1.5157353885677587e-05
Step 2643 | epoch: 2.546242774566474
Step 2644 | loss: 3.983090877532959
Step 2644 | grad_norm: 3.7442662715911865
Step 2644 | learning_rate: 1.51252408477842e-05
Step 2644 | epoch: 2.5472061657032756
Step 2645 | loss: 2.7522029876708984
Step 2645 | grad_norm: 2.7907049655914307
Step 2645 | learning_rate: 1.5093127809890817e-05
Step 2645 | epoch: 2.548169556840077
Step 2646 | loss: 3.272277355194092
Step 2646 | grad_norm: 3.031198501586914
Step 2646 | learning_rate: 1.5061014771997431e-05
Step 2646 | epoch: 2.5491329479768785
Step 2647 | loss: 3.484818696975708
Step 2647 | grad_norm: 3.1882858276367188
Step 2647 | learning_rate: 1.5028901734104049e-05
Step 2647 | epoch: 2.55009633911368
Step 2648 | loss: 3.5332038402557373
Step 2648 | grad_norm: 2.8646085262298584
Step 2648 | learning_rate: 1.4996788696210661e-05
Step 2648 | epoch: 2.551059730250482
Step 2649 | loss: 3.4875988960266113
Step 2649 | grad_norm: 2.9161908626556396
Step 2649 | learning_rate: 1.4964675658317279e-05
Step 2649 | epoch: 2.5520231213872835
Step 2650 | loss: 4.079429626464844
Step 2650 | grad_norm: 4.028158664703369
Step 2650 | learning_rate: 1.4932562620423893e-05
Step 2650 | epoch: 2.5529865125240847
Step 2651 | loss: 2.8854751586914062
Step 2651 | grad_norm: 3.2852251529693604
Step 2651 | learning_rate: 1.4900449582530507e-05
Step 2651 | epoch: 2.5539499036608864
Step 2652 | loss: 3.0900094509124756
Step 2652 | grad_norm: 2.6073596477508545
Step 2652 | learning_rate: 1.4868336544637123e-05
Step 2652 | epoch: 2.5549132947976876
Step 2653 | loss: 2.8981406688690186
Step 2653 | grad_norm: 3.1091103553771973
Step 2653 | learning_rate: 1.4836223506743737e-05
Step 2653 | epoch: 2.5558766859344892
Step 2654 | loss: 3.6433870792388916
Step 2654 | grad_norm: 3.0019679069519043
Step 2654 | learning_rate: 1.4804110468850355e-05
Step 2654 | epoch: 2.556840077071291
Step 2655 | loss: 3.588322877883911
Step 2655 | grad_norm: 2.744650363922119
Step 2655 | learning_rate: 1.4771997430956969e-05
Step 2655 | epoch: 2.5578034682080926
Step 2656 | loss: 2.500880002975464
Step 2656 | grad_norm: 2.6337039470672607
Step 2656 | learning_rate: 1.4739884393063585e-05
Step 2656 | epoch: 2.5587668593448942
Step 2657 | loss: 3.2898662090301514
Step 2657 | grad_norm: 3.066068410873413
Step 2657 | learning_rate: 1.4707771355170199e-05
Step 2657 | epoch: 2.5597302504816954
Step 2658 | loss: 2.9360640048980713
Step 2658 | grad_norm: 3.3075852394104004
Step 2658 | learning_rate: 1.4675658317276817e-05
Step 2658 | epoch: 2.560693641618497
Step 2659 | loss: 3.2580885887145996
Step 2659 | grad_norm: 3.6934263706207275
Step 2659 | learning_rate: 1.464354527938343e-05
Step 2659 | epoch: 2.5616570327552988
Step 2660 | loss: 3.500807523727417
Step 2660 | grad_norm: 3.654223918914795
Step 2660 | learning_rate: 1.4611432241490045e-05
Step 2660 | epoch: 2.5626204238921
Step 2661 | loss: 3.019991636276245
Step 2661 | grad_norm: 3.0932817459106445
Step 2661 | learning_rate: 1.457931920359666e-05
Step 2661 | epoch: 2.5635838150289016
Step 2662 | loss: 2.8380789756774902
Step 2662 | grad_norm: 3.5764963626861572
Step 2662 | learning_rate: 1.4547206165703275e-05
Step 2662 | epoch: 2.5645472061657033
Step 2663 | loss: 3.277949810028076
Step 2663 | grad_norm: 3.55926513671875
Step 2663 | learning_rate: 1.4515093127809893e-05
Step 2663 | epoch: 2.565510597302505
Step 2664 | loss: 3.9071691036224365
Step 2664 | grad_norm: 3.440692901611328
Step 2664 | learning_rate: 1.4482980089916507e-05
Step 2664 | epoch: 2.5664739884393066
Step 2665 | loss: 2.9249284267425537
Step 2665 | grad_norm: 2.2951109409332275
Step 2665 | learning_rate: 1.4450867052023123e-05
Step 2665 | epoch: 2.567437379576108
Step 2666 | loss: 2.8780813217163086
Step 2666 | grad_norm: 3.0368831157684326
Step 2666 | learning_rate: 1.4418754014129737e-05
Step 2666 | epoch: 2.5684007707129095
Step 2667 | loss: 3.7453227043151855
Step 2667 | grad_norm: 3.488314151763916
Step 2667 | learning_rate: 1.4386640976236351e-05
Step 2667 | epoch: 2.5693641618497107
Step 2668 | loss: 3.2925596237182617
Step 2668 | grad_norm: 3.318890333175659
Step 2668 | learning_rate: 1.4354527938342969e-05
Step 2668 | epoch: 2.5703275529865124
Step 2669 | loss: 2.7817935943603516
Step 2669 | grad_norm: 2.680760383605957
Step 2669 | learning_rate: 1.4322414900449583e-05
Step 2669 | epoch: 2.571290944123314
Step 2670 | loss: 2.725741147994995
Step 2670 | grad_norm: 3.0207929611206055
Step 2670 | learning_rate: 1.4290301862556199e-05
Step 2670 | epoch: 2.5722543352601157
Step 2671 | loss: 3.2814037799835205
Step 2671 | grad_norm: 2.9593892097473145
Step 2671 | learning_rate: 1.4258188824662813e-05
Step 2671 | epoch: 2.5732177263969174
Step 2672 | loss: 3.5549814701080322
Step 2672 | grad_norm: 3.1620965003967285
Step 2672 | learning_rate: 1.422607578676943e-05
Step 2672 | epoch: 2.5741811175337186
Step 2673 | loss: 2.959303140640259
Step 2673 | grad_norm: 3.2286641597747803
Step 2673 | learning_rate: 1.4193962748876045e-05
Step 2673 | epoch: 2.5751445086705202
Step 2674 | loss: 3.1753830909729004
Step 2674 | grad_norm: 5.0056867599487305
Step 2674 | learning_rate: 1.416184971098266e-05
Step 2674 | epoch: 2.576107899807322
Step 2675 | loss: 3.2771427631378174
Step 2675 | grad_norm: 2.6481852531433105
Step 2675 | learning_rate: 1.4129736673089275e-05
Step 2675 | epoch: 2.577071290944123
Step 2676 | loss: 3.604832649230957
Step 2676 | grad_norm: 2.9132027626037598
Step 2676 | learning_rate: 1.4097623635195889e-05
Step 2676 | epoch: 2.578034682080925
Step 2677 | loss: 3.4415555000305176
Step 2677 | grad_norm: 3.387028455734253
Step 2677 | learning_rate: 1.4065510597302506e-05
Step 2677 | epoch: 2.5789980732177264
Step 2678 | loss: 3.0715243816375732
Step 2678 | grad_norm: 2.9835238456726074
Step 2678 | learning_rate: 1.403339755940912e-05
Step 2678 | epoch: 2.579961464354528
Step 2679 | loss: 2.8604631423950195
Step 2679 | grad_norm: 3.925601005554199
Step 2679 | learning_rate: 1.4001284521515737e-05
Step 2679 | epoch: 2.5809248554913293
Step 2680 | loss: 3.39375901222229
Step 2680 | grad_norm: 3.2579469680786133
Step 2680 | learning_rate: 1.396917148362235e-05
Step 2680 | epoch: 2.581888246628131
Step 2681 | loss: 2.697575330734253
Step 2681 | grad_norm: 2.6781229972839355
Step 2681 | learning_rate: 1.3937058445728968e-05
Step 2681 | epoch: 2.5828516377649327
Step 2682 | loss: 3.5063254833221436
Step 2682 | grad_norm: 3.899494171142578
Step 2682 | learning_rate: 1.3904945407835582e-05
Step 2682 | epoch: 2.583815028901734
Step 2683 | loss: 3.1699397563934326
Step 2683 | grad_norm: 2.6741385459899902
Step 2683 | learning_rate: 1.3872832369942197e-05
Step 2683 | epoch: 2.5847784200385355
Step 2684 | loss: 3.2827839851379395
Step 2684 | grad_norm: 3.6331701278686523
Step 2684 | learning_rate: 1.3840719332048813e-05
Step 2684 | epoch: 2.585741811175337
Step 2685 | loss: 3.7378478050231934
Step 2685 | grad_norm: 3.097818613052368
Step 2685 | learning_rate: 1.3808606294155427e-05
Step 2685 | epoch: 2.586705202312139
Step 2686 | loss: 3.1010515689849854
Step 2686 | grad_norm: 3.052243947982788
Step 2686 | learning_rate: 1.3776493256262044e-05
Step 2686 | epoch: 2.5876685934489405
Step 2687 | loss: 3.1209216117858887
Step 2687 | grad_norm: 2.8746156692504883
Step 2687 | learning_rate: 1.3744380218368658e-05
Step 2687 | epoch: 2.5886319845857417
Step 2688 | loss: 3.1026220321655273
Step 2688 | grad_norm: 2.8801417350769043
Step 2688 | learning_rate: 1.3712267180475274e-05
Step 2688 | epoch: 2.5895953757225434
Step 2689 | loss: 2.524536371231079
Step 2689 | grad_norm: 2.6130661964416504
Step 2689 | learning_rate: 1.3680154142581889e-05
Step 2689 | epoch: 2.590558766859345
Step 2690 | loss: 2.8502817153930664
Step 2690 | grad_norm: 3.012298107147217
Step 2690 | learning_rate: 1.3648041104688506e-05
Step 2690 | epoch: 2.5915221579961463
Step 2691 | loss: 3.455947160720825
Step 2691 | grad_norm: 2.964484214782715
Step 2691 | learning_rate: 1.3615928066795119e-05
Step 2691 | epoch: 2.592485549132948
Step 2692 | loss: 2.963217258453369
Step 2692 | grad_norm: 2.625450849533081
Step 2692 | learning_rate: 1.3583815028901733e-05
Step 2692 | epoch: 2.5934489402697496
Step 2693 | loss: 3.3978216648101807
Step 2693 | grad_norm: 3.2555885314941406
Step 2693 | learning_rate: 1.355170199100835e-05
Step 2693 | epoch: 2.5944123314065513
Step 2694 | loss: 3.288586378097534
Step 2694 | grad_norm: 3.290755033493042
Step 2694 | learning_rate: 1.3519588953114965e-05
Step 2694 | epoch: 2.5953757225433525
Step 2695 | loss: 3.624150514602661
Step 2695 | grad_norm: 3.2021894454956055
Step 2695 | learning_rate: 1.348747591522158e-05
Step 2695 | epoch: 2.596339113680154
Step 2696 | loss: 3.55497670173645
Step 2696 | grad_norm: 3.261927366256714
Step 2696 | learning_rate: 1.3455362877328195e-05
Step 2696 | epoch: 2.597302504816956
Step 2697 | loss: 3.875743865966797
Step 2697 | grad_norm: 3.215144634246826
Step 2697 | learning_rate: 1.3423249839434812e-05
Step 2697 | epoch: 2.598265895953757
Step 2698 | loss: 2.6785600185394287
Step 2698 | grad_norm: 2.5730440616607666
Step 2698 | learning_rate: 1.3391136801541426e-05
Step 2698 | epoch: 2.5992292870905587
Step 2699 | loss: 2.9820311069488525
Step 2699 | grad_norm: 2.4599854946136475
Step 2699 | learning_rate: 1.3359023763648042e-05
Step 2699 | epoch: 2.6001926782273603
Step 2700 | loss: 3.354292869567871
Step 2700 | grad_norm: 3.2256875038146973
Step 2700 | learning_rate: 1.3326910725754656e-05
Step 2700 | epoch: 2.601156069364162
Step 2701 | loss: 3.6386559009552
Step 2701 | grad_norm: 3.568763017654419
Step 2701 | learning_rate: 1.329479768786127e-05
Step 2701 | epoch: 2.6021194605009637
Step 2702 | loss: 3.102217197418213
Step 2702 | grad_norm: 3.8028197288513184
Step 2702 | learning_rate: 1.3262684649967888e-05
Step 2702 | epoch: 2.603082851637765
Step 2703 | loss: 3.0293145179748535
Step 2703 | grad_norm: 3.6369686126708984
Step 2703 | learning_rate: 1.3230571612074502e-05
Step 2703 | epoch: 2.6040462427745665
Step 2704 | loss: 3.7189316749572754
Step 2704 | grad_norm: 2.4851224422454834
Step 2704 | learning_rate: 1.3198458574181118e-05
Step 2704 | epoch: 2.6050096339113678
Step 2705 | loss: 2.9965219497680664
Step 2705 | grad_norm: 3.138669490814209
Step 2705 | learning_rate: 1.3166345536287732e-05
Step 2705 | epoch: 2.6059730250481694
Step 2706 | loss: 3.6094508171081543
Step 2706 | grad_norm: 3.8393821716308594
Step 2706 | learning_rate: 1.313423249839435e-05
Step 2706 | epoch: 2.606936416184971
Step 2707 | loss: 3.4757559299468994
Step 2707 | grad_norm: 3.0274999141693115
Step 2707 | learning_rate: 1.3102119460500964e-05
Step 2707 | epoch: 2.6078998073217727
Step 2708 | loss: 3.1370859146118164
Step 2708 | grad_norm: 3.4923770427703857
Step 2708 | learning_rate: 1.3070006422607578e-05
Step 2708 | epoch: 2.6088631984585744
Step 2709 | loss: 3.174825429916382
Step 2709 | grad_norm: 2.8191311359405518
Step 2709 | learning_rate: 1.3037893384714194e-05
Step 2709 | epoch: 2.6098265895953756
Step 2710 | loss: 2.7760074138641357
Step 2710 | grad_norm: 2.5865864753723145
Step 2710 | learning_rate: 1.3005780346820809e-05
Step 2710 | epoch: 2.6107899807321773
Step 2711 | loss: 3.3354289531707764
Step 2711 | grad_norm: 3.98353910446167
Step 2711 | learning_rate: 1.2973667308927426e-05
Step 2711 | epoch: 2.611753371868979
Step 2712 | loss: 3.7226288318634033
Step 2712 | grad_norm: 3.6613805294036865
Step 2712 | learning_rate: 1.294155427103404e-05
Step 2712 | epoch: 2.61271676300578
Step 2713 | loss: 2.6016595363616943
Step 2713 | grad_norm: 2.235830307006836
Step 2713 | learning_rate: 1.2909441233140656e-05
Step 2713 | epoch: 2.613680154142582
Step 2714 | loss: 3.7771618366241455
Step 2714 | grad_norm: 3.098440408706665
Step 2714 | learning_rate: 1.287732819524727e-05
Step 2714 | epoch: 2.6146435452793835
Step 2715 | loss: 3.3998382091522217
Step 2715 | grad_norm: 3.944349527359009
Step 2715 | learning_rate: 1.2845215157353888e-05
Step 2715 | epoch: 2.615606936416185
Step 2716 | loss: 4.056024074554443
Step 2716 | grad_norm: 3.4919121265411377
Step 2716 | learning_rate: 1.2813102119460502e-05
Step 2716 | epoch: 2.6165703275529864
Step 2717 | loss: 3.8860809803009033
Step 2717 | grad_norm: 3.015366315841675
Step 2717 | learning_rate: 1.2780989081567116e-05
Step 2717 | epoch: 2.617533718689788
Step 2718 | loss: 3.0892443656921387
Step 2718 | grad_norm: 3.729114532470703
Step 2718 | learning_rate: 1.2748876043673732e-05
Step 2718 | epoch: 2.6184971098265897
Step 2719 | loss: 3.021193027496338
Step 2719 | grad_norm: 3.062596559524536
Step 2719 | learning_rate: 1.2716763005780346e-05
Step 2719 | epoch: 2.619460500963391
Step 2720 | loss: 3.6597342491149902
Step 2720 | grad_norm: 3.091196298599243
Step 2720 | learning_rate: 1.2684649967886964e-05
Step 2720 | epoch: 2.6204238921001926
Step 2721 | loss: 3.7888197898864746
Step 2721 | grad_norm: 4.257356643676758
Step 2721 | learning_rate: 1.2652536929993578e-05
Step 2721 | epoch: 2.621387283236994
Step 2722 | loss: 2.9550845623016357
Step 2722 | grad_norm: 2.6477673053741455
Step 2722 | learning_rate: 1.2620423892100194e-05
Step 2722 | epoch: 2.622350674373796
Step 2723 | loss: 3.7700798511505127
Step 2723 | grad_norm: 2.7872426509857178
Step 2723 | learning_rate: 1.2588310854206808e-05
Step 2723 | epoch: 2.6233140655105975
Step 2724 | loss: 3.2644784450531006
Step 2724 | grad_norm: 2.900146722793579
Step 2724 | learning_rate: 1.2556197816313422e-05
Step 2724 | epoch: 2.6242774566473988
Step 2725 | loss: 3.685699462890625
Step 2725 | grad_norm: 3.988586187362671
Step 2725 | learning_rate: 1.252408477842004e-05
Step 2725 | epoch: 2.6252408477842004
Step 2726 | loss: 2.47837495803833
Step 2726 | grad_norm: 3.023517370223999
Step 2726 | learning_rate: 1.2491971740526654e-05
Step 2726 | epoch: 2.626204238921002
Step 2727 | loss: 3.3202993869781494
Step 2727 | grad_norm: 2.9388375282287598
Step 2727 | learning_rate: 1.245985870263327e-05
Step 2727 | epoch: 2.6271676300578033
Step 2728 | loss: 3.2656679153442383
Step 2728 | grad_norm: 3.2351996898651123
Step 2728 | learning_rate: 1.2427745664739884e-05
Step 2728 | epoch: 2.628131021194605
Step 2729 | loss: 3.1817750930786133
Step 2729 | grad_norm: 3.2551193237304688
Step 2729 | learning_rate: 1.23956326268465e-05
Step 2729 | epoch: 2.6290944123314066
Step 2730 | loss: 3.237525701522827
Step 2730 | grad_norm: 2.759223461151123
Step 2730 | learning_rate: 1.2363519588953116e-05
Step 2730 | epoch: 2.6300578034682083
Step 2731 | loss: 3.256471872329712
Step 2731 | grad_norm: 2.9808871746063232
Step 2731 | learning_rate: 1.233140655105973e-05
Step 2731 | epoch: 2.6310211946050095
Step 2732 | loss: 3.8464996814727783
Step 2732 | grad_norm: 2.662965774536133
Step 2732 | learning_rate: 1.2299293513166346e-05
Step 2732 | epoch: 2.631984585741811
Step 2733 | loss: 3.1388938426971436
Step 2733 | grad_norm: 4.232155799865723
Step 2733 | learning_rate: 1.2267180475272962e-05
Step 2733 | epoch: 2.632947976878613
Step 2734 | loss: 3.875408411026001
Step 2734 | grad_norm: 3.5284154415130615
Step 2734 | learning_rate: 1.2235067437379578e-05
Step 2734 | epoch: 2.633911368015414
Step 2735 | loss: 3.4799954891204834
Step 2735 | grad_norm: 2.9628782272338867
Step 2735 | learning_rate: 1.2202954399486192e-05
Step 2735 | epoch: 2.6348747591522157
Step 2736 | loss: 3.1548848152160645
Step 2736 | grad_norm: 2.762542247772217
Step 2736 | learning_rate: 1.2170841361592806e-05
Step 2736 | epoch: 2.6358381502890174
Step 2737 | loss: 2.8396904468536377
Step 2737 | grad_norm: 2.594520092010498
Step 2737 | learning_rate: 1.2138728323699422e-05
Step 2737 | epoch: 2.636801541425819
Step 2738 | loss: 2.519367218017578
Step 2738 | grad_norm: 2.3180434703826904
Step 2738 | learning_rate: 1.2106615285806038e-05
Step 2738 | epoch: 2.6377649325626207
Step 2739 | loss: 3.3221497535705566
Step 2739 | grad_norm: 3.5482051372528076
Step 2739 | learning_rate: 1.2074502247912652e-05
Step 2739 | epoch: 2.638728323699422
Step 2740 | loss: 3.063159942626953
Step 2740 | grad_norm: 3.700291633605957
Step 2740 | learning_rate: 1.2042389210019268e-05
Step 2740 | epoch: 2.6396917148362236
Step 2741 | loss: 3.1659603118896484
Step 2741 | grad_norm: 2.8197152614593506
Step 2741 | learning_rate: 1.2010276172125884e-05
Step 2741 | epoch: 2.6406551059730248
Step 2742 | loss: 3.328397035598755
Step 2742 | grad_norm: 2.7895193099975586
Step 2742 | learning_rate: 1.19781631342325e-05
Step 2742 | epoch: 2.6416184971098264
Step 2743 | loss: 4.057857990264893
Step 2743 | grad_norm: 4.224606990814209
Step 2743 | learning_rate: 1.1946050096339114e-05
Step 2743 | epoch: 2.642581888246628
Step 2744 | loss: 2.3235268592834473
Step 2744 | grad_norm: 2.841719627380371
Step 2744 | learning_rate: 1.191393705844573e-05
Step 2744 | epoch: 2.6435452793834298
Step 2745 | loss: 4.145429611206055
Step 2745 | grad_norm: 4.044343948364258
Step 2745 | learning_rate: 1.1881824020552344e-05
Step 2745 | epoch: 2.6445086705202314
Step 2746 | loss: 3.0300850868225098
Step 2746 | grad_norm: 3.1944596767425537
Step 2746 | learning_rate: 1.184971098265896e-05
Step 2746 | epoch: 2.6454720616570326
Step 2747 | loss: 3.191164970397949
Step 2747 | grad_norm: 2.7075979709625244
Step 2747 | learning_rate: 1.1817597944765576e-05
Step 2747 | epoch: 2.6464354527938343
Step 2748 | loss: 2.913188934326172
Step 2748 | grad_norm: 3.0169131755828857
Step 2748 | learning_rate: 1.178548490687219e-05
Step 2748 | epoch: 2.647398843930636
Step 2749 | loss: 3.4712181091308594
Step 2749 | grad_norm: 2.771951913833618
Step 2749 | learning_rate: 1.1753371868978806e-05
Step 2749 | epoch: 2.648362235067437
Step 2750 | loss: 3.033362627029419
Step 2750 | grad_norm: 2.2770373821258545
Step 2750 | learning_rate: 1.1721258831085422e-05
Step 2750 | epoch: 2.649325626204239
Step 2751 | loss: 3.4478228092193604
Step 2751 | grad_norm: 3.092151641845703
Step 2751 | learning_rate: 1.1689145793192038e-05
Step 2751 | epoch: 2.6502890173410405
Step 2752 | loss: 3.2845749855041504
Step 2752 | grad_norm: 3.065749406814575
Step 2752 | learning_rate: 1.1657032755298652e-05
Step 2752 | epoch: 2.651252408477842
Step 2753 | loss: 3.102903366088867
Step 2753 | grad_norm: 2.9717869758605957
Step 2753 | learning_rate: 1.1624919717405266e-05
Step 2753 | epoch: 2.652215799614644
Step 2754 | loss: 2.9424378871917725
Step 2754 | grad_norm: 2.751652479171753
Step 2754 | learning_rate: 1.1592806679511882e-05
Step 2754 | epoch: 2.653179190751445
Step 2755 | loss: 3.323352813720703
Step 2755 | grad_norm: 2.8649888038635254
Step 2755 | learning_rate: 1.1560693641618498e-05
Step 2755 | epoch: 2.6541425818882467
Step 2756 | loss: 3.468865394592285
Step 2756 | grad_norm: 3.9795501232147217
Step 2756 | learning_rate: 1.1528580603725114e-05
Step 2756 | epoch: 2.655105973025048
Step 2757 | loss: 2.9365365505218506
Step 2757 | grad_norm: 3.0712146759033203
Step 2757 | learning_rate: 1.1496467565831728e-05
Step 2757 | epoch: 2.6560693641618496
Step 2758 | loss: 3.1946661472320557
Step 2758 | grad_norm: 2.9096786975860596
Step 2758 | learning_rate: 1.1464354527938344e-05
Step 2758 | epoch: 2.6570327552986512
Step 2759 | loss: 3.1124091148376465
Step 2759 | grad_norm: 3.576706886291504
Step 2759 | learning_rate: 1.143224149004496e-05
Step 2759 | epoch: 2.657996146435453
Step 2760 | loss: 3.4905757904052734
Step 2760 | grad_norm: 2.9826250076293945
Step 2760 | learning_rate: 1.1400128452151575e-05
Step 2760 | epoch: 2.6589595375722546
Step 2761 | loss: 2.6176559925079346
Step 2761 | grad_norm: 2.8796651363372803
Step 2761 | learning_rate: 1.1368015414258188e-05
Step 2761 | epoch: 2.659922928709056
Step 2762 | loss: 3.345381021499634
Step 2762 | grad_norm: 3.191391944885254
Step 2762 | learning_rate: 1.1335902376364804e-05
Step 2762 | epoch: 2.6608863198458574
Step 2763 | loss: 4.02121639251709
Step 2763 | grad_norm: 3.4613208770751953
Step 2763 | learning_rate: 1.130378933847142e-05
Step 2763 | epoch: 2.661849710982659
Step 2764 | loss: 3.6940083503723145
Step 2764 | grad_norm: 3.2588253021240234
Step 2764 | learning_rate: 1.1271676300578036e-05
Step 2764 | epoch: 2.6628131021194603
Step 2765 | loss: 3.147263765335083
Step 2765 | grad_norm: 2.785578489303589
Step 2765 | learning_rate: 1.123956326268465e-05
Step 2765 | epoch: 2.663776493256262
Step 2766 | loss: 3.846808910369873
Step 2766 | grad_norm: 3.4022960662841797
Step 2766 | learning_rate: 1.1207450224791266e-05
Step 2766 | epoch: 2.6647398843930636
Step 2767 | loss: 2.8456525802612305
Step 2767 | grad_norm: 2.595054864883423
Step 2767 | learning_rate: 1.1175337186897882e-05
Step 2767 | epoch: 2.6657032755298653
Step 2768 | loss: 3.949233055114746
Step 2768 | grad_norm: 3.208099365234375
Step 2768 | learning_rate: 1.1143224149004497e-05
Step 2768 | epoch: 2.6666666666666665
Step 2769 | loss: 3.0904242992401123
Step 2769 | grad_norm: 3.821474313735962
Step 2769 | learning_rate: 1.1111111111111112e-05
Step 2769 | epoch: 2.667630057803468
Step 2770 | loss: 3.1417629718780518
Step 2770 | grad_norm: 3.0281312465667725
Step 2770 | learning_rate: 1.1078998073217726e-05
Step 2770 | epoch: 2.66859344894027
Step 2771 | loss: 2.7529947757720947
Step 2771 | grad_norm: 2.7280218601226807
Step 2771 | learning_rate: 1.1046885035324342e-05
Step 2771 | epoch: 2.669556840077071
Step 2772 | loss: 2.4538283348083496
Step 2772 | grad_norm: 2.5485222339630127
Step 2772 | learning_rate: 1.1014771997430958e-05
Step 2772 | epoch: 2.6705202312138727
Step 2773 | loss: 2.9990665912628174
Step 2773 | grad_norm: 2.982910394668579
Step 2773 | learning_rate: 1.0982658959537573e-05
Step 2773 | epoch: 2.6714836223506744
Step 2774 | loss: 3.3844945430755615
Step 2774 | grad_norm: 2.9221906661987305
Step 2774 | learning_rate: 1.0950545921644188e-05
Step 2774 | epoch: 2.672447013487476
Step 2775 | loss: 3.4347970485687256
Step 2775 | grad_norm: 4.067512512207031
Step 2775 | learning_rate: 1.0918432883750804e-05
Step 2775 | epoch: 2.6734104046242777
Step 2776 | loss: 3.4682159423828125
Step 2776 | grad_norm: 4.571073055267334
Step 2776 | learning_rate: 1.088631984585742e-05
Step 2776 | epoch: 2.674373795761079
Step 2777 | loss: 3.9101459980010986
Step 2777 | grad_norm: 3.5321855545043945
Step 2777 | learning_rate: 1.0854206807964034e-05
Step 2777 | epoch: 2.6753371868978806
Step 2778 | loss: 3.1475830078125
Step 2778 | grad_norm: 2.8720967769622803
Step 2778 | learning_rate: 1.082209377007065e-05
Step 2778 | epoch: 2.6763005780346822
Step 2779 | loss: 3.8273911476135254
Step 2779 | grad_norm: 2.8234877586364746
Step 2779 | learning_rate: 1.0789980732177264e-05
Step 2779 | epoch: 2.6772639691714835
Step 2780 | loss: 3.205615520477295
Step 2780 | grad_norm: 3.082890748977661
Step 2780 | learning_rate: 1.075786769428388e-05
Step 2780 | epoch: 2.678227360308285
Step 2781 | loss: 3.372706413269043
Step 2781 | grad_norm: 2.4610040187835693
Step 2781 | learning_rate: 1.0725754656390495e-05
Step 2781 | epoch: 2.679190751445087
Step 2782 | loss: 3.3428688049316406
Step 2782 | grad_norm: 3.664980888366699
Step 2782 | learning_rate: 1.0693641618497111e-05
Step 2782 | epoch: 2.6801541425818884
Step 2783 | loss: 3.402576208114624
Step 2783 | grad_norm: 3.7769718170166016
Step 2783 | learning_rate: 1.0661528580603726e-05
Step 2783 | epoch: 2.6811175337186897
Step 2784 | loss: 4.074578762054443
Step 2784 | grad_norm: 4.280233860015869
Step 2784 | learning_rate: 1.0629415542710341e-05
Step 2784 | epoch: 2.6820809248554913
Step 2785 | loss: 3.6454761028289795
Step 2785 | grad_norm: 3.6068570613861084
Step 2785 | learning_rate: 1.0597302504816956e-05
Step 2785 | epoch: 2.683044315992293
Step 2786 | loss: 2.635711193084717
Step 2786 | grad_norm: 2.417222499847412
Step 2786 | learning_rate: 1.0565189466923571e-05
Step 2786 | epoch: 2.684007707129094
Step 2787 | loss: 3.065544843673706
Step 2787 | grad_norm: 2.3614413738250732
Step 2787 | learning_rate: 1.0533076429030186e-05
Step 2787 | epoch: 2.684971098265896
Step 2788 | loss: 3.2775185108184814
Step 2788 | grad_norm: 3.4768941402435303
Step 2788 | learning_rate: 1.0500963391136802e-05
Step 2788 | epoch: 2.6859344894026975
Step 2789 | loss: 3.1232998371124268
Step 2789 | grad_norm: 3.182596445083618
Step 2789 | learning_rate: 1.0468850353243417e-05
Step 2789 | epoch: 2.686897880539499
Step 2790 | loss: 3.343532085418701
Step 2790 | grad_norm: 2.7494568824768066
Step 2790 | learning_rate: 1.0436737315350033e-05
Step 2790 | epoch: 2.687861271676301
Step 2791 | loss: 3.323127508163452
Step 2791 | grad_norm: 3.4627606868743896
Step 2791 | learning_rate: 1.0404624277456647e-05
Step 2791 | epoch: 2.688824662813102
Step 2792 | loss: 3.563932180404663
Step 2792 | grad_norm: 3.2972652912139893
Step 2792 | learning_rate: 1.0372511239563263e-05
Step 2792 | epoch: 2.6897880539499037
Step 2793 | loss: 2.726501226425171
Step 2793 | grad_norm: 2.7984094619750977
Step 2793 | learning_rate: 1.034039820166988e-05
Step 2793 | epoch: 2.690751445086705
Step 2794 | loss: 3.4831578731536865
Step 2794 | grad_norm: 3.1309711933135986
Step 2794 | learning_rate: 1.0308285163776493e-05
Step 2794 | epoch: 2.6917148362235066
Step 2795 | loss: 3.1939074993133545
Step 2795 | grad_norm: 2.6738696098327637
Step 2795 | learning_rate: 1.027617212588311e-05
Step 2795 | epoch: 2.6926782273603083
Step 2796 | loss: 2.7724733352661133
Step 2796 | grad_norm: 2.816145420074463
Step 2796 | learning_rate: 1.0244059087989724e-05
Step 2796 | epoch: 2.69364161849711
Step 2797 | loss: 3.0014657974243164
Step 2797 | grad_norm: 2.569072961807251
Step 2797 | learning_rate: 1.021194605009634e-05
Step 2797 | epoch: 2.6946050096339116
Step 2798 | loss: 2.4014270305633545
Step 2798 | grad_norm: 2.907642126083374
Step 2798 | learning_rate: 1.0179833012202955e-05
Step 2798 | epoch: 2.695568400770713
Step 2799 | loss: 3.472621440887451
Step 2799 | grad_norm: 4.26271390914917
Step 2799 | learning_rate: 1.0147719974309571e-05
Step 2799 | epoch: 2.6965317919075145
Step 2800 | loss: 3.7256178855895996
Step 2800 | grad_norm: 3.556777000427246
Step 2800 | learning_rate: 1.0115606936416185e-05
Step 2800 | epoch: 2.697495183044316
Step 2801 | loss: 2.9008283615112305
Step 2801 | grad_norm: 2.8990278244018555
Step 2801 | learning_rate: 1.0083493898522801e-05
Step 2801 | epoch: 2.6984585741811173
Step 2802 | loss: 3.44770884513855
Step 2802 | grad_norm: 2.9443023204803467
Step 2802 | learning_rate: 1.0051380860629415e-05
Step 2802 | epoch: 2.699421965317919
Step 2803 | loss: 3.3517909049987793
Step 2803 | grad_norm: 3.244094133377075
Step 2803 | learning_rate: 1.0019267822736031e-05
Step 2803 | epoch: 2.7003853564547207
Step 2804 | loss: 4.21862268447876
Step 2804 | grad_norm: 3.1499409675598145
Step 2804 | learning_rate: 9.987154784842647e-06
Step 2804 | epoch: 2.7013487475915223
Step 2805 | loss: 3.515214443206787
Step 2805 | grad_norm: 4.898904800415039
Step 2805 | learning_rate: 9.955041746949261e-06
Step 2805 | epoch: 2.7023121387283235
Step 2806 | loss: 3.169250965118408
Step 2806 | grad_norm: 3.3195388317108154
Step 2806 | learning_rate: 9.922928709055877e-06
Step 2806 | epoch: 2.703275529865125
Step 2807 | loss: 4.168269634246826
Step 2807 | grad_norm: 3.462362766265869
Step 2807 | learning_rate: 9.890815671162493e-06
Step 2807 | epoch: 2.704238921001927
Step 2808 | loss: 3.448458671569824
Step 2808 | grad_norm: 2.970170259475708
Step 2808 | learning_rate: 9.858702633269109e-06
Step 2808 | epoch: 2.705202312138728
Step 2809 | loss: 3.0726711750030518
Step 2809 | grad_norm: 2.887540340423584
Step 2809 | learning_rate: 9.826589595375723e-06
Step 2809 | epoch: 2.7061657032755297
Step 2810 | loss: 2.957777976989746
Step 2810 | grad_norm: 2.964310646057129
Step 2810 | learning_rate: 9.794476557482337e-06
Step 2810 | epoch: 2.7071290944123314
Step 2811 | loss: 2.809936046600342
Step 2811 | grad_norm: 3.0057761669158936
Step 2811 | learning_rate: 9.762363519588953e-06
Step 2811 | epoch: 2.708092485549133
Step 2812 | loss: 3.524569511413574
Step 2812 | grad_norm: 3.178900957107544
Step 2812 | learning_rate: 9.730250481695569e-06
Step 2812 | epoch: 2.7090558766859347
Step 2813 | loss: 3.152757167816162
Step 2813 | grad_norm: 3.3983263969421387
Step 2813 | learning_rate: 9.698137443802183e-06
Step 2813 | epoch: 2.710019267822736
Step 2814 | loss: 4.5413055419921875
Step 2814 | grad_norm: 3.8220434188842773
Step 2814 | learning_rate: 9.6660244059088e-06
Step 2814 | epoch: 2.7109826589595376
Step 2815 | loss: 3.102294921875
Step 2815 | grad_norm: 4.493134021759033
Step 2815 | learning_rate: 9.633911368015415e-06
Step 2815 | epoch: 2.7119460500963393
Step 2816 | loss: 3.394639730453491
Step 2816 | grad_norm: 3.3977131843566895
Step 2816 | learning_rate: 9.601798330122031e-06
Step 2816 | epoch: 2.7129094412331405
Step 2817 | loss: 4.056806564331055
Step 2817 | grad_norm: 3.7200193405151367
Step 2817 | learning_rate: 9.569685292228645e-06
Step 2817 | epoch: 2.713872832369942
Step 2818 | loss: 3.7729475498199463
Step 2818 | grad_norm: 4.433237552642822
Step 2818 | learning_rate: 9.53757225433526e-06
Step 2818 | epoch: 2.714836223506744
Step 2819 | loss: 3.395777702331543
Step 2819 | grad_norm: 2.806917905807495
Step 2819 | learning_rate: 9.505459216441875e-06
Step 2819 | epoch: 2.7157996146435455
Step 2820 | loss: 3.018465995788574
Step 2820 | grad_norm: 3.3616976737976074
Step 2820 | learning_rate: 9.473346178548491e-06
Step 2820 | epoch: 2.7167630057803467
Step 2821 | loss: 2.9882924556732178
Step 2821 | grad_norm: 4.090648651123047
Step 2821 | learning_rate: 9.441233140655107e-06
Step 2821 | epoch: 2.7177263969171483
Step 2822 | loss: 2.713507890701294
Step 2822 | grad_norm: 2.859713315963745
Step 2822 | learning_rate: 9.409120102761721e-06
Step 2822 | epoch: 2.71868978805395
Step 2823 | loss: 3.4793035984039307
Step 2823 | grad_norm: 3.635545492172241
Step 2823 | learning_rate: 9.377007064868337e-06
Step 2823 | epoch: 2.7196531791907512
Step 2824 | loss: 4.146121025085449
Step 2824 | grad_norm: 3.473130464553833
Step 2824 | learning_rate: 9.344894026974953e-06
Step 2824 | epoch: 2.720616570327553
Step 2825 | loss: 3.648061752319336
Step 2825 | grad_norm: 4.336305141448975
Step 2825 | learning_rate: 9.312780989081569e-06
Step 2825 | epoch: 2.7215799614643545
Step 2826 | loss: 3.163583755493164
Step 2826 | grad_norm: 3.3731160163879395
Step 2826 | learning_rate: 9.280667951188183e-06
Step 2826 | epoch: 2.722543352601156
Step 2827 | loss: 3.2099759578704834
Step 2827 | grad_norm: 2.8126232624053955
Step 2827 | learning_rate: 9.248554913294797e-06
Step 2827 | epoch: 2.723506743737958
Step 2828 | loss: 3.2272698879241943
Step 2828 | grad_norm: 3.701688766479492
Step 2828 | learning_rate: 9.216441875401413e-06
Step 2828 | epoch: 2.724470134874759
Step 2829 | loss: 3.6692562103271484
Step 2829 | grad_norm: 2.811361789703369
Step 2829 | learning_rate: 9.184328837508029e-06
Step 2829 | epoch: 2.7254335260115607
Step 2830 | loss: 3.2985804080963135
Step 2830 | grad_norm: 3.3202736377716064
Step 2830 | learning_rate: 9.152215799614645e-06
Step 2830 | epoch: 2.726396917148362
Step 2831 | loss: 3.251795768737793
Step 2831 | grad_norm: 3.1972174644470215
Step 2831 | learning_rate: 9.120102761721259e-06
Step 2831 | epoch: 2.7273603082851636
Step 2832 | loss: 3.399899482727051
Step 2832 | grad_norm: 3.544280767440796
Step 2832 | learning_rate: 9.087989723827875e-06
Step 2832 | epoch: 2.7283236994219653
Step 2833 | loss: 3.0175626277923584
Step 2833 | grad_norm: 2.6608080863952637
Step 2833 | learning_rate: 9.05587668593449e-06
Step 2833 | epoch: 2.729287090558767
Step 2834 | loss: 2.688803195953369
Step 2834 | grad_norm: 3.6241295337677
Step 2834 | learning_rate: 9.023763648041105e-06
Step 2834 | epoch: 2.7302504816955686
Step 2835 | loss: 3.7534549236297607
Step 2835 | grad_norm: 3.0728344917297363
Step 2835 | learning_rate: 8.99165061014772e-06
Step 2835 | epoch: 2.73121387283237
Step 2836 | loss: 3.3782341480255127
Step 2836 | grad_norm: 3.7026164531707764
Step 2836 | learning_rate: 8.959537572254335e-06
Step 2836 | epoch: 2.7321772639691715
Step 2837 | loss: 3.3667383193969727
Step 2837 | grad_norm: 3.3393759727478027
Step 2837 | learning_rate: 8.927424534360951e-06
Step 2837 | epoch: 2.733140655105973
Step 2838 | loss: 3.292675733566284
Step 2838 | grad_norm: 3.2665858268737793
Step 2838 | learning_rate: 8.895311496467567e-06
Step 2838 | epoch: 2.7341040462427744
Step 2839 | loss: 3.0947265625
Step 2839 | grad_norm: 2.893507719039917
Step 2839 | learning_rate: 8.863198458574181e-06
Step 2839 | epoch: 2.735067437379576
Step 2840 | loss: 4.022987365722656
Step 2840 | grad_norm: 3.3012731075286865
Step 2840 | learning_rate: 8.831085420680797e-06
Step 2840 | epoch: 2.7360308285163777
Step 2841 | loss: 4.184196472167969
Step 2841 | grad_norm: 4.8096747398376465
Step 2841 | learning_rate: 8.798972382787413e-06
Step 2841 | epoch: 2.7369942196531793
Step 2842 | loss: 3.8231916427612305
Step 2842 | grad_norm: 3.1701748371124268
Step 2842 | learning_rate: 8.766859344894027e-06
Step 2842 | epoch: 2.7379576107899806
Step 2843 | loss: 2.536747932434082
Step 2843 | grad_norm: 2.8832738399505615
Step 2843 | learning_rate: 8.734746307000643e-06
Step 2843 | epoch: 2.7389210019267822
Step 2844 | loss: 3.5176680088043213
Step 2844 | grad_norm: 2.888899564743042
Step 2844 | learning_rate: 8.702633269107257e-06
Step 2844 | epoch: 2.739884393063584
Step 2845 | loss: 3.185433864593506
Step 2845 | grad_norm: 2.647021532058716
Step 2845 | learning_rate: 8.670520231213873e-06
Step 2845 | epoch: 2.740847784200385
Step 2846 | loss: 3.6216158866882324
Step 2846 | grad_norm: 3.429581880569458
Step 2846 | learning_rate: 8.638407193320489e-06
Step 2846 | epoch: 2.7418111753371868
Step 2847 | loss: 3.525887966156006
Step 2847 | grad_norm: 2.8479928970336914
Step 2847 | learning_rate: 8.606294155427105e-06
Step 2847 | epoch: 2.7427745664739884
Step 2848 | loss: 2.564783811569214
Step 2848 | grad_norm: 2.872797727584839
Step 2848 | learning_rate: 8.574181117533719e-06
Step 2848 | epoch: 2.74373795761079
Step 2849 | loss: 4.013174057006836
Step 2849 | grad_norm: 4.489817142486572
Step 2849 | learning_rate: 8.542068079640335e-06
Step 2849 | epoch: 2.7447013487475918
Step 2850 | loss: 3.586766004562378
Step 2850 | grad_norm: 3.689425230026245
Step 2850 | learning_rate: 8.50995504174695e-06
Step 2850 | epoch: 2.745664739884393
Step 2851 | loss: 4.054881572723389
Step 2851 | grad_norm: 3.5062246322631836
Step 2851 | learning_rate: 8.477842003853565e-06
Step 2851 | epoch: 2.7466281310211946
Step 2852 | loss: 2.9645540714263916
Step 2852 | grad_norm: 3.226915121078491
Step 2852 | learning_rate: 8.44572896596018e-06
Step 2852 | epoch: 2.7475915221579963
Step 2853 | loss: 3.0308358669281006
Step 2853 | grad_norm: 3.4792935848236084
Step 2853 | learning_rate: 8.413615928066795e-06
Step 2853 | epoch: 2.7485549132947975
Step 2854 | loss: 3.475450277328491
Step 2854 | grad_norm: 3.3114893436431885
Step 2854 | learning_rate: 8.38150289017341e-06
Step 2854 | epoch: 2.749518304431599
Step 2855 | loss: 2.7090628147125244
Step 2855 | grad_norm: 2.858957529067993
Step 2855 | learning_rate: 8.349389852280027e-06
Step 2855 | epoch: 2.750481695568401
Step 2856 | loss: 3.1808831691741943
Step 2856 | grad_norm: 2.711670398712158
Step 2856 | learning_rate: 8.317276814386643e-06
Step 2856 | epoch: 2.7514450867052025
Step 2857 | loss: 3.1473469734191895
Step 2857 | grad_norm: 3.009491443634033
Step 2857 | learning_rate: 8.285163776493257e-06
Step 2857 | epoch: 2.7524084778420037
Step 2858 | loss: 3.0910158157348633
Step 2858 | grad_norm: 3.3057987689971924
Step 2858 | learning_rate: 8.253050738599873e-06
Step 2858 | epoch: 2.7533718689788054
Step 2859 | loss: 3.2191545963287354
Step 2859 | grad_norm: 3.2547237873077393
Step 2859 | learning_rate: 8.220937700706487e-06
Step 2859 | epoch: 2.754335260115607
Step 2860 | loss: 3.665189266204834
Step 2860 | grad_norm: 3.725930690765381
Step 2860 | learning_rate: 8.188824662813103e-06
Step 2860 | epoch: 2.7552986512524082
Step 2861 | loss: 3.2197484970092773
Step 2861 | grad_norm: 3.3055853843688965
Step 2861 | learning_rate: 8.156711624919717e-06
Step 2861 | epoch: 2.75626204238921
Step 2862 | loss: 3.093104600906372
Step 2862 | grad_norm: 2.7121589183807373
Step 2862 | learning_rate: 8.124598587026333e-06
Step 2862 | epoch: 2.7572254335260116
Step 2863 | loss: 2.78944993019104
Step 2863 | grad_norm: 2.7117888927459717
Step 2863 | learning_rate: 8.092485549132949e-06
Step 2863 | epoch: 2.7581888246628132
Step 2864 | loss: 3.1852266788482666
Step 2864 | grad_norm: 3.0311505794525146
Step 2864 | learning_rate: 8.060372511239564e-06
Step 2864 | epoch: 2.759152215799615
Step 2865 | loss: 3.509202718734741
Step 2865 | grad_norm: 2.74442458152771
Step 2865 | learning_rate: 8.028259473346179e-06
Step 2865 | epoch: 2.760115606936416
Step 2866 | loss: 3.068248748779297
Step 2866 | grad_norm: 2.882875442504883
Step 2866 | learning_rate: 7.996146435452795e-06
Step 2866 | epoch: 2.7610789980732178
Step 2867 | loss: 3.7112650871276855
Step 2867 | grad_norm: 3.3386640548706055
Step 2867 | learning_rate: 7.964033397559409e-06
Step 2867 | epoch: 2.7620423892100194
Step 2868 | loss: 3.840757131576538
Step 2868 | grad_norm: 3.2607076168060303
Step 2868 | learning_rate: 7.931920359666025e-06
Step 2868 | epoch: 2.7630057803468207
Step 2869 | loss: 2.981626272201538
Step 2869 | grad_norm: 3.008446216583252
Step 2869 | learning_rate: 7.89980732177264e-06
Step 2869 | epoch: 2.7639691714836223
Step 2870 | loss: 3.118926525115967
Step 2870 | grad_norm: 3.3174591064453125
Step 2870 | learning_rate: 7.867694283879255e-06
Step 2870 | epoch: 2.764932562620424
Step 2871 | loss: 2.8136582374572754
Step 2871 | grad_norm: 3.725612163543701
Step 2871 | learning_rate: 7.83558124598587e-06
Step 2871 | epoch: 2.7658959537572256
Step 2872 | loss: 3.9150524139404297
Step 2872 | grad_norm: 4.4311299324035645
Step 2872 | learning_rate: 7.803468208092486e-06
Step 2872 | epoch: 2.766859344894027
Step 2873 | loss: 2.954587697982788
Step 2873 | grad_norm: 2.919646978378296
Step 2873 | learning_rate: 7.771355170199102e-06
Step 2873 | epoch: 2.7678227360308285
Step 2874 | loss: 3.259910821914673
Step 2874 | grad_norm: 2.588721752166748
Step 2874 | learning_rate: 7.739242132305717e-06
Step 2874 | epoch: 2.76878612716763
Step 2875 | loss: 3.483779191970825
Step 2875 | grad_norm: 3.0660855770111084
Step 2875 | learning_rate: 7.70712909441233e-06
Step 2875 | epoch: 2.7697495183044314
Step 2876 | loss: 3.2377119064331055
Step 2876 | grad_norm: 2.522646427154541
Step 2876 | learning_rate: 7.675016056518947e-06
Step 2876 | epoch: 2.770712909441233
Step 2877 | loss: 3.0294008255004883
Step 2877 | grad_norm: 2.612516164779663
Step 2877 | learning_rate: 7.642903018625562e-06
Step 2877 | epoch: 2.7716763005780347
Step 2878 | loss: 3.4113028049468994
Step 2878 | grad_norm: 3.3226566314697266
Step 2878 | learning_rate: 7.6107899807321775e-06
Step 2878 | epoch: 2.7726396917148364
Step 2879 | loss: 3.4143965244293213
Step 2879 | grad_norm: 4.039645195007324
Step 2879 | learning_rate: 7.578676942838793e-06
Step 2879 | epoch: 2.773603082851638
Step 2880 | loss: 3.730165958404541
Step 2880 | grad_norm: 3.2818942070007324
Step 2880 | learning_rate: 7.5465639049454084e-06
Step 2880 | epoch: 2.7745664739884393
Step 2881 | loss: 3.7734432220458984
Step 2881 | grad_norm: 3.5226571559906006
Step 2881 | learning_rate: 7.514450867052024e-06
Step 2881 | epoch: 2.775529865125241
Step 2882 | loss: 4.1430768966674805
Step 2882 | grad_norm: 3.7974071502685547
Step 2882 | learning_rate: 7.482337829158639e-06
Step 2882 | epoch: 2.776493256262042
Step 2883 | loss: 3.7372984886169434
Step 2883 | grad_norm: 3.0411148071289062
Step 2883 | learning_rate: 7.4502247912652535e-06
Step 2883 | epoch: 2.777456647398844
Step 2884 | loss: 3.845700740814209
Step 2884 | grad_norm: 3.362502336502075
Step 2884 | learning_rate: 7.4181117533718686e-06
Step 2884 | epoch: 2.7784200385356455
Step 2885 | loss: 3.862879753112793
Step 2885 | grad_norm: 3.0786001682281494
Step 2885 | learning_rate: 7.3859987154784845e-06
Step 2885 | epoch: 2.779383429672447
Step 2886 | loss: 2.7618257999420166
Step 2886 | grad_norm: 3.582569122314453
Step 2886 | learning_rate: 7.3538856775850995e-06
Step 2886 | epoch: 2.7803468208092488
Step 2887 | loss: 3.2238128185272217
Step 2887 | grad_norm: 2.7866005897521973
Step 2887 | learning_rate: 7.321772639691715e-06
Step 2887 | epoch: 2.78131021194605
Step 2888 | loss: 3.2757840156555176
Step 2888 | grad_norm: 2.848583221435547
Step 2888 | learning_rate: 7.28965960179833e-06
Step 2888 | epoch: 2.7822736030828517
Step 2889 | loss: 2.5904176235198975
Step 2889 | grad_norm: 2.530120611190796
Step 2889 | learning_rate: 7.257546563904946e-06
Step 2889 | epoch: 2.7832369942196533
Step 2890 | loss: 3.3528919219970703
Step 2890 | grad_norm: 3.167140245437622
Step 2890 | learning_rate: 7.225433526011561e-06
Step 2890 | epoch: 2.7842003853564545
Step 2891 | loss: 2.7183003425598145
Step 2891 | grad_norm: 2.463437080383301
Step 2891 | learning_rate: 7.1933204881181755e-06
Step 2891 | epoch: 2.785163776493256
Step 2892 | loss: 3.9559805393218994
Step 2892 | grad_norm: 3.413419246673584
Step 2892 | learning_rate: 7.161207450224791e-06
Step 2892 | epoch: 2.786127167630058
Step 2893 | loss: 3.6810011863708496
Step 2893 | grad_norm: 3.2614188194274902
Step 2893 | learning_rate: 7.129094412331406e-06
Step 2893 | epoch: 2.7870905587668595
Step 2894 | loss: 3.157515048980713
Step 2894 | grad_norm: 3.023881435394287
Step 2894 | learning_rate: 7.096981374438022e-06
Step 2894 | epoch: 2.7880539499036607
Step 2895 | loss: 2.845423460006714
Step 2895 | grad_norm: 2.6404950618743896
Step 2895 | learning_rate: 7.064868336544637e-06
Step 2895 | epoch: 2.7890173410404624
Step 2896 | loss: 3.7455430030822754
Step 2896 | grad_norm: 2.845360040664673
Step 2896 | learning_rate: 7.032755298651253e-06
Step 2896 | epoch: 2.789980732177264
Step 2897 | loss: 3.4093666076660156
Step 2897 | grad_norm: 2.888911485671997
Step 2897 | learning_rate: 7.000642260757868e-06
Step 2897 | epoch: 2.7909441233140653
Step 2898 | loss: 3.355336904525757
Step 2898 | grad_norm: 2.813852310180664
Step 2898 | learning_rate: 6.968529222864484e-06
Step 2898 | epoch: 2.791907514450867
Step 2899 | loss: 2.9802980422973633
Step 2899 | grad_norm: 2.845334768295288
Step 2899 | learning_rate: 6.936416184971098e-06
Step 2899 | epoch: 2.7928709055876686
Step 2900 | loss: 3.9565255641937256
Step 2900 | grad_norm: 3.079603672027588
Step 2900 | learning_rate: 6.904303147077713e-06
Step 2900 | epoch: 2.7938342967244703
Step 2901 | loss: 3.0030715465545654
Step 2901 | grad_norm: 2.733419179916382
Step 2901 | learning_rate: 6.872190109184329e-06
Step 2901 | epoch: 2.794797687861272
Step 2902 | loss: 3.769404888153076
Step 2902 | grad_norm: 3.7655160427093506
Step 2902 | learning_rate: 6.840077071290944e-06
Step 2902 | epoch: 2.795761078998073
Step 2903 | loss: 3.197389841079712
Step 2903 | grad_norm: 2.602445125579834
Step 2903 | learning_rate: 6.807964033397559e-06
Step 2903 | epoch: 2.796724470134875
Step 2904 | loss: 2.071897029876709
Step 2904 | grad_norm: 2.115569591522217
Step 2904 | learning_rate: 6.775850995504175e-06
Step 2904 | epoch: 2.7976878612716765
Step 2905 | loss: 3.5714902877807617
Step 2905 | grad_norm: 2.9909839630126953
Step 2905 | learning_rate: 6.74373795761079e-06
Step 2905 | epoch: 2.7986512524084777
Step 2906 | loss: 3.3702495098114014
Step 2906 | grad_norm: 3.070528984069824
Step 2906 | learning_rate: 6.711624919717406e-06
Step 2906 | epoch: 2.7996146435452793
Step 2907 | loss: 3.2002766132354736
Step 2907 | grad_norm: 3.237541913986206
Step 2907 | learning_rate: 6.679511881824021e-06
Step 2907 | epoch: 2.800578034682081
Step 2908 | loss: 2.9333317279815674
Step 2908 | grad_norm: 2.578441858291626
Step 2908 | learning_rate: 6.647398843930635e-06
Step 2908 | epoch: 2.8015414258188827
Step 2909 | loss: 3.398798942565918
Step 2909 | grad_norm: 2.891301155090332
Step 2909 | learning_rate: 6.615285806037251e-06
Step 2909 | epoch: 2.802504816955684
Step 2910 | loss: 3.536618947982788
Step 2910 | grad_norm: 3.997953176498413
Step 2910 | learning_rate: 6.583172768143866e-06
Step 2910 | epoch: 2.8034682080924855
Step 2911 | loss: 3.4076595306396484
Step 2911 | grad_norm: 2.997067451477051
Step 2911 | learning_rate: 6.551059730250482e-06
Step 2911 | epoch: 2.804431599229287
Step 2912 | loss: 3.7108547687530518
Step 2912 | grad_norm: 3.189314126968384
Step 2912 | learning_rate: 6.518946692357097e-06
Step 2912 | epoch: 2.8053949903660884
Step 2913 | loss: 2.5531368255615234
Step 2913 | grad_norm: 2.5923097133636475
Step 2913 | learning_rate: 6.486833654463713e-06
Step 2913 | epoch: 2.80635838150289
Step 2914 | loss: 2.992053270339966
Step 2914 | grad_norm: 2.8639204502105713
Step 2914 | learning_rate: 6.454720616570328e-06
Step 2914 | epoch: 2.8073217726396917
Step 2915 | loss: 3.129718065261841
Step 2915 | grad_norm: 3.6037256717681885
Step 2915 | learning_rate: 6.422607578676944e-06
Step 2915 | epoch: 2.8082851637764934
Step 2916 | loss: 2.862131118774414
Step 2916 | grad_norm: 3.5280075073242188
Step 2916 | learning_rate: 6.390494540783558e-06
Step 2916 | epoch: 2.809248554913295
Step 2917 | loss: 3.5343124866485596
Step 2917 | grad_norm: 2.742372512817383
Step 2917 | learning_rate: 6.358381502890173e-06
Step 2917 | epoch: 2.8102119460500963
Step 2918 | loss: 3.607381820678711
Step 2918 | grad_norm: 2.8054823875427246
Step 2918 | learning_rate: 6.326268464996789e-06
Step 2918 | epoch: 2.811175337186898
Step 2919 | loss: 3.3918616771698
Step 2919 | grad_norm: 2.9029860496520996
Step 2919 | learning_rate: 6.294155427103404e-06
Step 2919 | epoch: 2.812138728323699
Step 2920 | loss: 3.630152940750122
Step 2920 | grad_norm: 3.0731263160705566
Step 2920 | learning_rate: 6.26204238921002e-06
Step 2920 | epoch: 2.813102119460501
Step 2921 | loss: 4.219313144683838
Step 2921 | grad_norm: 5.328465938568115
Step 2921 | learning_rate: 6.229929351316635e-06
Step 2921 | epoch: 2.8140655105973025
Step 2922 | loss: 3.1269054412841797
Step 2922 | grad_norm: 3.6010496616363525
Step 2922 | learning_rate: 6.19781631342325e-06
Step 2922 | epoch: 2.815028901734104
Step 2923 | loss: 3.503086805343628
Step 2923 | grad_norm: 2.8664894104003906
Step 2923 | learning_rate: 6.165703275529865e-06
Step 2923 | epoch: 2.815992292870906
Step 2924 | loss: 2.8948609828948975
Step 2924 | grad_norm: 2.5957067012786865
Step 2924 | learning_rate: 6.133590237636481e-06
Step 2924 | epoch: 2.816955684007707
Step 2925 | loss: 2.4827136993408203
Step 2925 | grad_norm: 3.7286651134490967
Step 2925 | learning_rate: 6.101477199743096e-06
Step 2925 | epoch: 2.8179190751445087
Step 2926 | loss: 3.4328882694244385
Step 2926 | grad_norm: 3.0454869270324707
Step 2926 | learning_rate: 6.069364161849711e-06
Step 2926 | epoch: 2.8188824662813103
Step 2927 | loss: 3.7744133472442627
Step 2927 | grad_norm: 2.5025010108947754
Step 2927 | learning_rate: 6.037251123956326e-06
Step 2927 | epoch: 2.8198458574181116
Step 2928 | loss: 3.3864710330963135
Step 2928 | grad_norm: 3.124075412750244
Step 2928 | learning_rate: 6.005138086062942e-06
Step 2928 | epoch: 2.820809248554913
Step 2929 | loss: 3.1611714363098145
Step 2929 | grad_norm: 3.185596466064453
Step 2929 | learning_rate: 5.973025048169557e-06
Step 2929 | epoch: 2.821772639691715
Step 2930 | loss: 3.07730770111084
Step 2930 | grad_norm: 2.683276891708374
Step 2930 | learning_rate: 5.940912010276172e-06
Step 2930 | epoch: 2.8227360308285165
Step 2931 | loss: 3.4703497886657715
Step 2931 | grad_norm: 3.0807745456695557
Step 2931 | learning_rate: 5.908798972382788e-06
Step 2931 | epoch: 2.8236994219653178
Step 2932 | loss: 3.710012674331665
Step 2932 | grad_norm: 2.7686028480529785
Step 2932 | learning_rate: 5.876685934489403e-06
Step 2932 | epoch: 2.8246628131021194
Step 2933 | loss: 3.13223934173584
Step 2933 | grad_norm: 3.1722235679626465
Step 2933 | learning_rate: 5.844572896596019e-06
Step 2933 | epoch: 2.825626204238921
Step 2934 | loss: 3.3703105449676514
Step 2934 | grad_norm: 2.6656429767608643
Step 2934 | learning_rate: 5.812459858702633e-06
Step 2934 | epoch: 2.8265895953757223
Step 2935 | loss: 3.014342784881592
Step 2935 | grad_norm: 3.321319103240967
Step 2935 | learning_rate: 5.780346820809249e-06
Step 2935 | epoch: 2.827552986512524
Step 2936 | loss: 3.4276602268218994
Step 2936 | grad_norm: 2.7633137702941895
Step 2936 | learning_rate: 5.748233782915864e-06
Step 2936 | epoch: 2.8285163776493256
Step 2937 | loss: 4.694175720214844
Step 2937 | grad_norm: 3.7548813819885254
Step 2937 | learning_rate: 5.71612074502248e-06
Step 2937 | epoch: 2.8294797687861273
Step 2938 | loss: 3.0060336589813232
Step 2938 | grad_norm: 2.5049400329589844
Step 2938 | learning_rate: 5.684007707129094e-06
Step 2938 | epoch: 2.830443159922929
Step 2939 | loss: 3.7090256214141846
Step 2939 | grad_norm: 3.6304943561553955
Step 2939 | learning_rate: 5.65189466923571e-06
Step 2939 | epoch: 2.83140655105973
Step 2940 | loss: 3.156442642211914
Step 2940 | grad_norm: 3.049600839614868
Step 2940 | learning_rate: 5.619781631342325e-06
Step 2940 | epoch: 2.832369942196532
Step 2941 | loss: 3.468219041824341
Step 2941 | grad_norm: 2.565451145172119
Step 2941 | learning_rate: 5.587668593448941e-06
Step 2941 | epoch: 2.8333333333333335
Step 2942 | loss: 3.7004899978637695
Step 2942 | grad_norm: 2.959010362625122
Step 2942 | learning_rate: 5.555555555555556e-06
Step 2942 | epoch: 2.8342967244701347
Step 2943 | loss: 3.804266929626465
Step 2943 | grad_norm: 3.1834006309509277
Step 2943 | learning_rate: 5.523442517662171e-06
Step 2943 | epoch: 2.8352601156069364
Step 2944 | loss: 3.8977043628692627
Step 2944 | grad_norm: 3.702672243118286
Step 2944 | learning_rate: 5.491329479768787e-06
Step 2944 | epoch: 2.836223506743738
Step 2945 | loss: 3.1585114002227783
Step 2945 | grad_norm: 3.164039373397827
Step 2945 | learning_rate: 5.459216441875402e-06
Step 2945 | epoch: 2.8371868978805397
Step 2946 | loss: 2.5279183387756348
Step 2946 | grad_norm: 3.5353763103485107
Step 2946 | learning_rate: 5.427103403982017e-06
Step 2946 | epoch: 2.838150289017341
Step 2947 | loss: 2.8776190280914307
Step 2947 | grad_norm: 4.168482780456543
Step 2947 | learning_rate: 5.394990366088632e-06
Step 2947 | epoch: 2.8391136801541426
Step 2948 | loss: 2.580336570739746
Step 2948 | grad_norm: 2.4591660499572754
Step 2948 | learning_rate: 5.362877328195248e-06
Step 2948 | epoch: 2.840077071290944
Step 2949 | loss: 4.374993801116943
Step 2949 | grad_norm: 2.8980820178985596
Step 2949 | learning_rate: 5.330764290301863e-06
Step 2949 | epoch: 2.8410404624277454
Step 2950 | loss: 3.1057281494140625
Step 2950 | grad_norm: 3.2266688346862793
Step 2950 | learning_rate: 5.298651252408478e-06
Step 2950 | epoch: 2.842003853564547
Step 2951 | loss: 3.9978058338165283
Step 2951 | grad_norm: 5.0064802169799805
Step 2951 | learning_rate: 5.266538214515093e-06
Step 2951 | epoch: 2.8429672447013488
Step 2952 | loss: 2.7189347743988037
Step 2952 | grad_norm: 2.7432000637054443
Step 2952 | learning_rate: 5.234425176621709e-06
Step 2952 | epoch: 2.8439306358381504
Step 2953 | loss: 3.5334126949310303
Step 2953 | grad_norm: 3.2594292163848877
Step 2953 | learning_rate: 5.202312138728324e-06
Step 2953 | epoch: 2.844894026974952
Step 2954 | loss: 2.6572299003601074
Step 2954 | grad_norm: 3.2374842166900635
Step 2954 | learning_rate: 5.17019910083494e-06
Step 2954 | epoch: 2.8458574181117533
Step 2955 | loss: 3.484830617904663
Step 2955 | grad_norm: 2.9648640155792236
Step 2955 | learning_rate: 5.138086062941555e-06
Step 2955 | epoch: 2.846820809248555
Step 2956 | loss: 2.422401189804077
Step 2956 | grad_norm: 2.427421808242798
Step 2956 | learning_rate: 5.10597302504817e-06
Step 2956 | epoch: 2.847784200385356
Step 2957 | loss: 3.9676311016082764
Step 2957 | grad_norm: 3.3010618686676025
Step 2957 | learning_rate: 5.0738599871547856e-06
Step 2957 | epoch: 2.848747591522158
Step 2958 | loss: 2.7795815467834473
Step 2958 | grad_norm: 3.16479754447937
Step 2958 | learning_rate: 5.041746949261401e-06
Step 2958 | epoch: 2.8497109826589595
Step 2959 | loss: 3.3861310482025146
Step 2959 | grad_norm: 2.715646982192993
Step 2959 | learning_rate: 5.009633911368016e-06
Step 2959 | epoch: 2.850674373795761
Step 2960 | loss: 3.3712704181671143
Step 2960 | grad_norm: 3.179757833480835
Step 2960 | learning_rate: 4.977520873474631e-06
Step 2960 | epoch: 2.851637764932563
Step 2961 | loss: 3.4961814880371094
Step 2961 | grad_norm: 3.2348880767822266
Step 2961 | learning_rate: 4.9454078355812466e-06
Step 2961 | epoch: 2.852601156069364
Step 2962 | loss: 3.478750467300415
Step 2962 | grad_norm: 2.606341600418091
Step 2962 | learning_rate: 4.913294797687862e-06
Step 2962 | epoch: 2.8535645472061657
Step 2963 | loss: 3.129908561706543
Step 2963 | grad_norm: 3.1290342807769775
Step 2963 | learning_rate: 4.881181759794477e-06
Step 2963 | epoch: 2.8545279383429674
Step 2964 | loss: 3.5612294673919678
Step 2964 | grad_norm: 3.3717074394226074
Step 2964 | learning_rate: 4.849068721901092e-06
Step 2964 | epoch: 2.8554913294797686
Step 2965 | loss: 3.032620668411255
Step 2965 | grad_norm: 2.97774338722229
Step 2965 | learning_rate: 4.8169556840077075e-06
Step 2965 | epoch: 2.8564547206165702
Step 2966 | loss: 2.7642974853515625
Step 2966 | grad_norm: 3.408916473388672
Step 2966 | learning_rate: 4.784842646114323e-06
Step 2966 | epoch: 2.857418111753372
Step 2967 | loss: 3.384333372116089
Step 2967 | grad_norm: 3.2149691581726074
Step 2967 | learning_rate: 4.752729608220938e-06
Step 2967 | epoch: 2.8583815028901736
Step 2968 | loss: 4.129033088684082
Step 2968 | grad_norm: 5.190495491027832
Step 2968 | learning_rate: 4.7206165703275535e-06
Step 2968 | epoch: 2.8593448940269752
Step 2969 | loss: 2.956319808959961
Step 2969 | grad_norm: 2.845676898956299
Step 2969 | learning_rate: 4.6885035324341685e-06
Step 2969 | epoch: 2.8603082851637764
Step 2970 | loss: 3.8789587020874023
Step 2970 | grad_norm: 2.740783214569092
Step 2970 | learning_rate: 4.656390494540784e-06
Step 2970 | epoch: 2.861271676300578
Step 2971 | loss: 3.8507494926452637
Step 2971 | grad_norm: 3.6896090507507324
Step 2971 | learning_rate: 4.624277456647399e-06
Step 2971 | epoch: 2.8622350674373793
Step 2972 | loss: 3.5726675987243652
Step 2972 | grad_norm: 2.9599404335021973
Step 2972 | learning_rate: 4.5921644187540145e-06
Step 2972 | epoch: 2.863198458574181
Step 2973 | loss: 3.426926374435425
Step 2973 | grad_norm: 3.043297529220581
Step 2973 | learning_rate: 4.5600513808606295e-06
Step 2973 | epoch: 2.8641618497109826
Step 2974 | loss: 3.0735809803009033
Step 2974 | grad_norm: 2.727189302444458
Step 2974 | learning_rate: 4.527938342967245e-06
Step 2974 | epoch: 2.8651252408477843
Step 2975 | loss: 2.9912757873535156
Step 2975 | grad_norm: 3.449291229248047
Step 2975 | learning_rate: 4.49582530507386e-06
Step 2975 | epoch: 2.866088631984586
Step 2976 | loss: 3.163294553756714
Step 2976 | grad_norm: 3.9487218856811523
Step 2976 | learning_rate: 4.4637122671804755e-06
Step 2976 | epoch: 2.867052023121387
Step 2977 | loss: 3.4418442249298096
Step 2977 | grad_norm: 2.777858018875122
Step 2977 | learning_rate: 4.4315992292870905e-06
Step 2977 | epoch: 2.868015414258189
Step 2978 | loss: 3.422945261001587
Step 2978 | grad_norm: 3.3816521167755127
Step 2978 | learning_rate: 4.399486191393706e-06
Step 2978 | epoch: 2.8689788053949905
Step 2979 | loss: 3.0521273612976074
Step 2979 | grad_norm: 3.442716598510742
Step 2979 | learning_rate: 4.367373153500321e-06
Step 2979 | epoch: 2.8699421965317917
Step 2980 | loss: 3.790975570678711
Step 2980 | grad_norm: 3.417982816696167
Step 2980 | learning_rate: 4.3352601156069365e-06
Step 2980 | epoch: 2.8709055876685934
Step 2981 | loss: 3.0741055011749268
Step 2981 | grad_norm: 2.9579005241394043
Step 2981 | learning_rate: 4.303147077713552e-06
Step 2981 | epoch: 2.871868978805395
Step 2982 | loss: 3.0979514122009277
Step 2982 | grad_norm: 3.026360511779785
Step 2982 | learning_rate: 4.271034039820167e-06
Step 2982 | epoch: 2.8728323699421967
Step 2983 | loss: 3.9267098903656006
Step 2983 | grad_norm: 3.606421709060669
Step 2983 | learning_rate: 4.238921001926782e-06
Step 2983 | epoch: 2.873795761078998
Step 2984 | loss: 3.711972236633301
Step 2984 | grad_norm: 4.236213207244873
Step 2984 | learning_rate: 4.2068079640333974e-06
Step 2984 | epoch: 2.8747591522157996
Step 2985 | loss: 2.919173240661621
Step 2985 | grad_norm: 4.069000720977783
Step 2985 | learning_rate: 4.174694926140013e-06
Step 2985 | epoch: 2.8757225433526012
Step 2986 | loss: 3.418649435043335
Step 2986 | grad_norm: 3.194241762161255
Step 2986 | learning_rate: 4.142581888246628e-06
Step 2986 | epoch: 2.8766859344894025
Step 2987 | loss: 3.1205689907073975
Step 2987 | grad_norm: 2.6452786922454834
Step 2987 | learning_rate: 4.110468850353243e-06
Step 2987 | epoch: 2.877649325626204
Step 2988 | loss: 3.4001071453094482
Step 2988 | grad_norm: 2.90094256401062
Step 2988 | learning_rate: 4.078355812459858e-06
Step 2988 | epoch: 2.878612716763006
Step 2989 | loss: 3.0541374683380127
Step 2989 | grad_norm: 2.4291346073150635
Step 2989 | learning_rate: 4.046242774566474e-06
Step 2989 | epoch: 2.8795761078998074
Step 2990 | loss: 3.2071638107299805
Step 2990 | grad_norm: 2.66670560836792
Step 2990 | learning_rate: 4.014129736673089e-06
Step 2990 | epoch: 2.880539499036609
Step 2991 | loss: 3.563220977783203
Step 2991 | grad_norm: 2.9166359901428223
Step 2991 | learning_rate: 3.982016698779704e-06
Step 2991 | epoch: 2.8815028901734103
Step 2992 | loss: 4.174086093902588
Step 2992 | grad_norm: 2.9859063625335693
Step 2992 | learning_rate: 3.94990366088632e-06
Step 2992 | epoch: 2.882466281310212
Step 2993 | loss: 3.1401946544647217
Step 2993 | grad_norm: 3.638828754425049
Step 2993 | learning_rate: 3.917790622992935e-06
Step 2993 | epoch: 2.8834296724470136
Step 2994 | loss: 3.034116744995117
Step 2994 | grad_norm: 2.5344347953796387
Step 2994 | learning_rate: 3.885677585099551e-06
Step 2994 | epoch: 2.884393063583815
Step 2995 | loss: 4.001959800720215
Step 2995 | grad_norm: 4.252598285675049
Step 2995 | learning_rate: 3.853564547206165e-06
Step 2995 | epoch: 2.8853564547206165
Step 2996 | loss: 3.581648826599121
Step 2996 | grad_norm: 2.539933443069458
Step 2996 | learning_rate: 3.821451509312781e-06
Step 2996 | epoch: 2.886319845857418
Step 2997 | loss: 2.6448724269866943
Step 2997 | grad_norm: 2.9545741081237793
Step 2997 | learning_rate: 3.7893384714193967e-06
Step 2997 | epoch: 2.88728323699422
Step 2998 | loss: 3.4953272342681885
Step 2998 | grad_norm: 2.924251079559326
Step 2998 | learning_rate: 3.757225433526012e-06
Step 2998 | epoch: 2.888246628131021
Step 2999 | loss: 3.092876434326172
Step 2999 | grad_norm: 2.8012311458587646
Step 2999 | learning_rate: 3.7251123956326268e-06
Step 2999 | epoch: 2.8892100192678227
Step 3000 | loss: 3.217952251434326
Step 3000 | grad_norm: 4.7072954177856445
Step 3000 | learning_rate: 3.6929993577392422e-06
Step 3000 | epoch: 2.8901734104046244
Step 3001 | loss: 3.1488332748413086
Step 3001 | grad_norm: 2.5472493171691895
Step 3001 | learning_rate: 3.6608863198458577e-06
Step 3001 | epoch: 2.8911368015414256
Step 3002 | loss: 3.456777811050415
Step 3002 | grad_norm: 2.847900152206421
Step 3002 | learning_rate: 3.628773281952473e-06
Step 3002 | epoch: 2.8921001926782273
Step 3003 | loss: 3.751373052597046
Step 3003 | grad_norm: 3.8247549533843994
Step 3003 | learning_rate: 3.5966602440590878e-06
Step 3003 | epoch: 2.893063583815029
Step 3004 | loss: 3.521610736846924
Step 3004 | grad_norm: 8.795439720153809
Step 3004 | learning_rate: 3.564547206165703e-06
Step 3004 | epoch: 2.8940269749518306
Step 3005 | loss: 2.5711865425109863
Step 3005 | grad_norm: 2.7435214519500732
Step 3005 | learning_rate: 3.5324341682723187e-06
Step 3005 | epoch: 2.8949903660886322
Step 3006 | loss: 2.7944650650024414
Step 3006 | grad_norm: 3.403916358947754
Step 3006 | learning_rate: 3.500321130378934e-06
Step 3006 | epoch: 2.8959537572254335
Step 3007 | loss: 3.118704319000244
Step 3007 | grad_norm: 2.38887619972229
Step 3007 | learning_rate: 3.468208092485549e-06
Step 3007 | epoch: 2.896917148362235
Step 3008 | loss: 3.3217880725860596
Step 3008 | grad_norm: 3.2988779544830322
Step 3008 | learning_rate: 3.4360950545921646e-06
Step 3008 | epoch: 2.8978805394990363
Step 3009 | loss: 3.8793182373046875
Step 3009 | grad_norm: 3.0792200565338135
Step 3009 | learning_rate: 3.4039820166987797e-06
Step 3009 | epoch: 2.898843930635838
Step 3010 | loss: 3.2483344078063965
Step 3010 | grad_norm: 2.4812698364257812
Step 3010 | learning_rate: 3.371868978805395e-06
Step 3010 | epoch: 2.8998073217726397
Step 3011 | loss: 3.352760076522827
Step 3011 | grad_norm: 3.0195772647857666
Step 3011 | learning_rate: 3.3397559409120106e-06
Step 3011 | epoch: 2.9007707129094413
Step 3012 | loss: 3.576453924179077
Step 3012 | grad_norm: 3.1582233905792236
Step 3012 | learning_rate: 3.3076429030186256e-06
Step 3012 | epoch: 2.901734104046243
Step 3013 | loss: 2.9059863090515137
Step 3013 | grad_norm: 2.7222986221313477
Step 3013 | learning_rate: 3.275529865125241e-06
Step 3013 | epoch: 2.902697495183044
Step 3014 | loss: 2.8359975814819336
Step 3014 | grad_norm: 2.8480172157287598
Step 3014 | learning_rate: 3.2434168272318565e-06
Step 3014 | epoch: 2.903660886319846
Step 3015 | loss: 2.9256246089935303
Step 3015 | grad_norm: 2.8808326721191406
Step 3015 | learning_rate: 3.211303789338472e-06
Step 3015 | epoch: 2.9046242774566475
Step 3016 | loss: 3.128208875656128
Step 3016 | grad_norm: 2.5168681144714355
Step 3016 | learning_rate: 3.1791907514450866e-06
Step 3016 | epoch: 2.9055876685934487
Step 3017 | loss: 3.43505859375
Step 3017 | grad_norm: 2.946214199066162
Step 3017 | learning_rate: 3.147077713551702e-06
Step 3017 | epoch: 2.9065510597302504
Step 3018 | loss: 3.352871894836426
Step 3018 | grad_norm: 4.069548606872559
Step 3018 | learning_rate: 3.1149646756583175e-06
Step 3018 | epoch: 2.907514450867052
Step 3019 | loss: 2.7705488204956055
Step 3019 | grad_norm: 2.849043846130371
Step 3019 | learning_rate: 3.0828516377649325e-06
Step 3019 | epoch: 2.9084778420038537
Step 3020 | loss: 3.1929633617401123
Step 3020 | grad_norm: 3.0824081897735596
Step 3020 | learning_rate: 3.050738599871548e-06
Step 3020 | epoch: 2.909441233140655
Step 3021 | loss: 4.132687091827393
Step 3021 | grad_norm: 3.9693684577941895
Step 3021 | learning_rate: 3.018625561978163e-06
Step 3021 | epoch: 2.9104046242774566
Step 3022 | loss: 3.997819662094116
Step 3022 | grad_norm: 4.3094940185546875
Step 3022 | learning_rate: 2.9865125240847785e-06
Step 3022 | epoch: 2.9113680154142583
Step 3023 | loss: 2.693701982498169
Step 3023 | grad_norm: 3.7407681941986084
Step 3023 | learning_rate: 2.954399486191394e-06
Step 3023 | epoch: 2.9123314065510595
Step 3024 | loss: 3.1995859146118164
Step 3024 | grad_norm: 2.310096025466919
Step 3024 | learning_rate: 2.9222864482980094e-06
Step 3024 | epoch: 2.913294797687861
Step 3025 | loss: 3.464244842529297
Step 3025 | grad_norm: 4.098642349243164
Step 3025 | learning_rate: 2.8901734104046244e-06
Step 3025 | epoch: 2.914258188824663
Step 3026 | loss: 3.655454635620117
Step 3026 | grad_norm: 3.310807228088379
Step 3026 | learning_rate: 2.85806037251124e-06
Step 3026 | epoch: 2.9152215799614645
Step 3027 | loss: 3.5318212509155273
Step 3027 | grad_norm: 3.5469727516174316
Step 3027 | learning_rate: 2.825947334617855e-06
Step 3027 | epoch: 2.916184971098266
Step 3028 | loss: 3.3845295906066895
Step 3028 | grad_norm: 3.4358255863189697
Step 3028 | learning_rate: 2.7938342967244704e-06
Step 3028 | epoch: 2.9171483622350673
Step 3029 | loss: 3.5336880683898926
Step 3029 | grad_norm: 3.6574623584747314
Step 3029 | learning_rate: 2.7617212588310854e-06
Step 3029 | epoch: 2.918111753371869
Step 3030 | loss: 2.9583680629730225
Step 3030 | grad_norm: 2.8959178924560547
Step 3030 | learning_rate: 2.729608220937701e-06
Step 3030 | epoch: 2.9190751445086707
Step 3031 | loss: 3.732260227203369
Step 3031 | grad_norm: 3.070949077606201
Step 3031 | learning_rate: 2.697495183044316e-06
Step 3031 | epoch: 2.920038535645472
Step 3032 | loss: 3.4973998069763184
Step 3032 | grad_norm: 3.1229817867279053
Step 3032 | learning_rate: 2.6653821451509314e-06
Step 3032 | epoch: 2.9210019267822736
Step 3033 | loss: 3.6119894981384277
Step 3033 | grad_norm: 2.895890712738037
Step 3033 | learning_rate: 2.6332691072575464e-06
Step 3033 | epoch: 2.921965317919075
Step 3034 | loss: 2.9029178619384766
Step 3034 | grad_norm: 2.682600498199463
Step 3034 | learning_rate: 2.601156069364162e-06
Step 3034 | epoch: 2.922928709055877
Step 3035 | loss: 3.3445444107055664
Step 3035 | grad_norm: 3.885911226272583
Step 3035 | learning_rate: 2.5690430314707773e-06
Step 3035 | epoch: 2.923892100192678
Step 3036 | loss: 3.3239500522613525
Step 3036 | grad_norm: 3.2746951580047607
Step 3036 | learning_rate: 2.5369299935773928e-06
Step 3036 | epoch: 2.9248554913294798
Step 3037 | loss: 3.1665217876434326
Step 3037 | grad_norm: 2.582000970840454
Step 3037 | learning_rate: 2.504816955684008e-06
Step 3037 | epoch: 2.9258188824662814
Step 3038 | loss: 3.5805325508117676
Step 3038 | grad_norm: 3.734036445617676
Step 3038 | learning_rate: 2.4727039177906233e-06
Step 3038 | epoch: 2.9267822736030826
Step 3039 | loss: 3.264622688293457
Step 3039 | grad_norm: 3.770451068878174
Step 3039 | learning_rate: 2.4405908798972383e-06
Step 3039 | epoch: 2.9277456647398843
Step 3040 | loss: 3.0592868328094482
Step 3040 | grad_norm: 3.110370635986328
Step 3040 | learning_rate: 2.4084778420038538e-06
Step 3040 | epoch: 2.928709055876686
Step 3041 | loss: 3.3879921436309814
Step 3041 | grad_norm: 2.822641134262085
Step 3041 | learning_rate: 2.376364804110469e-06
Step 3041 | epoch: 2.9296724470134876
Step 3042 | loss: 2.358795642852783
Step 3042 | grad_norm: 1.9168574810028076
Step 3042 | learning_rate: 2.3442517662170843e-06
Step 3042 | epoch: 2.9306358381502893
Step 3043 | loss: 3.5547964572906494
Step 3043 | grad_norm: 2.9047327041625977
Step 3043 | learning_rate: 2.3121387283236993e-06
Step 3043 | epoch: 2.9315992292870905
Step 3044 | loss: 3.577106475830078
Step 3044 | grad_norm: 2.8976173400878906
Step 3044 | learning_rate: 2.2800256904303148e-06
Step 3044 | epoch: 2.932562620423892
Step 3045 | loss: 3.516108512878418
Step 3045 | grad_norm: 2.778224468231201
Step 3045 | learning_rate: 2.24791265253693e-06
Step 3045 | epoch: 2.9335260115606934
Step 3046 | loss: 3.6499338150024414
Step 3046 | grad_norm: 4.172734260559082
Step 3046 | learning_rate: 2.2157996146435453e-06
Step 3046 | epoch: 2.934489402697495
Step 3047 | loss: 2.4468350410461426
Step 3047 | grad_norm: 2.4734413623809814
Step 3047 | learning_rate: 2.1836865767501607e-06
Step 3047 | epoch: 2.9354527938342967
Step 3048 | loss: 3.469087600708008
Step 3048 | grad_norm: 2.8149290084838867
Step 3048 | learning_rate: 2.151573538856776e-06
Step 3048 | epoch: 2.9364161849710984
Step 3049 | loss: 3.9678032398223877
Step 3049 | grad_norm: 3.327744483947754
Step 3049 | learning_rate: 2.119460500963391e-06
Step 3049 | epoch: 2.9373795761079
Step 3050 | loss: 4.078281402587891
Step 3050 | grad_norm: 5.038406848907471
Step 3050 | learning_rate: 2.0873474630700067e-06
Step 3050 | epoch: 2.9383429672447012
Step 3051 | loss: 3.073408603668213
Step 3051 | grad_norm: 4.075214385986328
Step 3051 | learning_rate: 2.0552344251766217e-06
Step 3051 | epoch: 2.939306358381503
Step 3052 | loss: 3.2064242362976074
Step 3052 | grad_norm: 2.9033565521240234
Step 3052 | learning_rate: 2.023121387283237e-06
Step 3052 | epoch: 2.9402697495183046
Step 3053 | loss: 2.728571653366089
Step 3053 | grad_norm: 2.4677681922912598
Step 3053 | learning_rate: 1.991008349389852e-06
Step 3053 | epoch: 2.9412331406551058
Step 3054 | loss: 3.573418617248535
Step 3054 | grad_norm: 5.971215724945068
Step 3054 | learning_rate: 1.9588953114964676e-06
Step 3054 | epoch: 2.9421965317919074
Step 3055 | loss: 3.19793701171875
Step 3055 | grad_norm: 3.7186827659606934
Step 3055 | learning_rate: 1.9267822736030827e-06
Step 3055 | epoch: 2.943159922928709
Step 3056 | loss: 3.0768022537231445
Step 3056 | grad_norm: 2.6994450092315674
Step 3056 | learning_rate: 1.8946692357096983e-06
Step 3056 | epoch: 2.9441233140655108
Step 3057 | loss: 3.2703869342803955
Step 3057 | grad_norm: 3.0524356365203857
Step 3057 | learning_rate: 1.8625561978163134e-06
Step 3057 | epoch: 2.9450867052023124
Step 3058 | loss: 3.3413965702056885
Step 3058 | grad_norm: 3.2655527591705322
Step 3058 | learning_rate: 1.8304431599229288e-06
Step 3058 | epoch: 2.9460500963391136
Step 3059 | loss: 3.069157361984253
Step 3059 | grad_norm: 3.180945634841919
Step 3059 | learning_rate: 1.7983301220295439e-06
Step 3059 | epoch: 2.9470134874759153
Step 3060 | loss: 2.979058265686035
Step 3060 | grad_norm: 3.3018672466278076
Step 3060 | learning_rate: 1.7662170841361593e-06
Step 3060 | epoch: 2.9479768786127165
Step 3061 | loss: 2.909698247909546
Step 3061 | grad_norm: 2.6603288650512695
Step 3061 | learning_rate: 1.7341040462427746e-06
Step 3061 | epoch: 2.948940269749518
Step 3062 | loss: 3.5988564491271973
Step 3062 | grad_norm: 2.759885787963867
Step 3062 | learning_rate: 1.7019910083493898e-06
Step 3062 | epoch: 2.94990366088632
Step 3063 | loss: 4.6179022789001465
Step 3063 | grad_norm: 3.933260440826416
Step 3063 | learning_rate: 1.6698779704560053e-06
Step 3063 | epoch: 2.9508670520231215
Step 3064 | loss: 2.844747543334961
Step 3064 | grad_norm: 2.90144419670105
Step 3064 | learning_rate: 1.6377649325626205e-06
Step 3064 | epoch: 2.951830443159923
Step 3065 | loss: 3.122634172439575
Step 3065 | grad_norm: 4.0526227951049805
Step 3065 | learning_rate: 1.605651894669236e-06
Step 3065 | epoch: 2.9527938342967244
Step 3066 | loss: 3.484579563140869
Step 3066 | grad_norm: 2.366116523742676
Step 3066 | learning_rate: 1.573538856775851e-06
Step 3066 | epoch: 2.953757225433526
Step 3067 | loss: 3.516761541366577
Step 3067 | grad_norm: 3.533596992492676
Step 3067 | learning_rate: 1.5414258188824663e-06
Step 3067 | epoch: 2.9547206165703277
Step 3068 | loss: 3.369720458984375
Step 3068 | grad_norm: 2.943608522415161
Step 3068 | learning_rate: 1.5093127809890815e-06
Step 3068 | epoch: 2.955684007707129
Step 3069 | loss: 3.1996474266052246
Step 3069 | grad_norm: 2.416294813156128
Step 3069 | learning_rate: 1.477199743095697e-06
Step 3069 | epoch: 2.9566473988439306
Step 3070 | loss: 3.3976306915283203
Step 3070 | grad_norm: 2.5872886180877686
Step 3070 | learning_rate: 1.4450867052023122e-06
Step 3070 | epoch: 2.9576107899807322
Step 3071 | loss: 3.301837682723999
Step 3071 | grad_norm: 3.2287328243255615
Step 3071 | learning_rate: 1.4129736673089275e-06
Step 3071 | epoch: 2.958574181117534
Step 3072 | loss: 4.2101054191589355
Step 3072 | grad_norm: 3.463101387023926
Step 3072 | learning_rate: 1.3808606294155427e-06
Step 3072 | epoch: 2.959537572254335
Step 3073 | loss: 3.7847938537597656
Step 3073 | grad_norm: 3.4568700790405273
Step 3073 | learning_rate: 1.348747591522158e-06
Step 3073 | epoch: 2.9605009633911368
Step 3074 | loss: 3.6254281997680664
Step 3074 | grad_norm: 2.564614772796631
Step 3074 | learning_rate: 1.3166345536287732e-06
Step 3074 | epoch: 2.9614643545279384
Step 3075 | loss: 3.519584894180298
Step 3075 | grad_norm: 2.888430595397949
Step 3075 | learning_rate: 1.2845215157353887e-06
Step 3075 | epoch: 2.9624277456647397
Step 3076 | loss: 3.2749435901641846
Step 3076 | grad_norm: 2.7551071643829346
Step 3076 | learning_rate: 1.252408477842004e-06
Step 3076 | epoch: 2.9633911368015413
Step 3077 | loss: 4.180285453796387
Step 3077 | grad_norm: 4.510650157928467
Step 3077 | learning_rate: 1.2202954399486192e-06
Step 3077 | epoch: 2.964354527938343
Step 3078 | loss: 2.9848201274871826
Step 3078 | grad_norm: 2.6679999828338623
Step 3078 | learning_rate: 1.1881824020552344e-06
Step 3078 | epoch: 2.9653179190751446
Step 3079 | loss: 3.3575525283813477
Step 3079 | grad_norm: 3.4602649211883545
Step 3079 | learning_rate: 1.1560693641618497e-06
Step 3079 | epoch: 2.9662813102119463
Step 3080 | loss: 3.4864165782928467
Step 3080 | grad_norm: 3.6132209300994873
Step 3080 | learning_rate: 1.123956326268465e-06
Step 3080 | epoch: 2.9672447013487475
Step 3081 | loss: 3.7797396183013916
Step 3081 | grad_norm: 3.633242130279541
Step 3081 | learning_rate: 1.0918432883750804e-06
Step 3081 | epoch: 2.968208092485549
Step 3082 | loss: 3.0189168453216553
Step 3082 | grad_norm: 2.4061577320098877
Step 3082 | learning_rate: 1.0597302504816956e-06
Step 3082 | epoch: 2.969171483622351
Step 3083 | loss: 3.8702492713928223
Step 3083 | grad_norm: 4.6520538330078125
Step 3083 | learning_rate: 1.0276172125883108e-06
Step 3083 | epoch: 2.970134874759152
Step 3084 | loss: 2.8724238872528076
Step 3084 | grad_norm: 3.1614885330200195
Step 3084 | learning_rate: 9.95504174694926e-07
Step 3084 | epoch: 2.9710982658959537
Step 3085 | loss: 3.111027956008911
Step 3085 | grad_norm: 2.34899640083313
Step 3085 | learning_rate: 9.633911368015413e-07
Step 3085 | epoch: 2.9720616570327554
Step 3086 | loss: 2.916105031967163
Step 3086 | grad_norm: 2.247019052505493
Step 3086 | learning_rate: 9.312780989081567e-07
Step 3086 | epoch: 2.973025048169557
Step 3087 | loss: 2.6965584754943848
Step 3087 | grad_norm: 3.24747371673584
Step 3087 | learning_rate: 8.991650610147719e-07
Step 3087 | epoch: 2.9739884393063583
Step 3088 | loss: 2.3406739234924316
Step 3088 | grad_norm: 2.5463807582855225
Step 3088 | learning_rate: 8.670520231213873e-07
Step 3088 | epoch: 2.97495183044316
Step 3089 | loss: 3.1388185024261475
Step 3089 | grad_norm: 3.492725372314453
Step 3089 | learning_rate: 8.349389852280026e-07
Step 3089 | epoch: 2.9759152215799616
Step 3090 | loss: 2.9213902950286865
Step 3090 | grad_norm: 2.453752040863037
Step 3090 | learning_rate: 8.02825947334618e-07
Step 3090 | epoch: 2.976878612716763
Step 3091 | loss: 2.973939895629883
Step 3091 | grad_norm: 4.044888973236084
Step 3091 | learning_rate: 7.707129094412331e-07
Step 3091 | epoch: 2.9778420038535645
Step 3092 | loss: 3.632786512374878
Step 3092 | grad_norm: 3.202683687210083
Step 3092 | learning_rate: 7.385998715478485e-07
Step 3092 | epoch: 2.978805394990366
Step 3093 | loss: 3.5360167026519775
Step 3093 | grad_norm: 2.6007373332977295
Step 3093 | learning_rate: 7.064868336544637e-07
Step 3093 | epoch: 2.979768786127168
Step 3094 | loss: 3.7695248126983643
Step 3094 | grad_norm: 3.1419572830200195
Step 3094 | learning_rate: 6.74373795761079e-07
Step 3094 | epoch: 2.9807321772639694
Step 3095 | loss: 3.3450889587402344
Step 3095 | grad_norm: 3.3795745372772217
Step 3095 | learning_rate: 6.422607578676943e-07
Step 3095 | epoch: 2.9816955684007707
Step 3096 | loss: 4.647597789764404
Step 3096 | grad_norm: 3.1383049488067627
Step 3096 | learning_rate: 6.101477199743096e-07
Step 3096 | epoch: 2.9826589595375723
Step 3097 | loss: 3.252340078353882
Step 3097 | grad_norm: 2.9763131141662598
Step 3097 | learning_rate: 5.780346820809248e-07
Step 3097 | epoch: 2.9836223506743735
Step 3098 | loss: 2.9018561840057373
Step 3098 | grad_norm: 2.5530385971069336
Step 3098 | learning_rate: 5.459216441875402e-07
Step 3098 | epoch: 2.984585741811175
Step 3099 | loss: 3.514037847518921
Step 3099 | grad_norm: 3.499408483505249
Step 3099 | learning_rate: 5.138086062941554e-07
Step 3099 | epoch: 2.985549132947977
Step 3100 | loss: 3.1705846786499023
Step 3100 | grad_norm: 3.9242846965789795
Step 3100 | learning_rate: 4.816955684007707e-07
Step 3100 | epoch: 2.9865125240847785
Step 3101 | loss: 3.091825008392334
Step 3101 | grad_norm: 2.5072784423828125
Step 3101 | learning_rate: 4.4958253050738597e-07
Step 3101 | epoch: 2.98747591522158
Step 3102 | loss: 3.0479636192321777
Step 3102 | grad_norm: 2.3873069286346436
Step 3102 | learning_rate: 4.174694926140013e-07
Step 3102 | epoch: 2.9884393063583814
Step 3103 | loss: 3.6203527450561523
Step 3103 | grad_norm: 2.9750919342041016
Step 3103 | learning_rate: 3.8535645472061657e-07
Step 3103 | epoch: 2.989402697495183
Step 3104 | loss: 2.492757797241211
Step 3104 | grad_norm: 2.6162526607513428
Step 3104 | learning_rate: 3.5324341682723187e-07
Step 3104 | epoch: 2.9903660886319847
Step 3105 | loss: 3.000128746032715
Step 3105 | grad_norm: 2.5863051414489746
Step 3105 | learning_rate: 3.2113037893384717e-07
Step 3105 | epoch: 2.991329479768786
Step 3106 | loss: 2.8598082065582275
Step 3106 | grad_norm: 2.5343451499938965
Step 3106 | learning_rate: 2.890173410404624e-07
Step 3106 | epoch: 2.9922928709055876
Step 3107 | loss: 2.9754152297973633
Step 3107 | grad_norm: 3.0648419857025146
Step 3107 | learning_rate: 2.569043031470777e-07
Step 3107 | epoch: 2.9932562620423893
Step 3108 | loss: 3.265674591064453
Step 3108 | grad_norm: 2.9762721061706543
Step 3108 | learning_rate: 2.2479126525369298e-07
Step 3108 | epoch: 2.994219653179191
Step 3109 | loss: 3.574354410171509
Step 3109 | grad_norm: 3.2277884483337402
Step 3109 | learning_rate: 1.9267822736030828e-07
Step 3109 | epoch: 2.995183044315992
Step 3110 | loss: 3.0971741676330566
Step 3110 | grad_norm: 2.7876861095428467
Step 3110 | learning_rate: 1.6056518946692358e-07
Step 3110 | epoch: 2.996146435452794
Step 3111 | loss: 3.168775796890259
Step 3111 | grad_norm: 3.0975046157836914
Step 3111 | learning_rate: 1.2845215157353886e-07
Step 3111 | epoch: 2.9971098265895955
Step 3112 | loss: 3.0027313232421875
Step 3112 | grad_norm: 2.815920829772949
Step 3112 | learning_rate: 9.633911368015414e-08
Step 3112 | epoch: 2.9980732177263967
Step 3113 | loss: 3.089576482772827
Step 3113 | grad_norm: 2.6368653774261475
Step 3113 | learning_rate: 6.422607578676943e-08
Step 3113 | epoch: 2.9990366088631983
Step 3114 | loss: 2.8052499294281006
Step 3114 | grad_norm: 3.381890058517456
Step 3114 | learning_rate: 3.2113037893384714e-08
Step 3114 | epoch: 3.0
Step 3114 | train_runtime: 147.0505
Step 3114 | train_samples_per_second: 42.332
Step 3114 | train_steps_per_second: 21.176
Step 3114 | total_flos: 133251892236288.0
Step 3114 | train_loss: 3.3794175933091855
Step 3114 | epoch: 3.0