Step 10 | loss: 2.6090566635131838
Step 10 | grad_norm: 0.9357250332832336
Step 10 | learning_rate: 9.884615384615386e-05
Step 10 | epoch: 0.038535645472061654
Step 20 | loss: 2.2587814331054688
Step 20 | grad_norm: 1.1202237606048584
Step 20 | learning_rate: 9.756410256410257e-05
Step 20 | epoch: 0.07707129094412331
Step 30 | loss: 1.9902860641479492
Step 30 | grad_norm: 1.2067936658859253
Step 30 | learning_rate: 9.628205128205129e-05
Step 30 | epoch: 0.11560693641618497
Step 40 | loss: 1.6780923843383788
Step 40 | grad_norm: 1.5112605094909668
Step 40 | learning_rate: 9.5e-05
Step 40 | epoch: 0.15414258188824662
Step 50 | loss: 1.7640592575073242
Step 50 | grad_norm: 1.302623987197876
Step 50 | learning_rate: 9.371794871794872e-05
Step 50 | epoch: 0.1926782273603083
Step 60 | loss: 1.6195392608642578
Step 60 | grad_norm: 1.2638047933578491
Step 60 | learning_rate: 9.243589743589745e-05
Step 60 | epoch: 0.23121387283236994
Step 70 | loss: 1.6386308670043945
Step 70 | grad_norm: 1.242840051651001
Step 70 | learning_rate: 9.115384615384615e-05
Step 70 | epoch: 0.2697495183044316
Step 80 | loss: 1.6728851318359375
Step 80 | grad_norm: 1.546386480331421
Step 80 | learning_rate: 8.987179487179488e-05
Step 80 | epoch: 0.30828516377649323
Step 90 | loss: 1.5661144256591797
Step 90 | grad_norm: 1.3090425729751587
Step 90 | learning_rate: 8.858974358974359e-05
Step 90 | epoch: 0.3468208092485549
Step 100 | loss: 1.5834539413452149
Step 100 | grad_norm: 1.3920080661773682
Step 100 | learning_rate: 8.730769230769232e-05
Step 100 | epoch: 0.3853564547206166
Step 110 | loss: 1.7036815643310548
Step 110 | grad_norm: 1.3653217554092407
Step 110 | learning_rate: 8.602564102564103e-05
Step 110 | epoch: 0.4238921001926782
Step 120 | loss: 1.669204330444336
Step 120 | grad_norm: 1.30878484249115
Step 120 | learning_rate: 8.474358974358975e-05
Step 120 | epoch: 0.4624277456647399
Step 130 | loss: 1.7299932479858398
Step 130 | grad_norm: 1.8817404508590698
Step 130 | learning_rate: 8.346153846153847e-05
Step 130 | epoch: 0.5009633911368016
Step 140 | loss: 1.6830154418945313
Step 140 | grad_norm: 1.474470615386963
Step 140 | learning_rate: 8.217948717948718e-05
Step 140 | epoch: 0.5394990366088632
Step 150 | loss: 1.570826244354248
Step 150 | grad_norm: 1.2774745225906372
Step 150 | learning_rate: 8.08974358974359e-05
Step 150 | epoch: 0.5780346820809249
Step 160 | loss: 1.6428447723388673
Step 160 | grad_norm: 1.250479817390442
Step 160 | learning_rate: 7.961538461538461e-05
Step 160 | epoch: 0.6165703275529865
Step 170 | loss: 1.6515069961547852
Step 170 | grad_norm: 1.4045085906982422
Step 170 | learning_rate: 7.833333333333333e-05
Step 170 | epoch: 0.6551059730250481
Step 180 | loss: 1.6107986450195313
Step 180 | grad_norm: 1.412085771560669
Step 180 | learning_rate: 7.705128205128206e-05
Step 180 | epoch: 0.6936416184971098
Step 190 | loss: 1.541624164581299
Step 190 | grad_norm: 1.3120790719985962
Step 190 | learning_rate: 7.576923076923076e-05
Step 190 | epoch: 0.7321772639691715
Step 200 | loss: 1.6302114486694337
Step 200 | grad_norm: 1.1927733421325684
Step 200 | learning_rate: 7.44871794871795e-05
Step 200 | epoch: 0.7707129094412332
Step 210 | loss: 1.5521100997924804
Step 210 | grad_norm: 1.6493637561798096
Step 210 | learning_rate: 7.320512820512821e-05
Step 210 | epoch: 0.8092485549132948
Step 220 | loss: 1.548417854309082
Step 220 | grad_norm: 1.5305064916610718
Step 220 | learning_rate: 7.192307692307693e-05
Step 220 | epoch: 0.8477842003853564
Step 230 | loss: 1.5702698707580567
Step 230 | grad_norm: 1.6732368469238281
Step 230 | learning_rate: 7.064102564102564e-05
Step 230 | epoch: 0.8863198458574181
Step 240 | loss: 1.7515304565429688
Step 240 | grad_norm: 1.355486512184143
Step 240 | learning_rate: 6.935897435897436e-05
Step 240 | epoch: 0.9248554913294798
Step 250 | loss: 1.5804686546325684
Step 250 | grad_norm: 1.3879016637802124
Step 250 | learning_rate: 6.807692307692309e-05
Step 250 | epoch: 0.9633911368015414
Step 260 | loss: 1.5328250885009767
Step 260 | grad_norm: 2.681976318359375
Step 260 | learning_rate: 6.679487179487179e-05
Step 260 | epoch: 1.0
Step 270 | loss: 1.4894627571105956
Step 270 | grad_norm: 1.5499629974365234
Step 270 | learning_rate: 6.551282051282052e-05
Step 270 | epoch: 1.0385356454720616
Step 280 | loss: 1.5793493270874024
Step 280 | grad_norm: 1.713823914527893
Step 280 | learning_rate: 6.423076923076924e-05
Step 280 | epoch: 1.0770712909441233
Step 290 | loss: 1.4261491775512696
Step 290 | grad_norm: 1.6094791889190674
Step 290 | learning_rate: 6.294871794871795e-05
Step 290 | epoch: 1.115606936416185
Step 300 | loss: 1.5787789344787597
Step 300 | grad_norm: 1.4517735242843628
Step 300 | learning_rate: 6.166666666666667e-05
Step 300 | epoch: 1.1541425818882467
Step 310 | loss: 1.5047268867492676
Step 310 | grad_norm: 1.6210904121398926
Step 310 | learning_rate: 6.038461538461539e-05
Step 310 | epoch: 1.1926782273603083
Step 320 | loss: 1.4535527229309082
Step 320 | grad_norm: 1.5947999954223633
Step 320 | learning_rate: 5.910256410256411e-05
Step 320 | epoch: 1.2312138728323698
Step 330 | loss: 1.5406031608581543
Step 330 | grad_norm: 1.563832402229309
Step 330 | learning_rate: 5.7820512820512826e-05
Step 330 | epoch: 1.2697495183044316
Step 340 | loss: 1.5154228210449219
Step 340 | grad_norm: 1.498734712600708
Step 340 | learning_rate: 5.653846153846154e-05
Step 340 | epoch: 1.3082851637764932
Step 350 | loss: 1.6160003662109375
Step 350 | grad_norm: 1.5128730535507202
Step 350 | learning_rate: 5.5256410256410265e-05
Step 350 | epoch: 1.346820809248555
Step 360 | loss: 1.4681922912597656
Step 360 | grad_norm: 1.658103585243225
Step 360 | learning_rate: 5.3974358974358975e-05
Step 360 | epoch: 1.3853564547206165
Step 370 | loss: 1.5578727722167969
Step 370 | grad_norm: 1.702661156654358
Step 370 | learning_rate: 5.26923076923077e-05
Step 370 | epoch: 1.423892100192678
Step 380 | loss: 1.5402959823608398
Step 380 | grad_norm: 1.5776622295379639
Step 380 | learning_rate: 5.141025641025641e-05
Step 380 | epoch: 1.4624277456647399
Step 390 | loss: 1.5185512542724608
Step 390 | grad_norm: 1.5840805768966675
Step 390 | learning_rate: 5.012820512820513e-05
Step 390 | epoch: 1.5009633911368017
Step 400 | loss: 1.5909567832946778
Step 400 | grad_norm: 1.9050321578979492
Step 400 | learning_rate: 4.884615384615385e-05
Step 400 | epoch: 1.5394990366088632
Step 410 | loss: 1.522993278503418
Step 410 | grad_norm: 1.6868983507156372
Step 410 | learning_rate: 4.7564102564102563e-05
Step 410 | epoch: 1.5780346820809248
Step 420 | loss: 1.4821704864501952
Step 420 | grad_norm: 1.6862238645553589
Step 420 | learning_rate: 4.6282051282051287e-05
Step 420 | epoch: 1.6165703275529864
Step 430 | loss: 1.549382209777832
Step 430 | grad_norm: 1.6182574033737183
Step 430 | learning_rate: 4.5e-05
Step 430 | epoch: 1.6551059730250481
Step 440 | loss: 1.5105249404907226
Step 440 | grad_norm: 1.627803087234497
Step 440 | learning_rate: 4.371794871794872e-05
Step 440 | epoch: 1.69364161849711
Step 450 | loss: 1.6328195571899413
Step 450 | grad_norm: 1.8033710718154907
Step 450 | learning_rate: 4.2435897435897435e-05
Step 450 | epoch: 1.7321772639691715
Step 460 | loss: 1.5783759117126466
Step 460 | grad_norm: 1.6591383218765259
Step 460 | learning_rate: 4.115384615384615e-05
Step 460 | epoch: 1.770712909441233
Step 470 | loss: 1.5131380081176757
Step 470 | grad_norm: 1.8148468732833862
Step 470 | learning_rate: 3.9871794871794875e-05
Step 470 | epoch: 1.8092485549132948
Step 480 | loss: 1.411104393005371
Step 480 | grad_norm: 1.6412150859832764
Step 480 | learning_rate: 3.858974358974359e-05
Step 480 | epoch: 1.8477842003853564
Step 490 | loss: 1.5412589073181153
Step 490 | grad_norm: 1.5697544813156128
Step 490 | learning_rate: 3.730769230769231e-05
Step 490 | epoch: 1.8863198458574182
Step 500 | loss: 1.492654037475586
Step 500 | grad_norm: 1.6153948307037354
Step 500 | learning_rate: 3.6025641025641024e-05
Step 500 | epoch: 1.9248554913294798
Step 510 | loss: 1.489748191833496
Step 510 | grad_norm: 1.7666462659835815
Step 510 | learning_rate: 3.474358974358975e-05
Step 510 | epoch: 1.9633911368015413
Step 520 | loss: 1.500459575653076
Step 520 | grad_norm: 3.05220103263855
Step 520 | learning_rate: 3.346153846153846e-05
Step 520 | epoch: 2.0
Step 530 | loss: 1.4378642082214355
Step 530 | grad_norm: 1.792405605316162
Step 530 | learning_rate: 3.2179487179487186e-05
Step 530 | epoch: 2.0385356454720616
Step 540 | loss: 1.5024273872375489
Step 540 | grad_norm: 1.6316912174224854
Step 540 | learning_rate: 3.08974358974359e-05
Step 540 | epoch: 2.077071290944123
Step 550 | loss: 1.3881282806396484
Step 550 | grad_norm: 1.7386904954910278
Step 550 | learning_rate: 2.9615384615384616e-05
Step 550 | epoch: 2.115606936416185
Step 560 | loss: 1.306655216217041
Step 560 | grad_norm: 1.7755272388458252
Step 560 | learning_rate: 2.8333333333333335e-05
Step 560 | epoch: 2.1541425818882467
Step 570 | loss: 1.4532172203063964
Step 570 | grad_norm: 1.7820621728897095
Step 570 | learning_rate: 2.705128205128205e-05
Step 570 | epoch: 2.1926782273603083
Step 580 | loss: 1.4038814544677733
Step 580 | grad_norm: 1.9165279865264893
Step 580 | learning_rate: 2.5769230769230768e-05
Step 580 | epoch: 2.23121387283237
Step 590 | loss: 1.4980217933654785
Step 590 | grad_norm: 2.0546441078186035
Step 590 | learning_rate: 2.4487179487179488e-05
Step 590 | epoch: 2.2697495183044314
Step 600 | loss: 1.3950376510620117
Step 600 | grad_norm: 1.8810155391693115
Step 600 | learning_rate: 2.3205128205128207e-05
Step 600 | epoch: 2.3082851637764934
Step 610 | loss: 1.4183855056762695
Step 610 | grad_norm: 2.058460235595703
Step 610 | learning_rate: 2.1923076923076924e-05
Step 610 | epoch: 2.346820809248555
Step 620 | loss: 1.4218508720397949
Step 620 | grad_norm: 2.1065170764923096
Step 620 | learning_rate: 2.064102564102564e-05
Step 620 | epoch: 2.3853564547206165
Step 630 | loss: 1.4466514587402344
Step 630 | grad_norm: 1.8116931915283203
Step 630 | learning_rate: 1.935897435897436e-05
Step 630 | epoch: 2.423892100192678
Step 640 | loss: 1.4633867263793945
Step 640 | grad_norm: 2.124143362045288
Step 640 | learning_rate: 1.8076923076923076e-05
Step 640 | epoch: 2.4624277456647397
Step 650 | loss: 1.4156046867370606
Step 650 | grad_norm: 2.264838933944702
Step 650 | learning_rate: 1.6794871794871796e-05
Step 650 | epoch: 2.5009633911368017
Step 660 | loss: 1.5232132911682128
Step 660 | grad_norm: 1.9283812046051025
Step 660 | learning_rate: 1.5512820512820516e-05
Step 660 | epoch: 2.5394990366088632
Step 670 | loss: 1.4528221130371093
Step 670 | grad_norm: 2.042257070541382
Step 670 | learning_rate: 1.423076923076923e-05
Step 670 | epoch: 2.578034682080925
Step 680 | loss: 1.420903491973877
Step 680 | grad_norm: 2.001962423324585
Step 680 | learning_rate: 1.294871794871795e-05
Step 680 | epoch: 2.6165703275529864
Step 690 | loss: 1.4422385215759277
Step 690 | grad_norm: 2.777780771255493
Step 690 | learning_rate: 1.1666666666666668e-05
Step 690 | epoch: 2.655105973025048
Step 700 | loss: 1.4163264274597167
Step 700 | grad_norm: 1.9413340091705322
Step 700 | learning_rate: 1.0384615384615386e-05
Step 700 | epoch: 2.69364161849711
Step 710 | loss: 1.499219036102295
Step 710 | grad_norm: 1.9844121932983398
Step 710 | learning_rate: 9.102564102564102e-06
Step 710 | epoch: 2.7321772639691715
Step 720 | loss: 1.545828151702881
Step 720 | grad_norm: 1.8481435775756836
Step 720 | learning_rate: 7.82051282051282e-06
Step 720 | epoch: 2.770712909441233
Step 730 | loss: 1.5056784629821778
Step 730 | grad_norm: 1.8615413904190063
Step 730 | learning_rate: 6.538461538461539e-06
Step 730 | epoch: 2.809248554913295
Step 740 | loss: 1.5514805793762207
Step 740 | grad_norm: 1.7320008277893066
Step 740 | learning_rate: 5.256410256410257e-06
Step 740 | epoch: 2.847784200385356
Step 750 | loss: 1.5467754364013673
Step 750 | grad_norm: 1.7429587841033936
Step 750 | learning_rate: 3.974358974358974e-06
Step 750 | epoch: 2.886319845857418
Step 760 | loss: 1.4251480102539062
Step 760 | grad_norm: 1.9793161153793335
Step 760 | learning_rate: 2.6923076923076928e-06
Step 760 | epoch: 2.9248554913294798
Step 770 | loss: 1.4558327674865723
Step 770 | grad_norm: 2.023176908493042
Step 770 | learning_rate: 1.4102564102564104e-06
Step 770 | epoch: 2.9633911368015413
Step 780 | loss: 1.4808839797973632
Step 780 | grad_norm: 3.156968355178833
Step 780 | learning_rate: 1.282051282051282e-07
Step 780 | epoch: 3.0
Step 780 | train_runtime: 2320.654
Step 780 | train_samples_per_second: 2.682
Step 780 | train_steps_per_second: 0.336
Step 780 | total_flos: 4143261681377280.0
Step 780 | train_loss: 1.5611825099358192
Step 780 | epoch: 3.0