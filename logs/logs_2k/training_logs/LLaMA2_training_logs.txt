Step 10 | loss: 2.206424331665039
Step 10 | grad_norm: 0.8103867769241333
Step 10 | learning_rate: 9.884615384615386e-05
Step 10 | epoch: 0.03855421686746988
Step 20 | loss: 1.6923515319824218
Step 20 | grad_norm: 1.2830700874328613
Step 20 | learning_rate: 9.756410256410257e-05
Step 20 | epoch: 0.07710843373493977
Step 30 | loss: 1.4475696563720704
Step 30 | grad_norm: 1.5703837871551514
Step 30 | learning_rate: 9.628205128205129e-05
Step 30 | epoch: 0.11566265060240964
Step 40 | loss: 1.2991622924804687
Step 40 | grad_norm: 1.5081181526184082
Step 40 | learning_rate: 9.5e-05
Step 40 | epoch: 0.15421686746987953
Step 50 | loss: 1.310050106048584
Step 50 | grad_norm: 1.187841534614563
Step 50 | learning_rate: 9.371794871794872e-05
Step 50 | epoch: 0.1927710843373494
Step 60 | loss: 1.2352224349975587
Step 60 | grad_norm: 1.2536088228225708
Step 60 | learning_rate: 9.243589743589745e-05
Step 60 | epoch: 0.23132530120481928
Step 70 | loss: 1.254198932647705
Step 70 | grad_norm: 1.21150803565979
Step 70 | learning_rate: 9.115384615384615e-05
Step 70 | epoch: 0.26987951807228916
Step 80 | loss: 1.258408260345459
Step 80 | grad_norm: 1.2267637252807617
Step 80 | learning_rate: 8.987179487179488e-05
Step 80 | epoch: 0.30843373493975906
Step 90 | loss: 1.2570658683776856
Step 90 | grad_norm: 1.4275139570236206
Step 90 | learning_rate: 8.858974358974359e-05
Step 90 | epoch: 0.3469879518072289
Step 100 | loss: 1.2042497634887694
Step 100 | grad_norm: 1.7414021492004395
Step 100 | learning_rate: 8.730769230769232e-05
Step 100 | epoch: 0.3855421686746988
Step 110 | loss: 1.2999011993408203
Step 110 | grad_norm: 1.438689947128296
Step 110 | learning_rate: 8.602564102564103e-05
Step 110 | epoch: 0.42409638554216866
Step 120 | loss: 1.259638500213623
Step 120 | grad_norm: 1.3796145915985107
Step 120 | learning_rate: 8.474358974358975e-05
Step 120 | epoch: 0.46265060240963857
Step 130 | loss: 1.2520917892456054
Step 130 | grad_norm: 1.970658302307129
Step 130 | learning_rate: 8.346153846153847e-05
Step 130 | epoch: 0.5012048192771085
Step 140 | loss: 1.2742610931396485
Step 140 | grad_norm: 1.6161645650863647
Step 140 | learning_rate: 8.217948717948718e-05
Step 140 | epoch: 0.5397590361445783
Step 150 | loss: 1.2269189834594727
Step 150 | grad_norm: 1.7148284912109375
Step 150 | learning_rate: 8.08974358974359e-05
Step 150 | epoch: 0.5783132530120482
Step 160 | loss: 1.1999335289001465
Step 160 | grad_norm: 1.2778469324111938
Step 160 | learning_rate: 7.961538461538461e-05
Step 160 | epoch: 0.6168674698795181
Step 170 | loss: 1.2277901649475098
Step 170 | grad_norm: 1.566985845565796
Step 170 | learning_rate: 7.833333333333333e-05
Step 170 | epoch: 0.655421686746988
Step 180 | loss: 1.226875114440918
Step 180 | grad_norm: 1.813415288925171
Step 180 | learning_rate: 7.705128205128206e-05
Step 180 | epoch: 0.6939759036144578
Step 190 | loss: 1.1887116432189941
Step 190 | grad_norm: 1.5704150199890137
Step 190 | learning_rate: 7.576923076923076e-05
Step 190 | epoch: 0.7325301204819277
Step 200 | loss: 1.1818785667419434
Step 200 | grad_norm: 1.509177803993225
Step 200 | learning_rate: 7.44871794871795e-05
Step 200 | epoch: 0.7710843373493976
Step 210 | loss: 1.1969313621520996
Step 210 | grad_norm: 1.7796496152877808
Step 210 | learning_rate: 7.320512820512821e-05
Step 210 | epoch: 0.8096385542168675
Step 220 | loss: 1.1745793342590332
Step 220 | grad_norm: 1.4011621475219727
Step 220 | learning_rate: 7.192307692307693e-05
Step 220 | epoch: 0.8481927710843373
Step 230 | loss: 1.189006233215332
Step 230 | grad_norm: 1.5690159797668457
Step 230 | learning_rate: 7.064102564102564e-05
Step 230 | epoch: 0.8867469879518072
Step 240 | loss: 1.2564076423645019
Step 240 | grad_norm: 1.4212548732757568
Step 240 | learning_rate: 6.935897435897436e-05
Step 240 | epoch: 0.9253012048192771
Step 250 | loss: 1.2300897598266602
Step 250 | grad_norm: 1.5518550872802734
Step 250 | learning_rate: 6.807692307692309e-05
Step 250 | epoch: 0.963855421686747
Step 260 | loss: 1.1746654510498047
Step 260 | grad_norm: 2.7598814964294434
Step 260 | learning_rate: 6.679487179487179e-05
Step 260 | epoch: 1.0
Step 270 | loss: 1.0647375106811523
Step 270 | grad_norm: 1.620152473449707
Step 270 | learning_rate: 6.551282051282052e-05
Step 270 | epoch: 1.03855421686747
Step 280 | loss: 1.1080409049987794
Step 280 | grad_norm: 1.911185383796692
Step 280 | learning_rate: 6.423076923076924e-05
Step 280 | epoch: 1.0771084337349397
Step 290 | loss: 1.023460865020752
Step 290 | grad_norm: 1.6905708312988281
Step 290 | learning_rate: 6.294871794871795e-05
Step 290 | epoch: 1.1156626506024097
Step 300 | loss: 1.046984577178955
Step 300 | grad_norm: 1.691894292831421
Step 300 | learning_rate: 6.166666666666667e-05
Step 300 | epoch: 1.1542168674698796
Step 310 | loss: 1.0426555633544923
Step 310 | grad_norm: 1.8801101446151733
Step 310 | learning_rate: 6.038461538461539e-05
Step 310 | epoch: 1.1927710843373494
Step 320 | loss: 1.052673053741455
Step 320 | grad_norm: 1.8393099308013916
Step 320 | learning_rate: 5.910256410256411e-05
Step 320 | epoch: 1.2313253012048193
Step 330 | loss: 1.0738268852233888
Step 330 | grad_norm: 2.128718852996826
Step 330 | learning_rate: 5.7820512820512826e-05
Step 330 | epoch: 1.269879518072289
Step 340 | loss: 1.0628613471984862
Step 340 | grad_norm: 1.6093922853469849
Step 340 | learning_rate: 5.653846153846154e-05
Step 340 | epoch: 1.308433734939759
Step 350 | loss: 1.1197086334228517
Step 350 | grad_norm: 2.1149747371673584
Step 350 | learning_rate: 5.5256410256410265e-05
Step 350 | epoch: 1.346987951807229
Step 360 | loss: 1.031206226348877
Step 360 | grad_norm: 1.9091224670410156
Step 360 | learning_rate: 5.3974358974358975e-05
Step 360 | epoch: 1.3855421686746987
Step 370 | loss: 1.0622978210449219
Step 370 | grad_norm: 1.7304245233535767
Step 370 | learning_rate: 5.26923076923077e-05
Step 370 | epoch: 1.4240963855421687
Step 380 | loss: 1.1479658126831054
Step 380 | grad_norm: 1.8299593925476074
Step 380 | learning_rate: 5.141025641025641e-05
Step 380 | epoch: 1.4626506024096386
Step 390 | loss: 1.0605742454528808
Step 390 | grad_norm: 2.4232444763183594
Step 390 | learning_rate: 5.012820512820513e-05
Step 390 | epoch: 1.5012048192771084
Step 400 | loss: 1.0918140411376953
Step 400 | grad_norm: 2.283642292022705
Step 400 | learning_rate: 4.884615384615385e-05
Step 400 | epoch: 1.5397590361445783
Step 410 | loss: 1.0588128089904785
Step 410 | grad_norm: 2.288511037826538
Step 410 | learning_rate: 4.7564102564102563e-05
Step 410 | epoch: 1.5783132530120483
Step 420 | loss: 1.0408441543579101
Step 420 | grad_norm: 2.0167839527130127
Step 420 | learning_rate: 4.6282051282051287e-05
Step 420 | epoch: 1.616867469879518
Step 430 | loss: 1.0678521156311036
Step 430 | grad_norm: 2.2930655479431152
Step 430 | learning_rate: 4.5e-05
Step 430 | epoch: 1.655421686746988
Step 440 | loss: 1.0288664817810058
Step 440 | grad_norm: 2.3329246044158936
Step 440 | learning_rate: 4.371794871794872e-05
Step 440 | epoch: 1.693975903614458
Step 450 | loss: 1.1444024085998534
Step 450 | grad_norm: 2.3944671154022217
Step 450 | learning_rate: 4.2435897435897435e-05
Step 450 | epoch: 1.7325301204819277
Step 460 | loss: 1.132142925262451
Step 460 | grad_norm: 2.0383999347686768
Step 460 | learning_rate: 4.115384615384615e-05
Step 460 | epoch: 1.7710843373493976
Step 470 | loss: 1.0671188354492187
Step 470 | grad_norm: 2.3636817932128906
Step 470 | learning_rate: 3.9871794871794875e-05
Step 470 | epoch: 1.8096385542168676
Step 480 | loss: 0.9976268768310547
Step 480 | grad_norm: 1.91023588180542
Step 480 | learning_rate: 3.858974358974359e-05
Step 480 | epoch: 1.8481927710843373
Step 490 | loss: 1.0615139961242677
Step 490 | grad_norm: 2.745718240737915
Step 490 | learning_rate: 3.730769230769231e-05
Step 490 | epoch: 1.886746987951807
Step 500 | loss: 1.081441593170166
Step 500 | grad_norm: 1.9567755460739136
Step 500 | learning_rate: 3.6025641025641024e-05
Step 500 | epoch: 1.9253012048192772
Step 510 | loss: 1.057546043395996
Step 510 | grad_norm: 2.3823909759521484
Step 510 | learning_rate: 3.474358974358975e-05
Step 510 | epoch: 1.963855421686747
Step 520 | loss: 1.1085328102111816
Step 520 | grad_norm: 4.285077095031738
Step 520 | learning_rate: 3.346153846153846e-05
Step 520 | epoch: 2.0
Step 530 | loss: 0.9747322082519532
Step 530 | grad_norm: 2.504973888397217
Step 530 | learning_rate: 3.2179487179487186e-05
Step 530 | epoch: 2.0385542168674697
Step 540 | loss: 0.9789397239685058
Step 540 | grad_norm: 2.1541101932525635
Step 540 | learning_rate: 3.08974358974359e-05
Step 540 | epoch: 2.07710843373494
Step 550 | loss: 0.9032785415649414
Step 550 | grad_norm: 2.2368173599243164
Step 550 | learning_rate: 2.9615384615384616e-05
Step 550 | epoch: 2.1156626506024097
Step 560 | loss: 0.8709904670715332
Step 560 | grad_norm: 2.3730924129486084
Step 560 | learning_rate: 2.8333333333333335e-05
Step 560 | epoch: 2.1542168674698794
Step 570 | loss: 0.9706100463867188
Step 570 | grad_norm: 2.2250819206237793
Step 570 | learning_rate: 2.705128205128205e-05
Step 570 | epoch: 2.1927710843373496
Step 580 | loss: 0.895451831817627
Step 580 | grad_norm: 2.5058281421661377
Step 580 | learning_rate: 2.5769230769230768e-05
Step 580 | epoch: 2.2313253012048193
Step 590 | loss: 0.9669248580932617
Step 590 | grad_norm: 2.9176580905914307
Step 590 | learning_rate: 2.4487179487179488e-05
Step 590 | epoch: 2.269879518072289
Step 600 | loss: 0.9130523681640625
Step 600 | grad_norm: 2.2186286449432373
Step 600 | learning_rate: 2.3205128205128207e-05
Step 600 | epoch: 2.3084337349397592
Step 610 | loss: 0.9544654846191406
Step 610 | grad_norm: 2.9807698726654053
Step 610 | learning_rate: 2.1923076923076924e-05
Step 610 | epoch: 2.346987951807229
Step 620 | loss: 0.9341609954833985
Step 620 | grad_norm: 2.4100472927093506
Step 620 | learning_rate: 2.064102564102564e-05
Step 620 | epoch: 2.3855421686746987
Step 630 | loss: 0.8900781631469726
Step 630 | grad_norm: 2.3849048614501953
Step 630 | learning_rate: 1.935897435897436e-05
Step 630 | epoch: 2.4240963855421684
Step 640 | loss: 0.9958630561828613
Step 640 | grad_norm: 2.9303388595581055
Step 640 | learning_rate: 1.8076923076923076e-05
Step 640 | epoch: 2.4626506024096386
Step 650 | loss: 0.8918272972106933
Step 650 | grad_norm: 2.649996280670166
Step 650 | learning_rate: 1.6794871794871796e-05
Step 650 | epoch: 2.5012048192771084
Step 660 | loss: 0.9180926322937012
Step 660 | grad_norm: 2.9686031341552734
Step 660 | learning_rate: 1.5512820512820516e-05
Step 660 | epoch: 2.539759036144578
Step 670 | loss: 0.9948642730712891
Step 670 | grad_norm: 3.0772922039031982
Step 670 | learning_rate: 1.423076923076923e-05
Step 670 | epoch: 2.5783132530120483
Step 680 | loss: 0.8896489143371582
Step 680 | grad_norm: 2.2824926376342773
Step 680 | learning_rate: 1.294871794871795e-05
Step 680 | epoch: 2.616867469879518
Step 690 | loss: 0.9230381965637207
Step 690 | grad_norm: 2.6105332374572754
Step 690 | learning_rate: 1.1666666666666668e-05
Step 690 | epoch: 2.6554216867469878
Step 700 | loss: 0.916086483001709
Step 700 | grad_norm: 2.682384967803955
Step 700 | learning_rate: 1.0384615384615386e-05
Step 700 | epoch: 2.693975903614458
Step 710 | loss: 0.9240215301513672
Step 710 | grad_norm: 2.7457423210144043
Step 710 | learning_rate: 9.102564102564102e-06
Step 710 | epoch: 2.7325301204819277
Step 720 | loss: 1.0093597412109374
Step 720 | grad_norm: 2.4213507175445557
Step 720 | learning_rate: 7.82051282051282e-06
Step 720 | epoch: 2.7710843373493974
Step 730 | loss: 0.9537474632263183
Step 730 | grad_norm: 2.85103178024292
Step 730 | learning_rate: 6.538461538461539e-06
Step 730 | epoch: 2.8096385542168676
Step 740 | loss: 0.9878239631652832
Step 740 | grad_norm: 2.5199954509735107
Step 740 | learning_rate: 5.256410256410257e-06
Step 740 | epoch: 2.8481927710843373
Step 750 | loss: 0.9534003257751464
Step 750 | grad_norm: 2.360656261444092
Step 750 | learning_rate: 3.974358974358974e-06
Step 750 | epoch: 2.886746987951807
Step 760 | loss: 0.9381462097167969
Step 760 | grad_norm: 2.97578501701355
Step 760 | learning_rate: 2.6923076923076928e-06
Step 760 | epoch: 2.9253012048192772
Step 770 | loss: 0.9661883354187012
Step 770 | grad_norm: 2.7903425693511963
Step 770 | learning_rate: 1.4102564102564104e-06
Step 770 | epoch: 2.963855421686747
Step 780 | loss: 0.9832075119018555
Step 780 | grad_norm: 4.6395792961120605
Step 780 | learning_rate: 1.282051282051282e-07
Step 780 | epoch: 3.0
Step 780 | train_runtime: 1290.9288
Step 780 | train_samples_per_second: 4.822
Step 780 | train_steps_per_second: 0.604
Step 780 | total_flos: 1.0710817544798208e+16
Step 780 | train_loss: 1.103306316718077
Step 780 | epoch: 3.0