Step 1 | loss: 1.8663930892944336
Step 1 | grad_norm: 2.5483145713806152
Step 1 | learning_rate: 0.0001
Step 1 | epoch: 0.0038554216867469878
Step 2 | loss: 1.863089680671692
Step 2 | grad_norm: 2.534724473953247
Step 2 | learning_rate: 9.987179487179488e-05
Step 2 | epoch: 0.0077108433734939755
Step 3 | loss: 1.8462257385253906
Step 3 | grad_norm: 2.8631911277770996
Step 3 | learning_rate: 9.974358974358975e-05
Step 3 | epoch: 0.011566265060240964
Step 4 | loss: 1.984988808631897
Step 4 | grad_norm: 4.148845672607422
Step 4 | learning_rate: 9.961538461538463e-05
Step 4 | epoch: 0.015421686746987951
Step 5 | loss: 1.3679523468017578
Step 5 | grad_norm: 3.2478713989257812
Step 5 | learning_rate: 9.948717948717949e-05
Step 5 | epoch: 0.01927710843373494
Step 6 | loss: 1.5671250820159912
Step 6 | grad_norm: 7.204312324523926
Step 6 | learning_rate: 9.935897435897437e-05
Step 6 | epoch: 0.02313253012048193
Step 7 | loss: 1.5068174600601196
Step 7 | grad_norm: 3.7189688682556152
Step 7 | learning_rate: 9.923076923076923e-05
Step 7 | epoch: 0.026987951807228915
Step 8 | loss: 1.2745492458343506
Step 8 | grad_norm: 3.3182156085968018
Step 8 | learning_rate: 9.910256410256411e-05
Step 8 | epoch: 0.030843373493975902
Step 9 | loss: 1.495140552520752
Step 9 | grad_norm: 4.757673263549805
Step 9 | learning_rate: 9.897435897435898e-05
Step 9 | epoch: 0.03469879518072289
Step 10 | loss: 1.3139151334762573
Step 10 | grad_norm: 5.004037380218506
Step 10 | learning_rate: 9.884615384615386e-05
Step 10 | epoch: 0.03855421686746988
Step 11 | loss: 1.5640233755111694
Step 11 | grad_norm: 5.448366165161133
Step 11 | learning_rate: 9.871794871794872e-05
Step 11 | epoch: 0.042409638554216866
Step 12 | loss: 1.2447729110717773
Step 12 | grad_norm: 5.06779670715332
Step 12 | learning_rate: 9.85897435897436e-05
Step 12 | epoch: 0.04626506024096386
Step 13 | loss: 1.484753131866455
Step 13 | grad_norm: 4.5868611335754395
Step 13 | learning_rate: 9.846153846153848e-05
Step 13 | epoch: 0.05012048192771084
Step 14 | loss: 1.2423263788223267
Step 14 | grad_norm: 5.372990608215332
Step 14 | learning_rate: 9.833333333333333e-05
Step 14 | epoch: 0.05397590361445783
Step 15 | loss: 1.213044285774231
Step 15 | grad_norm: 3.9955391883850098
Step 15 | learning_rate: 9.820512820512821e-05
Step 15 | epoch: 0.05783132530120482
Step 16 | loss: 1.2384930849075317
Step 16 | grad_norm: 4.151424884796143
Step 16 | learning_rate: 9.807692307692307e-05
Step 16 | epoch: 0.061686746987951804
Step 17 | loss: 1.2592164278030396
Step 17 | grad_norm: 3.9038901329040527
Step 17 | learning_rate: 9.794871794871795e-05
Step 17 | epoch: 0.0655421686746988
Step 18 | loss: 1.03211510181427
Step 18 | grad_norm: 3.555330276489258
Step 18 | learning_rate: 9.782051282051282e-05
Step 18 | epoch: 0.06939759036144579
Step 19 | loss: 1.2331302165985107
Step 19 | grad_norm: 3.935448169708252
Step 19 | learning_rate: 9.76923076923077e-05
Step 19 | epoch: 0.07325301204819278
Step 20 | loss: 1.2173383235931396
Step 20 | grad_norm: 3.7736656665802
Step 20 | learning_rate: 9.756410256410257e-05
Step 20 | epoch: 0.07710843373493977
Step 21 | loss: 1.1025359630584717
Step 21 | grad_norm: 3.4250988960266113
Step 21 | learning_rate: 9.743589743589744e-05
Step 21 | epoch: 0.08096385542168674
Step 22 | loss: 1.3002897500991821
Step 22 | grad_norm: 3.5340147018432617
Step 22 | learning_rate: 9.730769230769232e-05
Step 22 | epoch: 0.08481927710843373
Step 23 | loss: 0.9490368366241455
Step 23 | grad_norm: 3.246586322784424
Step 23 | learning_rate: 9.717948717948718e-05
Step 23 | epoch: 0.08867469879518072
Step 24 | loss: 1.315802812576294
Step 24 | grad_norm: 3.9686076641082764
Step 24 | learning_rate: 9.705128205128206e-05
Step 24 | epoch: 0.09253012048192771
Step 25 | loss: 1.1190948486328125
Step 25 | grad_norm: 4.118720531463623
Step 25 | learning_rate: 9.692307692307692e-05
Step 25 | epoch: 0.0963855421686747
Step 26 | loss: 1.143465518951416
Step 26 | grad_norm: 4.057266712188721
Step 26 | learning_rate: 9.67948717948718e-05
Step 26 | epoch: 0.10024096385542168
Step 27 | loss: 1.415612816810608
Step 27 | grad_norm: 4.0577802658081055
Step 27 | learning_rate: 9.666666666666667e-05
Step 27 | epoch: 0.10409638554216867
Step 28 | loss: 1.2566932439804077
Step 28 | grad_norm: 4.751467704772949
Step 28 | learning_rate: 9.653846153846155e-05
Step 28 | epoch: 0.10795180722891566
Step 29 | loss: 0.9706935882568359
Step 29 | grad_norm: 3.834317445755005
Step 29 | learning_rate: 9.641025641025641e-05
Step 29 | epoch: 0.11180722891566265
Step 30 | loss: 1.2808297872543335
Step 30 | grad_norm: 4.592613220214844
Step 30 | learning_rate: 9.628205128205129e-05
Step 30 | epoch: 0.11566265060240964
Step 31 | loss: 0.951763391494751
Step 31 | grad_norm: 3.7014129161834717
Step 31 | learning_rate: 9.615384615384617e-05
Step 31 | epoch: 0.11951807228915663
Step 32 | loss: 1.1869267225265503
Step 32 | grad_norm: 3.7198126316070557
Step 32 | learning_rate: 9.602564102564103e-05
Step 32 | epoch: 0.12337349397590361
Step 33 | loss: 1.0227880477905273
Step 33 | grad_norm: 4.5316619873046875
Step 33 | learning_rate: 9.589743589743591e-05
Step 33 | epoch: 0.1272289156626506
Step 34 | loss: 1.163662314414978
Step 34 | grad_norm: 3.470564603805542
Step 34 | learning_rate: 9.576923076923078e-05
Step 34 | epoch: 0.1310843373493976
Step 35 | loss: 1.2393537759780884
Step 35 | grad_norm: 3.9441115856170654
Step 35 | learning_rate: 9.564102564102565e-05
Step 35 | epoch: 0.13493975903614458
Step 36 | loss: 0.9776601791381836
Step 36 | grad_norm: 3.4663171768188477
Step 36 | learning_rate: 9.551282051282052e-05
Step 36 | epoch: 0.13879518072289157
Step 37 | loss: 1.3074387311935425
Step 37 | grad_norm: 3.6097869873046875
Step 37 | learning_rate: 9.53846153846154e-05
Step 37 | epoch: 0.14265060240963856
Step 38 | loss: 0.9486372470855713
Step 38 | grad_norm: 3.1264491081237793
Step 38 | learning_rate: 9.525641025641026e-05
Step 38 | epoch: 0.14650602409638555
Step 39 | loss: 1.2070379257202148
Step 39 | grad_norm: 3.5617294311523438
Step 39 | learning_rate: 9.512820512820513e-05
Step 39 | epoch: 0.15036144578313254
Step 40 | loss: 1.2051622867584229
Step 40 | grad_norm: 4.370866298675537
Step 40 | learning_rate: 9.5e-05
Step 40 | epoch: 0.15421686746987953
Step 41 | loss: 1.1029571294784546
Step 41 | grad_norm: 3.5628836154937744
Step 41 | learning_rate: 9.487179487179487e-05
Step 41 | epoch: 0.1580722891566265
Step 42 | loss: 1.0178546905517578
Step 42 | grad_norm: 6.147338390350342
Step 42 | learning_rate: 9.474358974358975e-05
Step 42 | epoch: 0.16192771084337348
Step 43 | loss: 1.1712340116500854
Step 43 | grad_norm: 3.4649429321289062
Step 43 | learning_rate: 9.461538461538461e-05
Step 43 | epoch: 0.16578313253012048
Step 44 | loss: 1.090998649597168
Step 44 | grad_norm: 3.3864407539367676
Step 44 | learning_rate: 9.448717948717949e-05
Step 44 | epoch: 0.16963855421686747
Step 45 | loss: 1.093880534172058
Step 45 | grad_norm: 3.5498898029327393
Step 45 | learning_rate: 9.435897435897436e-05
Step 45 | epoch: 0.17349397590361446
Step 46 | loss: 1.1215572357177734
Step 46 | grad_norm: 3.6423637866973877
Step 46 | learning_rate: 9.423076923076924e-05
Step 46 | epoch: 0.17734939759036145
Step 47 | loss: 1.1924254894256592
Step 47 | grad_norm: 3.887535572052002
Step 47 | learning_rate: 9.41025641025641e-05
Step 47 | epoch: 0.18120481927710844
Step 48 | loss: 0.9356170892715454
Step 48 | grad_norm: 4.463481426239014
Step 48 | learning_rate: 9.397435897435898e-05
Step 48 | epoch: 0.18506024096385543
Step 49 | loss: 0.9231821298599243
Step 49 | grad_norm: 3.5400943756103516
Step 49 | learning_rate: 9.384615384615386e-05
Step 49 | epoch: 0.18891566265060242
Step 50 | loss: 1.0433977842330933
Step 50 | grad_norm: 3.591128349304199
Step 50 | learning_rate: 9.371794871794872e-05
Step 50 | epoch: 0.1927710843373494
Step 51 | loss: 1.2411446571350098
Step 51 | grad_norm: 6.200884819030762
Step 51 | learning_rate: 9.35897435897436e-05
Step 51 | epoch: 0.1966265060240964
Step 52 | loss: 1.0894379615783691
Step 52 | grad_norm: 4.565885543823242
Step 52 | learning_rate: 9.346153846153846e-05
Step 52 | epoch: 0.20048192771084336
Step 53 | loss: 1.1623435020446777
Step 53 | grad_norm: 5.3960981369018555
Step 53 | learning_rate: 9.333333333333334e-05
Step 53 | epoch: 0.20433734939759035
Step 54 | loss: 0.9933568239212036
Step 54 | grad_norm: 3.9685492515563965
Step 54 | learning_rate: 9.320512820512821e-05
Step 54 | epoch: 0.20819277108433734
Step 55 | loss: 1.0845171213150024
Step 55 | grad_norm: 4.892269134521484
Step 55 | learning_rate: 9.307692307692309e-05
Step 55 | epoch: 0.21204819277108433
Step 56 | loss: 1.0798832178115845
Step 56 | grad_norm: 6.6584577560424805
Step 56 | learning_rate: 9.294871794871795e-05
Step 56 | epoch: 0.21590361445783132
Step 57 | loss: 1.1397440433502197
Step 57 | grad_norm: 4.750858783721924
Step 57 | learning_rate: 9.282051282051283e-05
Step 57 | epoch: 0.2197590361445783
Step 58 | loss: 0.8956308364868164
Step 58 | grad_norm: 3.9597508907318115
Step 58 | learning_rate: 9.26923076923077e-05
Step 58 | epoch: 0.2236144578313253
Step 59 | loss: 1.0145759582519531
Step 59 | grad_norm: 4.184235095977783
Step 59 | learning_rate: 9.256410256410257e-05
Step 59 | epoch: 0.2274698795180723
Step 60 | loss: 1.02077317237854
Step 60 | grad_norm: 4.065821170806885
Step 60 | learning_rate: 9.243589743589745e-05
Step 60 | epoch: 0.23132530120481928
Step 61 | loss: 0.9965379238128662
Step 61 | grad_norm: 4.133733749389648
Step 61 | learning_rate: 9.230769230769232e-05
Step 61 | epoch: 0.23518072289156627
Step 62 | loss: 1.1157838106155396
Step 62 | grad_norm: 4.2201409339904785
Step 62 | learning_rate: 9.217948717948718e-05
Step 62 | epoch: 0.23903614457831326
Step 63 | loss: 1.2197027206420898
Step 63 | grad_norm: 3.9195122718811035
Step 63 | learning_rate: 9.205128205128205e-05
Step 63 | epoch: 0.24289156626506025
Step 64 | loss: 1.013511300086975
Step 64 | grad_norm: 4.016743183135986
Step 64 | learning_rate: 9.192307692307692e-05
Step 64 | epoch: 0.24674698795180722
Step 65 | loss: 1.0622429847717285
Step 65 | grad_norm: 3.783874750137329
Step 65 | learning_rate: 9.179487179487179e-05
Step 65 | epoch: 0.25060240963855424
Step 66 | loss: 1.1502790451049805
Step 66 | grad_norm: 3.9589192867279053
Step 66 | learning_rate: 9.166666666666667e-05
Step 66 | epoch: 0.2544578313253012
Step 67 | loss: 1.1714680194854736
Step 67 | grad_norm: 4.309979438781738
Step 67 | learning_rate: 9.153846153846155e-05
Step 67 | epoch: 0.2583132530120482
Step 68 | loss: 1.0629278421401978
Step 68 | grad_norm: 4.0597944259643555
Step 68 | learning_rate: 9.141025641025641e-05
Step 68 | epoch: 0.2621686746987952
Step 69 | loss: 1.0962189435958862
Step 69 | grad_norm: 5.37184476852417
Step 69 | learning_rate: 9.128205128205129e-05
Step 69 | epoch: 0.26602409638554214
Step 70 | loss: 0.9803637266159058
Step 70 | grad_norm: 4.396790504455566
Step 70 | learning_rate: 9.115384615384615e-05
Step 70 | epoch: 0.26987951807228916
Step 71 | loss: 1.0774013996124268
Step 71 | grad_norm: 4.001187324523926
Step 71 | learning_rate: 9.102564102564103e-05
Step 71 | epoch: 0.2737349397590361
Step 72 | loss: 1.0168527364730835
Step 72 | grad_norm: 4.799330234527588
Step 72 | learning_rate: 9.08974358974359e-05
Step 72 | epoch: 0.27759036144578314
Step 73 | loss: 1.1002416610717773
Step 73 | grad_norm: 4.781501293182373
Step 73 | learning_rate: 9.076923076923078e-05
Step 73 | epoch: 0.2814457831325301
Step 74 | loss: 1.204694151878357
Step 74 | grad_norm: 5.167848587036133
Step 74 | learning_rate: 9.064102564102564e-05
Step 74 | epoch: 0.2853012048192771
Step 75 | loss: 0.9778620600700378
Step 75 | grad_norm: 3.6725215911865234
Step 75 | learning_rate: 9.051282051282052e-05
Step 75 | epoch: 0.2891566265060241
Step 76 | loss: 1.093461036682129
Step 76 | grad_norm: 4.084905624389648
Step 76 | learning_rate: 9.038461538461538e-05
Step 76 | epoch: 0.2930120481927711
Step 77 | loss: 1.041213870048523
Step 77 | grad_norm: 3.1729893684387207
Step 77 | learning_rate: 9.025641025641026e-05
Step 77 | epoch: 0.29686746987951806
Step 78 | loss: 0.9259688258171082
Step 78 | grad_norm: 3.7341272830963135
Step 78 | learning_rate: 9.012820512820514e-05
Step 78 | epoch: 0.3007228915662651
Step 79 | loss: 1.0171713829040527
Step 79 | grad_norm: 3.432300567626953
Step 79 | learning_rate: 9e-05
Step 79 | epoch: 0.30457831325301205
Step 80 | loss: 0.9975510835647583
Step 80 | grad_norm: 3.3055551052093506
Step 80 | learning_rate: 8.987179487179488e-05
Step 80 | epoch: 0.30843373493975906
Step 81 | loss: 0.9618781805038452
Step 81 | grad_norm: 3.064969778060913
Step 81 | learning_rate: 8.974358974358975e-05
Step 81 | epoch: 0.312289156626506
Step 82 | loss: 0.9300782680511475
Step 82 | grad_norm: 3.9551336765289307
Step 82 | learning_rate: 8.961538461538463e-05
Step 82 | epoch: 0.316144578313253
Step 83 | loss: 0.9113302826881409
Step 83 | grad_norm: 3.387667417526245
Step 83 | learning_rate: 8.948717948717949e-05
Step 83 | epoch: 0.32
Step 84 | loss: 0.9651016592979431
Step 84 | grad_norm: 3.490124225616455
Step 84 | learning_rate: 8.935897435897437e-05
Step 84 | epoch: 0.32385542168674697
Step 85 | loss: 1.0162771940231323
Step 85 | grad_norm: 3.705172300338745
Step 85 | learning_rate: 8.923076923076924e-05
Step 85 | epoch: 0.327710843373494
Step 86 | loss: 1.058929681777954
Step 86 | grad_norm: 3.325125217437744
Step 86 | learning_rate: 8.910256410256411e-05
Step 86 | epoch: 0.33156626506024095
Step 87 | loss: 0.9745307564735413
Step 87 | grad_norm: 3.9594409465789795
Step 87 | learning_rate: 8.897435897435898e-05
Step 87 | epoch: 0.33542168674698797
Step 88 | loss: 1.1399338245391846
Step 88 | grad_norm: 4.0742411613464355
Step 88 | learning_rate: 8.884615384615384e-05
Step 88 | epoch: 0.33927710843373493
Step 89 | loss: 1.2151620388031006
Step 89 | grad_norm: 3.4871737957000732
Step 89 | learning_rate: 8.871794871794872e-05
Step 89 | epoch: 0.34313253012048195
Step 90 | loss: 1.180767297744751
Step 90 | grad_norm: 4.04047155380249
Step 90 | learning_rate: 8.858974358974359e-05
Step 90 | epoch: 0.3469879518072289
Step 91 | loss: 0.9579772353172302
Step 91 | grad_norm: 3.8133809566497803
Step 91 | learning_rate: 8.846153846153847e-05
Step 91 | epoch: 0.35084337349397593
Step 92 | loss: 0.9644148349761963
Step 92 | grad_norm: 4.320765495300293
Step 92 | learning_rate: 8.833333333333333e-05
Step 92 | epoch: 0.3546987951807229
Step 93 | loss: 1.0298789739608765
Step 93 | grad_norm: 4.019571781158447
Step 93 | learning_rate: 8.820512820512821e-05
Step 93 | epoch: 0.35855421686746985
Step 94 | loss: 0.9802075624465942
Step 94 | grad_norm: 3.4292824268341064
Step 94 | learning_rate: 8.807692307692307e-05
Step 94 | epoch: 0.3624096385542169
Step 95 | loss: 1.0681160688400269
Step 95 | grad_norm: 4.39198637008667
Step 95 | learning_rate: 8.794871794871795e-05
Step 95 | epoch: 0.36626506024096384
Step 96 | loss: 1.1084364652633667
Step 96 | grad_norm: 4.4952216148376465
Step 96 | learning_rate: 8.782051282051283e-05
Step 96 | epoch: 0.37012048192771085
Step 97 | loss: 0.9429650902748108
Step 97 | grad_norm: 4.103448390960693
Step 97 | learning_rate: 8.76923076923077e-05
Step 97 | epoch: 0.3739759036144578
Step 98 | loss: 1.0593317747116089
Step 98 | grad_norm: 3.753305196762085
Step 98 | learning_rate: 8.756410256410257e-05
Step 98 | epoch: 0.37783132530120483
Step 99 | loss: 1.1131811141967773
Step 99 | grad_norm: 3.682764768600464
Step 99 | learning_rate: 8.743589743589744e-05
Step 99 | epoch: 0.3816867469879518
Step 100 | loss: 0.9399795532226562
Step 100 | grad_norm: 3.153672695159912
Step 100 | learning_rate: 8.730769230769232e-05
Step 100 | epoch: 0.3855421686746988
Step 101 | loss: 1.2605576515197754
Step 101 | grad_norm: 4.401956558227539
Step 101 | learning_rate: 8.717948717948718e-05
Step 101 | epoch: 0.3893975903614458
Step 102 | loss: 1.1895458698272705
Step 102 | grad_norm: 3.6283698081970215
Step 102 | learning_rate: 8.705128205128206e-05
Step 102 | epoch: 0.3932530120481928
Step 103 | loss: 1.2194252014160156
Step 103 | grad_norm: 3.90960955619812
Step 103 | learning_rate: 8.692307692307692e-05
Step 103 | epoch: 0.39710843373493976
Step 104 | loss: 1.125244140625
Step 104 | grad_norm: 3.7242283821105957
Step 104 | learning_rate: 8.67948717948718e-05
Step 104 | epoch: 0.4009638554216867
Step 105 | loss: 1.0328495502471924
Step 105 | grad_norm: 4.135798454284668
Step 105 | learning_rate: 8.666666666666667e-05
Step 105 | epoch: 0.40481927710843374
Step 106 | loss: 0.9137117266654968
Step 106 | grad_norm: 3.352548837661743
Step 106 | learning_rate: 8.653846153846155e-05
Step 106 | epoch: 0.4086746987951807
Step 107 | loss: 1.0024170875549316
Step 107 | grad_norm: 3.5979433059692383
Step 107 | learning_rate: 8.641025641025642e-05
Step 107 | epoch: 0.4125301204819277
Step 108 | loss: 1.3258146047592163
Step 108 | grad_norm: 4.9344658851623535
Step 108 | learning_rate: 8.628205128205129e-05
Step 108 | epoch: 0.4163855421686747
Step 109 | loss: 1.0981593132019043
Step 109 | grad_norm: 3.342122793197632
Step 109 | learning_rate: 8.615384615384617e-05
Step 109 | epoch: 0.4202409638554217
Step 110 | loss: 1.0215198993682861
Step 110 | grad_norm: 3.790043592453003
Step 110 | learning_rate: 8.602564102564103e-05
Step 110 | epoch: 0.42409638554216866
Step 111 | loss: 1.0416483879089355
Step 111 | grad_norm: 5.179849147796631
Step 111 | learning_rate: 8.58974358974359e-05
Step 111 | epoch: 0.4279518072289157
Step 112 | loss: 1.004530906677246
Step 112 | grad_norm: 3.743370532989502
Step 112 | learning_rate: 8.576923076923076e-05
Step 112 | epoch: 0.43180722891566264
Step 113 | loss: 1.0829145908355713
Step 113 | grad_norm: 3.6374526023864746
Step 113 | learning_rate: 8.564102564102564e-05
Step 113 | epoch: 0.43566265060240966
Step 114 | loss: 1.0001063346862793
Step 114 | grad_norm: 3.4607157707214355
Step 114 | learning_rate: 8.551282051282052e-05
Step 114 | epoch: 0.4395180722891566
Step 115 | loss: 0.9926652908325195
Step 115 | grad_norm: 3.8138926029205322
Step 115 | learning_rate: 8.538461538461538e-05
Step 115 | epoch: 0.4433734939759036
Step 116 | loss: 0.9395232200622559
Step 116 | grad_norm: 3.5571794509887695
Step 116 | learning_rate: 8.525641025641026e-05
Step 116 | epoch: 0.4472289156626506
Step 117 | loss: 1.2098917961120605
Step 117 | grad_norm: 3.6668310165405273
Step 117 | learning_rate: 8.512820512820513e-05
Step 117 | epoch: 0.45108433734939757
Step 118 | loss: 1.3102543354034424
Step 118 | grad_norm: 4.249512672424316
Step 118 | learning_rate: 8.5e-05
Step 118 | epoch: 0.4549397590361446
Step 119 | loss: 1.2377588748931885
Step 119 | grad_norm: 4.66183614730835
Step 119 | learning_rate: 8.487179487179487e-05
Step 119 | epoch: 0.45879518072289155
Step 120 | loss: 1.0198637247085571
Step 120 | grad_norm: 4.425663471221924
Step 120 | learning_rate: 8.474358974358975e-05
Step 120 | epoch: 0.46265060240963857
Step 121 | loss: 1.2009373903274536
Step 121 | grad_norm: 3.8633158206939697
Step 121 | learning_rate: 8.461538461538461e-05
Step 121 | epoch: 0.46650602409638553
Step 122 | loss: 1.0406283140182495
Step 122 | grad_norm: 4.159351348876953
Step 122 | learning_rate: 8.448717948717949e-05
Step 122 | epoch: 0.47036144578313255
Step 123 | loss: 0.9927928447723389
Step 123 | grad_norm: 4.078582286834717
Step 123 | learning_rate: 8.435897435897436e-05
Step 123 | epoch: 0.4742168674698795
Step 124 | loss: 0.8557249307632446
Step 124 | grad_norm: 3.434476137161255
Step 124 | learning_rate: 8.423076923076924e-05
Step 124 | epoch: 0.47807228915662653
Step 125 | loss: 1.1534866094589233
Step 125 | grad_norm: 3.4957306385040283
Step 125 | learning_rate: 8.410256410256411e-05
Step 125 | epoch: 0.4819277108433735
Step 126 | loss: 1.1061285734176636
Step 126 | grad_norm: 3.374955415725708
Step 126 | learning_rate: 8.397435897435898e-05
Step 126 | epoch: 0.4857831325301205
Step 127 | loss: 1.2240886688232422
Step 127 | grad_norm: 4.0196380615234375
Step 127 | learning_rate: 8.384615384615386e-05
Step 127 | epoch: 0.48963855421686747
Step 128 | loss: 1.13702392578125
Step 128 | grad_norm: 4.320640563964844
Step 128 | learning_rate: 8.371794871794872e-05
Step 128 | epoch: 0.49349397590361443
Step 129 | loss: 0.755976140499115
Step 129 | grad_norm: 3.0643789768218994
Step 129 | learning_rate: 8.35897435897436e-05
Step 129 | epoch: 0.49734939759036145
Step 130 | loss: 1.1131845712661743
Step 130 | grad_norm: 5.098968982696533
Step 130 | learning_rate: 8.346153846153847e-05
Step 130 | epoch: 0.5012048192771085
Step 131 | loss: 1.018266201019287
Step 131 | grad_norm: 3.393608570098877
Step 131 | learning_rate: 8.333333333333334e-05
Step 131 | epoch: 0.5050602409638554
Step 132 | loss: 0.9771153330802917
Step 132 | grad_norm: 4.276566028594971
Step 132 | learning_rate: 8.320512820512821e-05
Step 132 | epoch: 0.5089156626506024
Step 133 | loss: 1.0261132717132568
Step 133 | grad_norm: 4.262152671813965
Step 133 | learning_rate: 8.307692307692309e-05
Step 133 | epoch: 0.5127710843373494
Step 134 | loss: 0.9229255318641663
Step 134 | grad_norm: 4.590575695037842
Step 134 | learning_rate: 8.294871794871795e-05
Step 134 | epoch: 0.5166265060240964
Step 135 | loss: 0.921244204044342
Step 135 | grad_norm: 3.9889442920684814
Step 135 | learning_rate: 8.282051282051283e-05
Step 135 | epoch: 0.5204819277108433
Step 136 | loss: 1.1681936979293823
Step 136 | grad_norm: 4.392820835113525
Step 136 | learning_rate: 8.26923076923077e-05
Step 136 | epoch: 0.5243373493975904
Step 137 | loss: 1.127810001373291
Step 137 | grad_norm: 5.160278797149658
Step 137 | learning_rate: 8.256410256410256e-05
Step 137 | epoch: 0.5281927710843374
Step 138 | loss: 1.1385127305984497
Step 138 | grad_norm: 4.131138801574707
Step 138 | learning_rate: 8.243589743589744e-05
Step 138 | epoch: 0.5320481927710843
Step 139 | loss: 1.0217125415802002
Step 139 | grad_norm: 4.607470989227295
Step 139 | learning_rate: 8.23076923076923e-05
Step 139 | epoch: 0.5359036144578313
Step 140 | loss: 1.0951282978057861
Step 140 | grad_norm: 5.3868303298950195
Step 140 | learning_rate: 8.217948717948718e-05
Step 140 | epoch: 0.5397590361445783
Step 141 | loss: 1.147257924079895
Step 141 | grad_norm: 4.30295991897583
Step 141 | learning_rate: 8.205128205128205e-05
Step 141 | epoch: 0.5436144578313253
Step 142 | loss: 1.0111618041992188
Step 142 | grad_norm: 4.422308444976807
Step 142 | learning_rate: 8.192307692307693e-05
Step 142 | epoch: 0.5474698795180722
Step 143 | loss: 1.1129523515701294
Step 143 | grad_norm: 4.07436466217041
Step 143 | learning_rate: 8.179487179487179e-05
Step 143 | epoch: 0.5513253012048193
Step 144 | loss: 1.0102128982543945
Step 144 | grad_norm: 5.243031978607178
Step 144 | learning_rate: 8.166666666666667e-05
Step 144 | epoch: 0.5551807228915663
Step 145 | loss: 1.1148117780685425
Step 145 | grad_norm: 4.520011901855469
Step 145 | learning_rate: 8.153846153846155e-05
Step 145 | epoch: 0.5590361445783133
Step 146 | loss: 1.0639805793762207
Step 146 | grad_norm: 4.053629398345947
Step 146 | learning_rate: 8.141025641025641e-05
Step 146 | epoch: 0.5628915662650602
Step 147 | loss: 0.914924144744873
Step 147 | grad_norm: 3.6179208755493164
Step 147 | learning_rate: 8.128205128205129e-05
Step 147 | epoch: 0.5667469879518072
Step 148 | loss: 1.080682635307312
Step 148 | grad_norm: 4.836063861846924
Step 148 | learning_rate: 8.115384615384616e-05
Step 148 | epoch: 0.5706024096385542
Step 149 | loss: 0.9573656320571899
Step 149 | grad_norm: 3.598721504211426
Step 149 | learning_rate: 8.102564102564103e-05
Step 149 | epoch: 0.5744578313253013
Step 150 | loss: 1.163329839706421
Step 150 | grad_norm: 4.448854923248291
Step 150 | learning_rate: 8.08974358974359e-05
Step 150 | epoch: 0.5783132530120482
Step 151 | loss: 0.8199654817581177
Step 151 | grad_norm: 3.212054967880249
Step 151 | learning_rate: 8.076923076923078e-05
Step 151 | epoch: 0.5821686746987952
Step 152 | loss: 0.9700772762298584
Step 152 | grad_norm: 4.1635003089904785
Step 152 | learning_rate: 8.064102564102564e-05
Step 152 | epoch: 0.5860240963855422
Step 153 | loss: 1.161851406097412
Step 153 | grad_norm: 3.5990922451019287
Step 153 | learning_rate: 8.051282051282052e-05
Step 153 | epoch: 0.5898795180722891
Step 154 | loss: 1.0743227005004883
Step 154 | grad_norm: 4.667956352233887
Step 154 | learning_rate: 8.038461538461538e-05
Step 154 | epoch: 0.5937349397590361
Step 155 | loss: 0.8950209617614746
Step 155 | grad_norm: 3.7184181213378906
Step 155 | learning_rate: 8.025641025641026e-05
Step 155 | epoch: 0.5975903614457831
Step 156 | loss: 1.0421682596206665
Step 156 | grad_norm: 4.268918037414551
Step 156 | learning_rate: 8.012820512820514e-05
Step 156 | epoch: 0.6014457831325302
Step 157 | loss: 1.1905267238616943
Step 157 | grad_norm: 4.404247760772705
Step 157 | learning_rate: 8e-05
Step 157 | epoch: 0.6053012048192771
Step 158 | loss: 0.981714129447937
Step 158 | grad_norm: 3.5313289165496826
Step 158 | learning_rate: 7.987179487179488e-05
Step 158 | epoch: 0.6091566265060241
Step 159 | loss: 1.053871989250183
Step 159 | grad_norm: 4.268401145935059
Step 159 | learning_rate: 7.974358974358975e-05
Step 159 | epoch: 0.6130120481927711
Step 160 | loss: 0.8955273628234863
Step 160 | grad_norm: 3.877561569213867
Step 160 | learning_rate: 7.961538461538461e-05
Step 160 | epoch: 0.6168674698795181
Step 161 | loss: 0.9363455772399902
Step 161 | grad_norm: 3.9226953983306885
Step 161 | learning_rate: 7.948717948717948e-05
Step 161 | epoch: 0.620722891566265
Step 162 | loss: 1.141807198524475
Step 162 | grad_norm: 4.564474105834961
Step 162 | learning_rate: 7.935897435897436e-05
Step 162 | epoch: 0.624578313253012
Step 163 | loss: 1.069263219833374
Step 163 | grad_norm: 4.009348392486572
Step 163 | learning_rate: 7.923076923076924e-05
Step 163 | epoch: 0.6284337349397591
Step 164 | loss: 1.3062864542007446
Step 164 | grad_norm: 4.401000022888184
Step 164 | learning_rate: 7.91025641025641e-05
Step 164 | epoch: 0.632289156626506
Step 165 | loss: 1.0827769041061401
Step 165 | grad_norm: 4.011373519897461
Step 165 | learning_rate: 7.897435897435898e-05
Step 165 | epoch: 0.636144578313253
Step 166 | loss: 0.8529372215270996
Step 166 | grad_norm: 3.7589564323425293
Step 166 | learning_rate: 7.884615384615384e-05
Step 166 | epoch: 0.64
Step 167 | loss: 1.1163710355758667
Step 167 | grad_norm: 4.753505229949951
Step 167 | learning_rate: 7.871794871794872e-05
Step 167 | epoch: 0.643855421686747
Step 168 | loss: 0.8656007647514343
Step 168 | grad_norm: 4.692632675170898
Step 168 | learning_rate: 7.858974358974359e-05
Step 168 | epoch: 0.6477108433734939
Step 169 | loss: 0.93048495054245
Step 169 | grad_norm: 3.281736373901367
Step 169 | learning_rate: 7.846153846153847e-05
Step 169 | epoch: 0.651566265060241
Step 170 | loss: 1.1412081718444824
Step 170 | grad_norm: 3.851118803024292
Step 170 | learning_rate: 7.833333333333333e-05
Step 170 | epoch: 0.655421686746988
Step 171 | loss: 1.0159708261489868
Step 171 | grad_norm: 4.042410850524902
Step 171 | learning_rate: 7.820512820512821e-05
Step 171 | epoch: 0.659277108433735
Step 172 | loss: 0.9587225317955017
Step 172 | grad_norm: 3.7184314727783203
Step 172 | learning_rate: 7.807692307692307e-05
Step 172 | epoch: 0.6631325301204819
Step 173 | loss: 1.1849678754806519
Step 173 | grad_norm: 4.188266754150391
Step 173 | learning_rate: 7.794871794871795e-05
Step 173 | epoch: 0.6669879518072289
Step 174 | loss: 1.1228134632110596
Step 174 | grad_norm: 3.5678751468658447
Step 174 | learning_rate: 7.782051282051283e-05
Step 174 | epoch: 0.6708433734939759
Step 175 | loss: 1.256231665611267
Step 175 | grad_norm: 4.2091240882873535
Step 175 | learning_rate: 7.76923076923077e-05
Step 175 | epoch: 0.6746987951807228
Step 176 | loss: 0.9103702306747437
Step 176 | grad_norm: 3.6242411136627197
Step 176 | learning_rate: 7.756410256410257e-05
Step 176 | epoch: 0.6785542168674699
Step 177 | loss: 0.9475806355476379
Step 177 | grad_norm: 3.3384716510772705
Step 177 | learning_rate: 7.743589743589744e-05
Step 177 | epoch: 0.6824096385542169
Step 178 | loss: 1.0305140018463135
Step 178 | grad_norm: 3.8451695442199707
Step 178 | learning_rate: 7.730769230769232e-05
Step 178 | epoch: 0.6862650602409639
Step 179 | loss: 1.0442101955413818
Step 179 | grad_norm: 3.5366785526275635
Step 179 | learning_rate: 7.717948717948718e-05
Step 179 | epoch: 0.6901204819277108
Step 180 | loss: 0.9042108654975891
Step 180 | grad_norm: 3.793797731399536
Step 180 | learning_rate: 7.705128205128206e-05
Step 180 | epoch: 0.6939759036144578
Step 181 | loss: 1.1284559965133667
Step 181 | grad_norm: 4.039085388183594
Step 181 | learning_rate: 7.692307692307693e-05
Step 181 | epoch: 0.6978313253012048
Step 182 | loss: 0.9883180856704712
Step 182 | grad_norm: 3.4522757530212402
Step 182 | learning_rate: 7.67948717948718e-05
Step 182 | epoch: 0.7016867469879519
Step 183 | loss: 0.7654386758804321
Step 183 | grad_norm: 3.0167617797851562
Step 183 | learning_rate: 7.666666666666667e-05
Step 183 | epoch: 0.7055421686746988
Step 184 | loss: 1.1192549467086792
Step 184 | grad_norm: 3.8761777877807617
Step 184 | learning_rate: 7.653846153846153e-05
Step 184 | epoch: 0.7093975903614458
Step 185 | loss: 0.8691247701644897
Step 185 | grad_norm: 3.9309215545654297
Step 185 | learning_rate: 7.641025641025641e-05
Step 185 | epoch: 0.7132530120481928
Step 186 | loss: 0.925460696220398
Step 186 | grad_norm: 3.097834587097168
Step 186 | learning_rate: 7.628205128205128e-05
Step 186 | epoch: 0.7171084337349397
Step 187 | loss: 0.9666712284088135
Step 187 | grad_norm: 3.589322805404663
Step 187 | learning_rate: 7.615384615384616e-05
Step 187 | epoch: 0.7209638554216867
Step 188 | loss: 1.091795563697815
Step 188 | grad_norm: 4.165306568145752
Step 188 | learning_rate: 7.602564102564102e-05
Step 188 | epoch: 0.7248192771084337
Step 189 | loss: 1.0003609657287598
Step 189 | grad_norm: 4.005610942840576
Step 189 | learning_rate: 7.58974358974359e-05
Step 189 | epoch: 0.7286746987951808
Step 190 | loss: 1.2624999284744263
Step 190 | grad_norm: 3.488265037536621
Step 190 | learning_rate: 7.576923076923076e-05
Step 190 | epoch: 0.7325301204819277
Step 191 | loss: 0.9452172517776489
Step 191 | grad_norm: 3.593857765197754
Step 191 | learning_rate: 7.564102564102564e-05
Step 191 | epoch: 0.7363855421686747
Step 192 | loss: 1.226824164390564
Step 192 | grad_norm: 6.355898380279541
Step 192 | learning_rate: 7.551282051282052e-05
Step 192 | epoch: 0.7402409638554217
Step 193 | loss: 0.9662705063819885
Step 193 | grad_norm: 4.465843200683594
Step 193 | learning_rate: 7.538461538461539e-05
Step 193 | epoch: 0.7440963855421687
Step 194 | loss: 0.8249883055686951
Step 194 | grad_norm: 3.4145727157592773
Step 194 | learning_rate: 7.525641025641026e-05
Step 194 | epoch: 0.7479518072289156
Step 195 | loss: 0.907465934753418
Step 195 | grad_norm: 3.2932193279266357
Step 195 | learning_rate: 7.512820512820513e-05
Step 195 | epoch: 0.7518072289156627
Step 196 | loss: 0.9946959614753723
Step 196 | grad_norm: 3.511481285095215
Step 196 | learning_rate: 7.500000000000001e-05
Step 196 | epoch: 0.7556626506024097
Step 197 | loss: 0.9058113098144531
Step 197 | grad_norm: 2.8179333209991455
Step 197 | learning_rate: 7.487179487179487e-05
Step 197 | epoch: 0.7595180722891566
Step 198 | loss: 0.9130194783210754
Step 198 | grad_norm: 3.51841402053833
Step 198 | learning_rate: 7.474358974358975e-05
Step 198 | epoch: 0.7633734939759036
Step 199 | loss: 1.1608048677444458
Step 199 | grad_norm: 3.3885905742645264
Step 199 | learning_rate: 7.461538461538462e-05
Step 199 | epoch: 0.7672289156626506
Step 200 | loss: 0.8581258654594421
Step 200 | grad_norm: 3.1197822093963623
Step 200 | learning_rate: 7.44871794871795e-05
Step 200 | epoch: 0.7710843373493976
Step 201 | loss: 0.8950304388999939
Step 201 | grad_norm: 2.931844472885132
Step 201 | learning_rate: 7.435897435897436e-05
Step 201 | epoch: 0.7749397590361445
Step 202 | loss: 1.0611591339111328
Step 202 | grad_norm: 6.602297306060791
Step 202 | learning_rate: 7.423076923076924e-05
Step 202 | epoch: 0.7787951807228916
Step 203 | loss: 0.8119082450866699
Step 203 | grad_norm: 2.9262020587921143
Step 203 | learning_rate: 7.410256410256412e-05
Step 203 | epoch: 0.7826506024096386
Step 204 | loss: 0.8478126525878906
Step 204 | grad_norm: 4.749422550201416
Step 204 | learning_rate: 7.397435897435898e-05
Step 204 | epoch: 0.7865060240963856
Step 205 | loss: 0.8502641320228577
Step 205 | grad_norm: 3.5841832160949707
Step 205 | learning_rate: 7.384615384615386e-05
Step 205 | epoch: 0.7903614457831325
Step 206 | loss: 1.1150407791137695
Step 206 | grad_norm: 3.950840473175049
Step 206 | learning_rate: 7.371794871794872e-05
Step 206 | epoch: 0.7942168674698795
Step 207 | loss: 1.0177017450332642
Step 207 | grad_norm: 4.330709934234619
Step 207 | learning_rate: 7.35897435897436e-05
Step 207 | epoch: 0.7980722891566265
Step 208 | loss: 0.9649384021759033
Step 208 | grad_norm: 3.855346918106079
Step 208 | learning_rate: 7.346153846153847e-05
Step 208 | epoch: 0.8019277108433734
Step 209 | loss: 1.0755283832550049
Step 209 | grad_norm: 3.4119436740875244
Step 209 | learning_rate: 7.333333333333333e-05
Step 209 | epoch: 0.8057831325301205
Step 210 | loss: 1.1376943588256836
Step 210 | grad_norm: 4.513587474822998
Step 210 | learning_rate: 7.320512820512821e-05
Step 210 | epoch: 0.8096385542168675
Step 211 | loss: 0.9414304494857788
Step 211 | grad_norm: 4.340689659118652
Step 211 | learning_rate: 7.307692307692307e-05
Step 211 | epoch: 0.8134939759036145
Step 212 | loss: 0.9051188826560974
Step 212 | grad_norm: 3.561566114425659
Step 212 | learning_rate: 7.294871794871795e-05
Step 212 | epoch: 0.8173493975903614
Step 213 | loss: 0.9770530462265015
Step 213 | grad_norm: 3.077072858810425
Step 213 | learning_rate: 7.282051282051282e-05
Step 213 | epoch: 0.8212048192771084
Step 214 | loss: 1.111425757408142
Step 214 | grad_norm: 4.132966041564941
Step 214 | learning_rate: 7.26923076923077e-05
Step 214 | epoch: 0.8250602409638554
Step 215 | loss: 1.1560029983520508
Step 215 | grad_norm: 4.1398820877075195
Step 215 | learning_rate: 7.256410256410256e-05
Step 215 | epoch: 0.8289156626506025
Step 216 | loss: 1.137657880783081
Step 216 | grad_norm: 3.912893295288086
Step 216 | learning_rate: 7.243589743589744e-05
Step 216 | epoch: 0.8327710843373494
Step 217 | loss: 0.930671215057373
Step 217 | grad_norm: 3.3282313346862793
Step 217 | learning_rate: 7.23076923076923e-05
Step 217 | epoch: 0.8366265060240964
Step 218 | loss: 0.8727437257766724
Step 218 | grad_norm: 3.7371606826782227
Step 218 | learning_rate: 7.217948717948718e-05
Step 218 | epoch: 0.8404819277108434
Step 219 | loss: 1.0956486463546753
Step 219 | grad_norm: 4.023116588592529
Step 219 | learning_rate: 7.205128205128205e-05
Step 219 | epoch: 0.8443373493975903
Step 220 | loss: 0.7975476384162903
Step 220 | grad_norm: 3.1749660968780518
Step 220 | learning_rate: 7.192307692307693e-05
Step 220 | epoch: 0.8481927710843373
Step 221 | loss: 0.7617907524108887
Step 221 | grad_norm: 3.868976593017578
Step 221 | learning_rate: 7.17948717948718e-05
Step 221 | epoch: 0.8520481927710843
Step 222 | loss: 0.9081580638885498
Step 222 | grad_norm: 4.042607307434082
Step 222 | learning_rate: 7.166666666666667e-05
Step 222 | epoch: 0.8559036144578314
Step 223 | loss: 0.8354424238204956
Step 223 | grad_norm: 3.9563100337982178
Step 223 | learning_rate: 7.153846153846155e-05
Step 223 | epoch: 0.8597590361445783
Step 224 | loss: 1.3083547353744507
Step 224 | grad_norm: 4.189018249511719
Step 224 | learning_rate: 7.141025641025641e-05
Step 224 | epoch: 0.8636144578313253
Step 225 | loss: 0.9295207858085632
Step 225 | grad_norm: 4.808187007904053
Step 225 | learning_rate: 7.128205128205129e-05
Step 225 | epoch: 0.8674698795180723
Step 226 | loss: 1.355953574180603
Step 226 | grad_norm: 4.674830913543701
Step 226 | learning_rate: 7.115384615384616e-05
Step 226 | epoch: 0.8713253012048193
Step 227 | loss: 1.170238971710205
Step 227 | grad_norm: 4.216633319854736
Step 227 | learning_rate: 7.102564102564103e-05
Step 227 | epoch: 0.8751807228915662
Step 228 | loss: 1.0128564834594727
Step 228 | grad_norm: 4.036520957946777
Step 228 | learning_rate: 7.08974358974359e-05
Step 228 | epoch: 0.8790361445783132
Step 229 | loss: 0.9299979209899902
Step 229 | grad_norm: 4.553236484527588
Step 229 | learning_rate: 7.076923076923078e-05
Step 229 | epoch: 0.8828915662650603
Step 230 | loss: 1.2356139421463013
Step 230 | grad_norm: 4.6550092697143555
Step 230 | learning_rate: 7.064102564102564e-05
Step 230 | epoch: 0.8867469879518072
Step 231 | loss: 0.8678828477859497
Step 231 | grad_norm: 3.207108497619629
Step 231 | learning_rate: 7.051282051282052e-05
Step 231 | epoch: 0.8906024096385542
Step 232 | loss: 1.2165353298187256
Step 232 | grad_norm: 4.2665510177612305
Step 232 | learning_rate: 7.03846153846154e-05
Step 232 | epoch: 0.8944578313253012
Step 233 | loss: 1.1001642942428589
Step 233 | grad_norm: 4.039172649383545
Step 233 | learning_rate: 7.025641025641025e-05
Step 233 | epoch: 0.8983132530120482
Step 234 | loss: 1.039341688156128
Step 234 | grad_norm: 3.623504638671875
Step 234 | learning_rate: 7.012820512820513e-05
Step 234 | epoch: 0.9021686746987951
Step 235 | loss: 1.0180118083953857
Step 235 | grad_norm: 5.074408054351807
Step 235 | learning_rate: 7e-05
Step 235 | epoch: 0.9060240963855422
Step 236 | loss: 0.8829696178436279
Step 236 | grad_norm: 3.3681015968322754
Step 236 | learning_rate: 6.987179487179487e-05
Step 236 | epoch: 0.9098795180722892
Step 237 | loss: 1.2246477603912354
Step 237 | grad_norm: 3.3550267219543457
Step 237 | learning_rate: 6.974358974358974e-05
Step 237 | epoch: 0.9137349397590362
Step 238 | loss: 1.1777877807617188
Step 238 | grad_norm: 4.147578239440918
Step 238 | learning_rate: 6.961538461538462e-05
Step 238 | epoch: 0.9175903614457831
Step 239 | loss: 1.0496516227722168
Step 239 | grad_norm: 3.8623673915863037
Step 239 | learning_rate: 6.94871794871795e-05
Step 239 | epoch: 0.9214457831325301
Step 240 | loss: 1.061482310295105
Step 240 | grad_norm: 3.376896381378174
Step 240 | learning_rate: 6.935897435897436e-05
Step 240 | epoch: 0.9253012048192771
Step 241 | loss: 0.9975374937057495
Step 241 | grad_norm: 3.4021544456481934
Step 241 | learning_rate: 6.923076923076924e-05
Step 241 | epoch: 0.929156626506024
Step 242 | loss: 1.1896677017211914
Step 242 | grad_norm: 3.7473807334899902
Step 242 | learning_rate: 6.91025641025641e-05
Step 242 | epoch: 0.9330120481927711
Step 243 | loss: 0.8227133750915527
Step 243 | grad_norm: 3.744173288345337
Step 243 | learning_rate: 6.897435897435898e-05
Step 243 | epoch: 0.9368674698795181
Step 244 | loss: 0.9882370829582214
Step 244 | grad_norm: 3.3456621170043945
Step 244 | learning_rate: 6.884615384615385e-05
Step 244 | epoch: 0.9407228915662651
Step 245 | loss: 1.066488265991211
Step 245 | grad_norm: 3.941319465637207
Step 245 | learning_rate: 6.871794871794872e-05
Step 245 | epoch: 0.944578313253012
Step 246 | loss: 1.0125430822372437
Step 246 | grad_norm: 4.105533123016357
Step 246 | learning_rate: 6.858974358974359e-05
Step 246 | epoch: 0.948433734939759
Step 247 | loss: 1.092411756515503
Step 247 | grad_norm: 3.6983840465545654
Step 247 | learning_rate: 6.846153846153847e-05
Step 247 | epoch: 0.952289156626506
Step 248 | loss: 0.9177778959274292
Step 248 | grad_norm: 3.985170602798462
Step 248 | learning_rate: 6.833333333333333e-05
Step 248 | epoch: 0.9561445783132531
Step 249 | loss: 1.1865856647491455
Step 249 | grad_norm: 5.37609338760376
Step 249 | learning_rate: 6.820512820512821e-05
Step 249 | epoch: 0.96
Step 250 | loss: 1.2108521461486816
Step 250 | grad_norm: 4.13450813293457
Step 250 | learning_rate: 6.807692307692309e-05
Step 250 | epoch: 0.963855421686747
Step 251 | loss: 1.1092058420181274
Step 251 | grad_norm: 4.114517688751221
Step 251 | learning_rate: 6.794871794871795e-05
Step 251 | epoch: 0.967710843373494
Step 252 | loss: 1.1645863056182861
Step 252 | grad_norm: 7.1249098777771
Step 252 | learning_rate: 6.782051282051283e-05
Step 252 | epoch: 0.971566265060241
Step 253 | loss: 0.9617133736610413
Step 253 | grad_norm: 4.983471393585205
Step 253 | learning_rate: 6.76923076923077e-05
Step 253 | epoch: 0.9754216867469879
Step 254 | loss: 0.973842203617096
Step 254 | grad_norm: 3.7781713008880615
Step 254 | learning_rate: 6.756410256410258e-05
Step 254 | epoch: 0.9792771084337349
Step 255 | loss: 0.9109927415847778
Step 255 | grad_norm: 3.9255993366241455
Step 255 | learning_rate: 6.743589743589744e-05
Step 255 | epoch: 0.983132530120482
Step 256 | loss: 0.8147485256195068
Step 256 | grad_norm: 3.366227626800537
Step 256 | learning_rate: 6.730769230769232e-05
Step 256 | epoch: 0.9869879518072289
Step 257 | loss: 0.853177011013031
Step 257 | grad_norm: 3.7031240463256836
Step 257 | learning_rate: 6.717948717948718e-05
Step 257 | epoch: 0.9908433734939759
Step 258 | loss: 1.0019766092300415
Step 258 | grad_norm: 4.587342739105225
Step 258 | learning_rate: 6.705128205128205e-05
Step 258 | epoch: 0.9946987951807229
Step 259 | loss: 0.9961453676223755
Step 259 | grad_norm: 3.6994290351867676
Step 259 | learning_rate: 6.692307692307693e-05
Step 259 | epoch: 0.9985542168674699
Step 260 | loss: 0.712511420249939
Step 260 | grad_norm: 5.458093166351318
Step 260 | learning_rate: 6.679487179487179e-05
Step 260 | epoch: 1.0
Step 261 | loss: 0.7439042925834656
Step 261 | grad_norm: 4.350214958190918
Step 261 | learning_rate: 6.666666666666667e-05
Step 261 | epoch: 1.003855421686747
Step 262 | loss: 1.1980056762695312
Step 262 | grad_norm: 3.6732237339019775
Step 262 | learning_rate: 6.653846153846153e-05
Step 262 | epoch: 1.007710843373494
Step 263 | loss: 0.8144867420196533
Step 263 | grad_norm: 3.598402738571167
Step 263 | learning_rate: 6.641025641025641e-05
Step 263 | epoch: 1.011566265060241
Step 264 | loss: 0.7578882575035095
Step 264 | grad_norm: 3.5089423656463623
Step 264 | learning_rate: 6.628205128205128e-05
Step 264 | epoch: 1.0154216867469879
Step 265 | loss: 0.7754682302474976
Step 265 | grad_norm: 3.4207541942596436
Step 265 | learning_rate: 6.615384615384616e-05
Step 265 | epoch: 1.0192771084337349
Step 266 | loss: 0.758962869644165
Step 266 | grad_norm: 3.4779484272003174
Step 266 | learning_rate: 6.602564102564102e-05
Step 266 | epoch: 1.0231325301204819
Step 267 | loss: 0.8063798546791077
Step 267 | grad_norm: 4.576241493225098
Step 267 | learning_rate: 6.58974358974359e-05
Step 267 | epoch: 1.026987951807229
Step 268 | loss: 0.8671209812164307
Step 268 | grad_norm: 5.045065879821777
Step 268 | learning_rate: 6.576923076923078e-05
Step 268 | epoch: 1.030843373493976
Step 269 | loss: 0.8462280631065369
Step 269 | grad_norm: 3.6580543518066406
Step 269 | learning_rate: 6.564102564102564e-05
Step 269 | epoch: 1.034698795180723
Step 270 | loss: 0.5763731598854065
Step 270 | grad_norm: 3.187286853790283
Step 270 | learning_rate: 6.551282051282052e-05
Step 270 | epoch: 1.03855421686747
Step 271 | loss: 0.8638547658920288
Step 271 | grad_norm: 3.658003091812134
Step 271 | learning_rate: 6.538461538461539e-05
Step 271 | epoch: 1.0424096385542168
Step 272 | loss: 0.87566077709198
Step 272 | grad_norm: 4.6999192237854
Step 272 | learning_rate: 6.525641025641026e-05
Step 272 | epoch: 1.0462650602409638
Step 273 | loss: 0.6657900810241699
Step 273 | grad_norm: 4.268926620483398
Step 273 | learning_rate: 6.512820512820513e-05
Step 273 | epoch: 1.0501204819277108
Step 274 | loss: 0.961848258972168
Step 274 | grad_norm: 5.144266605377197
Step 274 | learning_rate: 6.500000000000001e-05
Step 274 | epoch: 1.0539759036144578
Step 275 | loss: 0.8843549489974976
Step 275 | grad_norm: 5.665648460388184
Step 275 | learning_rate: 6.487179487179487e-05
Step 275 | epoch: 1.0578313253012048
Step 276 | loss: 0.8343912959098816
Step 276 | grad_norm: 4.652595043182373
Step 276 | learning_rate: 6.474358974358975e-05
Step 276 | epoch: 1.0616867469879518
Step 277 | loss: 0.8802427053451538
Step 277 | grad_norm: 4.707510471343994
Step 277 | learning_rate: 6.461538461538462e-05
Step 277 | epoch: 1.0655421686746989
Step 278 | loss: 0.6199423670768738
Step 278 | grad_norm: 5.189672946929932
Step 278 | learning_rate: 6.44871794871795e-05
Step 278 | epoch: 1.0693975903614459
Step 279 | loss: 0.8316900134086609
Step 279 | grad_norm: 5.0833740234375
Step 279 | learning_rate: 6.435897435897437e-05
Step 279 | epoch: 1.0732530120481927
Step 280 | loss: 0.8052300214767456
Step 280 | grad_norm: 4.840207099914551
Step 280 | learning_rate: 6.423076923076924e-05
Step 280 | epoch: 1.0771084337349397
Step 281 | loss: 0.9733623266220093
Step 281 | grad_norm: 4.2948102951049805
Step 281 | learning_rate: 6.410256410256412e-05
Step 281 | epoch: 1.0809638554216867
Step 282 | loss: 1.0274585485458374
Step 282 | grad_norm: 5.283914089202881
Step 282 | learning_rate: 6.397435897435897e-05
Step 282 | epoch: 1.0848192771084337
Step 283 | loss: 0.6814340949058533
Step 283 | grad_norm: 4.166132926940918
Step 283 | learning_rate: 6.384615384615385e-05
Step 283 | epoch: 1.0886746987951808
Step 284 | loss: 0.7348777651786804
Step 284 | grad_norm: 4.217555046081543
Step 284 | learning_rate: 6.371794871794871e-05
Step 284 | epoch: 1.0925301204819278
Step 285 | loss: 0.6577064990997314
Step 285 | grad_norm: 3.837210178375244
Step 285 | learning_rate: 6.358974358974359e-05
Step 285 | epoch: 1.0963855421686748
Step 286 | loss: 0.6846927404403687
Step 286 | grad_norm: 4.374179840087891
Step 286 | learning_rate: 6.346153846153847e-05
Step 286 | epoch: 1.1002409638554216
Step 287 | loss: 0.7030883431434631
Step 287 | grad_norm: 4.398637294769287
Step 287 | learning_rate: 6.333333333333333e-05
Step 287 | epoch: 1.1040963855421686
Step 288 | loss: 0.8207652568817139
Step 288 | grad_norm: 3.9550201892852783
Step 288 | learning_rate: 6.320512820512821e-05
Step 288 | epoch: 1.1079518072289156
Step 289 | loss: 0.5827029943466187
Step 289 | grad_norm: 3.527045488357544
Step 289 | learning_rate: 6.307692307692308e-05
Step 289 | epoch: 1.1118072289156626
Step 290 | loss: 0.6306131482124329
Step 290 | grad_norm: 4.008740425109863
Step 290 | learning_rate: 6.294871794871795e-05
Step 290 | epoch: 1.1156626506024097
Step 291 | loss: 0.6798239946365356
Step 291 | grad_norm: 4.711292266845703
Step 291 | learning_rate: 6.282051282051282e-05
Step 291 | epoch: 1.1195180722891567
Step 292 | loss: 1.0463420152664185
Step 292 | grad_norm: 5.18014669418335
Step 292 | learning_rate: 6.26923076923077e-05
Step 292 | epoch: 1.1233734939759037
Step 293 | loss: 0.711571455001831
Step 293 | grad_norm: 3.9701151847839355
Step 293 | learning_rate: 6.256410256410256e-05
Step 293 | epoch: 1.1272289156626507
Step 294 | loss: 0.7651051878929138
Step 294 | grad_norm: 3.591062068939209
Step 294 | learning_rate: 6.243589743589744e-05
Step 294 | epoch: 1.1310843373493975
Step 295 | loss: 0.7596261501312256
Step 295 | grad_norm: 4.947089195251465
Step 295 | learning_rate: 6.23076923076923e-05
Step 295 | epoch: 1.1349397590361445
Step 296 | loss: 0.6619172692298889
Step 296 | grad_norm: 4.321592807769775
Step 296 | learning_rate: 6.217948717948718e-05
Step 296 | epoch: 1.1387951807228915
Step 297 | loss: 0.7692047357559204
Step 297 | grad_norm: 3.8494579792022705
Step 297 | learning_rate: 6.205128205128206e-05
Step 297 | epoch: 1.1426506024096386
Step 298 | loss: 0.818107008934021
Step 298 | grad_norm: 4.855798244476318
Step 298 | learning_rate: 6.192307692307693e-05
Step 298 | epoch: 1.1465060240963856
Step 299 | loss: 0.7625445127487183
Step 299 | grad_norm: 4.443519115447998
Step 299 | learning_rate: 6.17948717948718e-05
Step 299 | epoch: 1.1503614457831326
Step 300 | loss: 0.5800603032112122
Step 300 | grad_norm: 3.3666703701019287
Step 300 | learning_rate: 6.166666666666667e-05
Step 300 | epoch: 1.1542168674698796
Step 301 | loss: 0.6920375227928162
Step 301 | grad_norm: 3.848869562149048
Step 301 | learning_rate: 6.153846153846155e-05
Step 301 | epoch: 1.1580722891566264
Step 302 | loss: 0.6851505041122437
Step 302 | grad_norm: 4.044360160827637
Step 302 | learning_rate: 6.141025641025641e-05
Step 302 | epoch: 1.1619277108433734
Step 303 | loss: 0.8313071131706238
Step 303 | grad_norm: 4.7818169593811035
Step 303 | learning_rate: 6.128205128205129e-05
Step 303 | epoch: 1.1657831325301204
Step 304 | loss: 0.6446830034255981
Step 304 | grad_norm: 3.881967544555664
Step 304 | learning_rate: 6.115384615384616e-05
Step 304 | epoch: 1.1696385542168675
Step 305 | loss: 0.7406821846961975
Step 305 | grad_norm: 5.453915596008301
Step 305 | learning_rate: 6.1025641025641035e-05
Step 305 | epoch: 1.1734939759036145
Step 306 | loss: 0.6665921807289124
Step 306 | grad_norm: 4.041615962982178
Step 306 | learning_rate: 6.089743589743589e-05
Step 306 | epoch: 1.1773493975903615
Step 307 | loss: 0.8242112994194031
Step 307 | grad_norm: 4.7746500968933105
Step 307 | learning_rate: 6.0769230769230765e-05
Step 307 | epoch: 1.1812048192771085
Step 308 | loss: 0.6814175248146057
Step 308 | grad_norm: 4.318298816680908
Step 308 | learning_rate: 6.0641025641025637e-05
Step 308 | epoch: 1.1850602409638555
Step 309 | loss: 0.9315301775932312
Step 309 | grad_norm: 5.231047630310059
Step 309 | learning_rate: 6.0512820512820515e-05
Step 309 | epoch: 1.1889156626506023
Step 310 | loss: 0.8404138088226318
Step 310 | grad_norm: 5.799229621887207
Step 310 | learning_rate: 6.038461538461539e-05
Step 310 | epoch: 1.1927710843373494
Step 311 | loss: 0.7033343315124512
Step 311 | grad_norm: 5.507450103759766
Step 311 | learning_rate: 6.025641025641026e-05
Step 311 | epoch: 1.1966265060240964
Step 312 | loss: 0.8367303609848022
Step 312 | grad_norm: 5.742828369140625
Step 312 | learning_rate: 6.012820512820513e-05
Step 312 | epoch: 1.2004819277108434
Step 313 | loss: 0.6761582493782043
Step 313 | grad_norm: 4.569295883178711
Step 313 | learning_rate: 6e-05
Step 313 | epoch: 1.2043373493975904
Step 314 | loss: 0.8504526615142822
Step 314 | grad_norm: 4.56090784072876
Step 314 | learning_rate: 5.987179487179487e-05
Step 314 | epoch: 1.2081927710843374
Step 315 | loss: 0.8950040936470032
Step 315 | grad_norm: 4.924112796783447
Step 315 | learning_rate: 5.9743589743589745e-05
Step 315 | epoch: 1.2120481927710842
Step 316 | loss: 0.7965061664581299
Step 316 | grad_norm: 4.402158737182617
Step 316 | learning_rate: 5.9615384615384616e-05
Step 316 | epoch: 1.2159036144578312
Step 317 | loss: 0.674674928188324
Step 317 | grad_norm: 4.330007076263428
Step 317 | learning_rate: 5.948717948717949e-05
Step 317 | epoch: 1.2197590361445783
Step 318 | loss: 0.7376487255096436
Step 318 | grad_norm: 5.312062740325928
Step 318 | learning_rate: 5.935897435897436e-05
Step 318 | epoch: 1.2236144578313253
Step 319 | loss: 0.7651023268699646
Step 319 | grad_norm: 4.779189109802246
Step 319 | learning_rate: 5.923076923076923e-05
Step 319 | epoch: 1.2274698795180723
Step 320 | loss: 0.6953752636909485
Step 320 | grad_norm: 4.555286884307861
Step 320 | learning_rate: 5.910256410256411e-05
Step 320 | epoch: 1.2313253012048193
Step 321 | loss: 0.6928178071975708
Step 321 | grad_norm: 3.810002565383911
Step 321 | learning_rate: 5.897435897435898e-05
Step 321 | epoch: 1.2351807228915663
Step 322 | loss: 0.8161348104476929
Step 322 | grad_norm: 5.482511520385742
Step 322 | learning_rate: 5.884615384615385e-05
Step 322 | epoch: 1.2390361445783133
Step 323 | loss: 0.7947838306427002
Step 323 | grad_norm: 3.939239740371704
Step 323 | learning_rate: 5.8717948717948725e-05
Step 323 | epoch: 1.2428915662650604
Step 324 | loss: 0.7117884159088135
Step 324 | grad_norm: 5.073875904083252
Step 324 | learning_rate: 5.8589743589743596e-05
Step 324 | epoch: 1.2467469879518072
Step 325 | loss: 0.7231497168540955
Step 325 | grad_norm: 5.246329307556152
Step 325 | learning_rate: 5.846153846153847e-05
Step 325 | epoch: 1.2506024096385542
Step 326 | loss: 0.8017929792404175
Step 326 | grad_norm: 4.492032051086426
Step 326 | learning_rate: 5.833333333333334e-05
Step 326 | epoch: 1.2544578313253012
Step 327 | loss: 0.6123989224433899
Step 327 | grad_norm: 5.443071365356445
Step 327 | learning_rate: 5.820512820512821e-05
Step 327 | epoch: 1.2583132530120482
Step 328 | loss: 0.8899831771850586
Step 328 | grad_norm: 5.491662979125977
Step 328 | learning_rate: 5.807692307692308e-05
Step 328 | epoch: 1.2621686746987952
Step 329 | loss: 0.8755162954330444
Step 329 | grad_norm: 4.983004570007324
Step 329 | learning_rate: 5.7948717948717954e-05
Step 329 | epoch: 1.266024096385542
Step 330 | loss: 0.9821756482124329
Step 330 | grad_norm: 6.557994842529297
Step 330 | learning_rate: 5.7820512820512826e-05
Step 330 | epoch: 1.269879518072289
Step 331 | loss: 0.6933925151824951
Step 331 | grad_norm: 4.909484386444092
Step 331 | learning_rate: 5.769230769230769e-05
Step 331 | epoch: 1.273734939759036
Step 332 | loss: 0.8340117931365967
Step 332 | grad_norm: 4.795546054840088
Step 332 | learning_rate: 5.756410256410256e-05
Step 332 | epoch: 1.277590361445783
Step 333 | loss: 0.8946582078933716
Step 333 | grad_norm: 4.388667106628418
Step 333 | learning_rate: 5.7435897435897434e-05
Step 333 | epoch: 1.28144578313253
Step 334 | loss: 0.888154149055481
Step 334 | grad_norm: 4.711656093597412
Step 334 | learning_rate: 5.7307692307692306e-05
Step 334 | epoch: 1.2853012048192771
Step 335 | loss: 0.6915244460105896
Step 335 | grad_norm: 4.159220218658447
Step 335 | learning_rate: 5.717948717948718e-05
Step 335 | epoch: 1.2891566265060241
Step 336 | loss: 0.8052679896354675
Step 336 | grad_norm: 4.1392130851745605
Step 336 | learning_rate: 5.705128205128205e-05
Step 336 | epoch: 1.2930120481927712
Step 337 | loss: 0.6654669642448425
Step 337 | grad_norm: 4.2316999435424805
Step 337 | learning_rate: 5.692307692307692e-05
Step 337 | epoch: 1.2968674698795182
Step 338 | loss: 0.47859275341033936
Step 338 | grad_norm: 4.163045883178711
Step 338 | learning_rate: 5.679487179487179e-05
Step 338 | epoch: 1.3007228915662652
Step 339 | loss: 0.683477520942688
Step 339 | grad_norm: 5.240133762359619
Step 339 | learning_rate: 5.666666666666667e-05
Step 339 | epoch: 1.304578313253012
Step 340 | loss: 0.642274796962738
Step 340 | grad_norm: 3.8911895751953125
Step 340 | learning_rate: 5.653846153846154e-05
Step 340 | epoch: 1.308433734939759
Step 341 | loss: 0.7807395458221436
Step 341 | grad_norm: 4.129082679748535
Step 341 | learning_rate: 5.6410256410256414e-05
Step 341 | epoch: 1.312289156626506
Step 342 | loss: 0.8958843946456909
Step 342 | grad_norm: 5.424281597137451
Step 342 | learning_rate: 5.6282051282051286e-05
Step 342 | epoch: 1.316144578313253
Step 343 | loss: 0.7102228403091431
Step 343 | grad_norm: 4.165657043457031
Step 343 | learning_rate: 5.615384615384616e-05
Step 343 | epoch: 1.32
Step 344 | loss: 0.5982236862182617
Step 344 | grad_norm: 4.300398826599121
Step 344 | learning_rate: 5.602564102564103e-05
Step 344 | epoch: 1.3238554216867469
Step 345 | loss: 0.804581880569458
Step 345 | grad_norm: 4.519138336181641
Step 345 | learning_rate: 5.58974358974359e-05
Step 345 | epoch: 1.3277108433734939
Step 346 | loss: 0.8277890086174011
Step 346 | grad_norm: 5.50302791595459
Step 346 | learning_rate: 5.576923076923077e-05
Step 346 | epoch: 1.331566265060241
Step 347 | loss: 0.9036376476287842
Step 347 | grad_norm: 5.234253883361816
Step 347 | learning_rate: 5.5641025641025644e-05
Step 347 | epoch: 1.335421686746988
Step 348 | loss: 0.8603811264038086
Step 348 | grad_norm: 4.828195095062256
Step 348 | learning_rate: 5.5512820512820515e-05
Step 348 | epoch: 1.339277108433735
Step 349 | loss: 0.8359081745147705
Step 349 | grad_norm: 4.484179973602295
Step 349 | learning_rate: 5.538461538461539e-05
Step 349 | epoch: 1.343132530120482
Step 350 | loss: 0.6954048275947571
Step 350 | grad_norm: 4.632992744445801
Step 350 | learning_rate: 5.5256410256410265e-05
Step 350 | epoch: 1.346987951807229
Step 351 | loss: 0.6304150819778442
Step 351 | grad_norm: 4.4820685386657715
Step 351 | learning_rate: 5.512820512820514e-05
Step 351 | epoch: 1.350843373493976
Step 352 | loss: 0.7755927443504333
Step 352 | grad_norm: 4.457450866699219
Step 352 | learning_rate: 5.500000000000001e-05
Step 352 | epoch: 1.354698795180723
Step 353 | loss: 0.4733027219772339
Step 353 | grad_norm: 4.225396156311035
Step 353 | learning_rate: 5.487179487179488e-05
Step 353 | epoch: 1.3585542168674698
Step 354 | loss: 0.5791862607002258
Step 354 | grad_norm: 3.752532720565796
Step 354 | learning_rate: 5.474358974358975e-05
Step 354 | epoch: 1.3624096385542168
Step 355 | loss: 0.7026658654212952
Step 355 | grad_norm: 4.538552761077881
Step 355 | learning_rate: 5.461538461538461e-05
Step 355 | epoch: 1.3662650602409638
Step 356 | loss: 0.730904757976532
Step 356 | grad_norm: 5.889381408691406
Step 356 | learning_rate: 5.448717948717948e-05
Step 356 | epoch: 1.3701204819277109
Step 357 | loss: 0.9141433835029602
Step 357 | grad_norm: 4.999213218688965
Step 357 | learning_rate: 5.435897435897436e-05
Step 357 | epoch: 1.3739759036144579
Step 358 | loss: 0.8404273986816406
Step 358 | grad_norm: 5.9510626792907715
Step 358 | learning_rate: 5.423076923076923e-05
Step 358 | epoch: 1.377831325301205
Step 359 | loss: 0.9029541015625
Step 359 | grad_norm: 5.554919719696045
Step 359 | learning_rate: 5.41025641025641e-05
Step 359 | epoch: 1.3816867469879517
Step 360 | loss: 0.7089888453483582
Step 360 | grad_norm: 4.467408657073975
Step 360 | learning_rate: 5.3974358974358975e-05
Step 360 | epoch: 1.3855421686746987
Step 361 | loss: 1.0210027694702148
Step 361 | grad_norm: 5.79587984085083
Step 361 | learning_rate: 5.384615384615385e-05
Step 361 | epoch: 1.3893975903614457
Step 362 | loss: 0.6382094621658325
Step 362 | grad_norm: 5.458649635314941
Step 362 | learning_rate: 5.371794871794872e-05
Step 362 | epoch: 1.3932530120481927
Step 363 | loss: 0.7325130701065063
Step 363 | grad_norm: 4.084562301635742
Step 363 | learning_rate: 5.358974358974359e-05
Step 363 | epoch: 1.3971084337349398
Step 364 | loss: 0.768867015838623
Step 364 | grad_norm: 4.6823649406433105
Step 364 | learning_rate: 5.346153846153846e-05
Step 364 | epoch: 1.4009638554216868
Step 365 | loss: 0.915066659450531
Step 365 | grad_norm: 5.203983306884766
Step 365 | learning_rate: 5.333333333333333e-05
Step 365 | epoch: 1.4048192771084338
Step 366 | loss: 0.6479238867759705
Step 366 | grad_norm: 4.310754776000977
Step 366 | learning_rate: 5.3205128205128205e-05
Step 366 | epoch: 1.4086746987951808
Step 367 | loss: 0.8781396746635437
Step 367 | grad_norm: 4.629777431488037
Step 367 | learning_rate: 5.3076923076923076e-05
Step 367 | epoch: 1.4125301204819278
Step 368 | loss: 0.8814172744750977
Step 368 | grad_norm: 4.7845377922058105
Step 368 | learning_rate: 5.2948717948717955e-05
Step 368 | epoch: 1.4163855421686746
Step 369 | loss: 0.6933857202529907
Step 369 | grad_norm: 4.951262950897217
Step 369 | learning_rate: 5.2820512820512826e-05
Step 369 | epoch: 1.4202409638554216
Step 370 | loss: 0.8113295435905457
Step 370 | grad_norm: 4.085917949676514
Step 370 | learning_rate: 5.26923076923077e-05
Step 370 | epoch: 1.4240963855421687
Step 371 | loss: 0.8551633358001709
Step 371 | grad_norm: 4.456656455993652
Step 371 | learning_rate: 5.256410256410257e-05
Step 371 | epoch: 1.4279518072289157
Step 372 | loss: 0.7539682388305664
Step 372 | grad_norm: 4.908009052276611
Step 372 | learning_rate: 5.243589743589744e-05
Step 372 | epoch: 1.4318072289156627
Step 373 | loss: 0.6564369201660156
Step 373 | grad_norm: 5.274575710296631
Step 373 | learning_rate: 5.230769230769231e-05
Step 373 | epoch: 1.4356626506024097
Step 374 | loss: 0.8612393140792847
Step 374 | grad_norm: 17.729307174682617
Step 374 | learning_rate: 5.2179487179487185e-05
Step 374 | epoch: 1.4395180722891565
Step 375 | loss: 0.8752960562705994
Step 375 | grad_norm: 6.223072052001953
Step 375 | learning_rate: 5.2051282051282056e-05
Step 375 | epoch: 1.4433734939759035
Step 376 | loss: 0.7176825404167175
Step 376 | grad_norm: 7.773497581481934
Step 376 | learning_rate: 5.192307692307693e-05
Step 376 | epoch: 1.4472289156626506
Step 377 | loss: 0.8135859370231628
Step 377 | grad_norm: 5.200961112976074
Step 377 | learning_rate: 5.17948717948718e-05
Step 377 | epoch: 1.4510843373493976
Step 378 | loss: 0.7998546957969666
Step 378 | grad_norm: 4.990593433380127
Step 378 | learning_rate: 5.166666666666667e-05
Step 378 | epoch: 1.4549397590361446
Step 379 | loss: 0.8528429269790649
Step 379 | grad_norm: 12.974933624267578
Step 379 | learning_rate: 5.1538461538461536e-05
Step 379 | epoch: 1.4587951807228916
Step 380 | loss: 0.7886706590652466
Step 380 | grad_norm: 12.19079875946045
Step 380 | learning_rate: 5.141025641025641e-05
Step 380 | epoch: 1.4626506024096386
Step 381 | loss: 0.6471788883209229
Step 381 | grad_norm: 5.823410987854004
Step 381 | learning_rate: 5.128205128205128e-05
Step 381 | epoch: 1.4665060240963856
Step 382 | loss: 0.937501072883606
Step 382 | grad_norm: 39.36800765991211
Step 382 | learning_rate: 5.115384615384615e-05
Step 382 | epoch: 1.4703614457831327
Step 383 | loss: 0.7739619016647339
Step 383 | grad_norm: 7.9175567626953125
Step 383 | learning_rate: 5.102564102564102e-05
Step 383 | epoch: 1.4742168674698795
Step 384 | loss: 0.6999286413192749
Step 384 | grad_norm: 11.766043663024902
Step 384 | learning_rate: 5.0897435897435894e-05
Step 384 | epoch: 1.4780722891566265
Step 385 | loss: 0.6545456051826477
Step 385 | grad_norm: 31.966323852539062
Step 385 | learning_rate: 5.0769230769230766e-05
Step 385 | epoch: 1.4819277108433735
Step 386 | loss: 0.9585321545600891
Step 386 | grad_norm: 5.638939380645752
Step 386 | learning_rate: 5.0641025641025644e-05
Step 386 | epoch: 1.4857831325301205
Step 387 | loss: 0.7388886213302612
Step 387 | grad_norm: 4.793699741363525
Step 387 | learning_rate: 5.0512820512820516e-05
Step 387 | epoch: 1.4896385542168675
Step 388 | loss: 0.8000381588935852
Step 388 | grad_norm: 4.386501789093018
Step 388 | learning_rate: 5.038461538461539e-05
Step 388 | epoch: 1.4934939759036143
Step 389 | loss: 0.786252498626709
Step 389 | grad_norm: 4.081370830535889
Step 389 | learning_rate: 5.025641025641026e-05
Step 389 | epoch: 1.4973493975903613
Step 390 | loss: 0.6228963136672974
Step 390 | grad_norm: 4.436598300933838
Step 390 | learning_rate: 5.012820512820513e-05
Step 390 | epoch: 1.5012048192771084
Step 391 | loss: 0.815777063369751
Step 391 | grad_norm: 4.921470642089844
Step 391 | learning_rate: 5e-05
Step 391 | epoch: 1.5050602409638554
Step 392 | loss: 0.866290807723999
Step 392 | grad_norm: 5.102272033691406
Step 392 | learning_rate: 4.9871794871794874e-05
Step 392 | epoch: 1.5089156626506024
Step 393 | loss: 0.7691495418548584
Step 393 | grad_norm: 5.159501552581787
Step 393 | learning_rate: 4.9743589743589746e-05
Step 393 | epoch: 1.5127710843373494
Step 394 | loss: 0.7136147022247314
Step 394 | grad_norm: 3.8994319438934326
Step 394 | learning_rate: 4.961538461538462e-05
Step 394 | epoch: 1.5166265060240964
Step 395 | loss: 0.6876245141029358
Step 395 | grad_norm: 4.488900184631348
Step 395 | learning_rate: 4.948717948717949e-05
Step 395 | epoch: 1.5204819277108435
Step 396 | loss: 0.7592606544494629
Step 396 | grad_norm: 4.693617343902588
Step 396 | learning_rate: 4.935897435897436e-05
Step 396 | epoch: 1.5243373493975905
Step 397 | loss: 0.7037993669509888
Step 397 | grad_norm: 4.696421146392822
Step 397 | learning_rate: 4.923076923076924e-05
Step 397 | epoch: 1.5281927710843375
Step 398 | loss: 0.7827814817428589
Step 398 | grad_norm: 4.911726474761963
Step 398 | learning_rate: 4.9102564102564104e-05
Step 398 | epoch: 1.5320481927710843
Step 399 | loss: 0.7082928419113159
Step 399 | grad_norm: 4.860736846923828
Step 399 | learning_rate: 4.8974358974358975e-05
Step 399 | epoch: 1.5359036144578313
Step 400 | loss: 0.7851388454437256
Step 400 | grad_norm: 5.129582405090332
Step 400 | learning_rate: 4.884615384615385e-05
Step 400 | epoch: 1.5397590361445783
Step 401 | loss: 0.6068757772445679
Step 401 | grad_norm: 4.598056316375732
Step 401 | learning_rate: 4.871794871794872e-05
Step 401 | epoch: 1.5436144578313253
Step 402 | loss: 0.881588339805603
Step 402 | grad_norm: 4.358565807342529
Step 402 | learning_rate: 4.858974358974359e-05
Step 402 | epoch: 1.5474698795180721
Step 403 | loss: 0.8275895714759827
Step 403 | grad_norm: 4.600119113922119
Step 403 | learning_rate: 4.846153846153846e-05
Step 403 | epoch: 1.5513253012048192
Step 404 | loss: 0.9091293811798096
Step 404 | grad_norm: 5.934371471405029
Step 404 | learning_rate: 4.8333333333333334e-05
Step 404 | epoch: 1.5551807228915662
Step 405 | loss: 0.8356478214263916
Step 405 | grad_norm: 4.575656890869141
Step 405 | learning_rate: 4.8205128205128205e-05
Step 405 | epoch: 1.5590361445783132
Step 406 | loss: 0.6902871131896973
Step 406 | grad_norm: 5.01743221282959
Step 406 | learning_rate: 4.8076923076923084e-05
Step 406 | epoch: 1.5628915662650602
Step 407 | loss: 0.6866618394851685
Step 407 | grad_norm: 4.303092002868652
Step 407 | learning_rate: 4.7948717948717955e-05
Step 407 | epoch: 1.5667469879518072
Step 408 | loss: 0.7784543633460999
Step 408 | grad_norm: 4.341768741607666
Step 408 | learning_rate: 4.782051282051283e-05
Step 408 | epoch: 1.5706024096385542
Step 409 | loss: 0.7570545077323914
Step 409 | grad_norm: 5.3575520515441895
Step 409 | learning_rate: 4.76923076923077e-05
Step 409 | epoch: 1.5744578313253013
Step 410 | loss: 0.9198506474494934
Step 410 | grad_norm: 12.167905807495117
Step 410 | learning_rate: 4.7564102564102563e-05
Step 410 | epoch: 1.5783132530120483
Step 411 | loss: 0.6577690839767456
Step 411 | grad_norm: 4.1180806159973145
Step 411 | learning_rate: 4.7435897435897435e-05
Step 411 | epoch: 1.5821686746987953
Step 412 | loss: 0.7050157785415649
Step 412 | grad_norm: 4.5526323318481445
Step 412 | learning_rate: 4.730769230769231e-05
Step 412 | epoch: 1.5860240963855423
Step 413 | loss: 0.6240150928497314
Step 413 | grad_norm: 4.5797834396362305
Step 413 | learning_rate: 4.717948717948718e-05
Step 413 | epoch: 1.589879518072289
Step 414 | loss: 0.7641468048095703
Step 414 | grad_norm: 4.093219757080078
Step 414 | learning_rate: 4.705128205128205e-05
Step 414 | epoch: 1.5937349397590361
Step 415 | loss: 0.676859438419342
Step 415 | grad_norm: 4.7458295822143555
Step 415 | learning_rate: 4.692307692307693e-05
Step 415 | epoch: 1.5975903614457831
Step 416 | loss: 0.8808997869491577
Step 416 | grad_norm: 4.768708229064941
Step 416 | learning_rate: 4.67948717948718e-05
Step 416 | epoch: 1.6014457831325302
Step 417 | loss: 0.8114875555038452
Step 417 | grad_norm: 4.535199165344238
Step 417 | learning_rate: 4.666666666666667e-05
Step 417 | epoch: 1.605301204819277
Step 418 | loss: 0.6813240647315979
Step 418 | grad_norm: 4.803022384643555
Step 418 | learning_rate: 4.653846153846154e-05
Step 418 | epoch: 1.609156626506024
Step 419 | loss: 0.8707457780838013
Step 419 | grad_norm: 5.261709690093994
Step 419 | learning_rate: 4.6410256410256415e-05
Step 419 | epoch: 1.613012048192771
Step 420 | loss: 0.5702612996101379
Step 420 | grad_norm: 3.9711833000183105
Step 420 | learning_rate: 4.6282051282051287e-05
Step 420 | epoch: 1.616867469879518
Step 421 | loss: 0.7233444452285767
Step 421 | grad_norm: 4.97755241394043
Step 421 | learning_rate: 4.615384615384616e-05
Step 421 | epoch: 1.620722891566265
Step 422 | loss: 0.5515346527099609
Step 422 | grad_norm: 4.065104007720947
Step 422 | learning_rate: 4.602564102564102e-05
Step 422 | epoch: 1.624578313253012
Step 423 | loss: 0.7832654118537903
Step 423 | grad_norm: 5.239349842071533
Step 423 | learning_rate: 4.5897435897435895e-05
Step 423 | epoch: 1.628433734939759
Step 424 | loss: 0.9599940180778503
Step 424 | grad_norm: 7.308547496795654
Step 424 | learning_rate: 4.576923076923077e-05
Step 424 | epoch: 1.632289156626506
Step 425 | loss: 0.835584282875061
Step 425 | grad_norm: 4.992557048797607
Step 425 | learning_rate: 4.5641025641025645e-05
Step 425 | epoch: 1.636144578313253
Step 426 | loss: 0.8422642350196838
Step 426 | grad_norm: 5.910422325134277
Step 426 | learning_rate: 4.5512820512820516e-05
Step 426 | epoch: 1.6400000000000001
Step 427 | loss: 0.6886577010154724
Step 427 | grad_norm: 4.61690616607666
Step 427 | learning_rate: 4.538461538461539e-05
Step 427 | epoch: 1.6438554216867471
Step 428 | loss: 0.6844461560249329
Step 428 | grad_norm: 3.9439542293548584
Step 428 | learning_rate: 4.525641025641026e-05
Step 428 | epoch: 1.647710843373494
Step 429 | loss: 1.000498652458191
Step 429 | grad_norm: 5.555530071258545
Step 429 | learning_rate: 4.512820512820513e-05
Step 429 | epoch: 1.651566265060241
Step 430 | loss: 0.8867141604423523
Step 430 | grad_norm: 5.158025741577148
Step 430 | learning_rate: 4.5e-05
Step 430 | epoch: 1.655421686746988
Step 431 | loss: 0.5985577702522278
Step 431 | grad_norm: 4.980506896972656
Step 431 | learning_rate: 4.4871794871794874e-05
Step 431 | epoch: 1.659277108433735
Step 432 | loss: 0.7916141152381897
Step 432 | grad_norm: 4.533975601196289
Step 432 | learning_rate: 4.4743589743589746e-05
Step 432 | epoch: 1.6631325301204818
Step 433 | loss: 0.6664090752601624
Step 433 | grad_norm: 4.687090873718262
Step 433 | learning_rate: 4.461538461538462e-05
Step 433 | epoch: 1.6669879518072288
Step 434 | loss: 0.7739339470863342
Step 434 | grad_norm: 4.659975051879883
Step 434 | learning_rate: 4.448717948717949e-05
Step 434 | epoch: 1.6708433734939758
Step 435 | loss: 0.7585999369621277
Step 435 | grad_norm: 5.575331211090088
Step 435 | learning_rate: 4.435897435897436e-05
Step 435 | epoch: 1.6746987951807228
Step 436 | loss: 0.6587098240852356
Step 436 | grad_norm: 4.467930316925049
Step 436 | learning_rate: 4.423076923076923e-05
Step 436 | epoch: 1.6785542168674699
Step 437 | loss: 0.7686488628387451
Step 437 | grad_norm: 4.502208232879639
Step 437 | learning_rate: 4.4102564102564104e-05
Step 437 | epoch: 1.6824096385542169
Step 438 | loss: 0.6728363633155823
Step 438 | grad_norm: 4.71950626373291
Step 438 | learning_rate: 4.3974358974358976e-05
Step 438 | epoch: 1.686265060240964
Step 439 | loss: 0.5874707698822021
Step 439 | grad_norm: 4.316167831420898
Step 439 | learning_rate: 4.384615384615385e-05
Step 439 | epoch: 1.690120481927711
Step 440 | loss: 0.7911579012870789
Step 440 | grad_norm: 5.237878799438477
Step 440 | learning_rate: 4.371794871794872e-05
Step 440 | epoch: 1.693975903614458
Step 441 | loss: 0.7495341300964355
Step 441 | grad_norm: 5.894880294799805
Step 441 | learning_rate: 4.358974358974359e-05
Step 441 | epoch: 1.697831325301205
Step 442 | loss: 1.2500760555267334
Step 442 | grad_norm: 5.4134063720703125
Step 442 | learning_rate: 4.346153846153846e-05
Step 442 | epoch: 1.701686746987952
Step 443 | loss: 0.9417776465415955
Step 443 | grad_norm: 5.095537185668945
Step 443 | learning_rate: 4.3333333333333334e-05
Step 443 | epoch: 1.7055421686746988
Step 444 | loss: 0.8060537576675415
Step 444 | grad_norm: 5.316760063171387
Step 444 | learning_rate: 4.320512820512821e-05
Step 444 | epoch: 1.7093975903614458
Step 445 | loss: 0.6487427353858948
Step 445 | grad_norm: 4.594454765319824
Step 445 | learning_rate: 4.3076923076923084e-05
Step 445 | epoch: 1.7132530120481928
Step 446 | loss: 0.7998817563056946
Step 446 | grad_norm: 4.103346824645996
Step 446 | learning_rate: 4.294871794871795e-05
Step 446 | epoch: 1.7171084337349396
Step 447 | loss: 0.6652851700782776
Step 447 | grad_norm: 4.182888984680176
Step 447 | learning_rate: 4.282051282051282e-05
Step 447 | epoch: 1.7209638554216866
Step 448 | loss: 0.8286938667297363
Step 448 | grad_norm: 4.154667377471924
Step 448 | learning_rate: 4.269230769230769e-05
Step 448 | epoch: 1.7248192771084336
Step 449 | loss: 0.6859315037727356
Step 449 | grad_norm: 5.106231212615967
Step 449 | learning_rate: 4.2564102564102564e-05
Step 449 | epoch: 1.7286746987951807
Step 450 | loss: 0.8914297819137573
Step 450 | grad_norm: 5.762372970581055
Step 450 | learning_rate: 4.2435897435897435e-05
Step 450 | epoch: 1.7325301204819277
Step 451 | loss: 0.8351938724517822
Step 451 | grad_norm: 5.850734710693359
Step 451 | learning_rate: 4.230769230769231e-05
Step 451 | epoch: 1.7363855421686747
Step 452 | loss: 0.6778438091278076
Step 452 | grad_norm: 4.674446105957031
Step 452 | learning_rate: 4.217948717948718e-05
Step 452 | epoch: 1.7402409638554217
Step 453 | loss: 0.8435021638870239
Step 453 | grad_norm: 5.354106426239014
Step 453 | learning_rate: 4.205128205128206e-05
Step 453 | epoch: 1.7440963855421687
Step 454 | loss: 0.9523261189460754
Step 454 | grad_norm: 5.476362228393555
Step 454 | learning_rate: 4.192307692307693e-05
Step 454 | epoch: 1.7479518072289157
Step 455 | loss: 0.8522621393203735
Step 455 | grad_norm: 5.870628833770752
Step 455 | learning_rate: 4.17948717948718e-05
Step 455 | epoch: 1.7518072289156628
Step 456 | loss: 0.8371882438659668
Step 456 | grad_norm: 6.7914934158325195
Step 456 | learning_rate: 4.166666666666667e-05
Step 456 | epoch: 1.7556626506024098
Step 457 | loss: 1.081517219543457
Step 457 | grad_norm: 5.405633926391602
Step 457 | learning_rate: 4.1538461538461544e-05
Step 457 | epoch: 1.7595180722891566
Step 458 | loss: 0.7251718640327454
Step 458 | grad_norm: 4.296842575073242
Step 458 | learning_rate: 4.1410256410256415e-05
Step 458 | epoch: 1.7633734939759036
Step 459 | loss: 0.7240304350852966
Step 459 | grad_norm: 4.174404621124268
Step 459 | learning_rate: 4.128205128205128e-05
Step 459 | epoch: 1.7672289156626506
Step 460 | loss: 0.8182626962661743
Step 460 | grad_norm: 5.301703929901123
Step 460 | learning_rate: 4.115384615384615e-05
Step 460 | epoch: 1.7710843373493976
Step 461 | loss: 0.6365004181861877
Step 461 | grad_norm: 4.222678184509277
Step 461 | learning_rate: 4.1025641025641023e-05
Step 461 | epoch: 1.7749397590361444
Step 462 | loss: 0.7102962732315063
Step 462 | grad_norm: 4.069265365600586
Step 462 | learning_rate: 4.0897435897435895e-05
Step 462 | epoch: 1.7787951807228914
Step 463 | loss: 0.8848065137863159
Step 463 | grad_norm: 5.375572681427002
Step 463 | learning_rate: 4.0769230769230773e-05
Step 463 | epoch: 1.7826506024096385
Step 464 | loss: 0.5237175226211548
Step 464 | grad_norm: 3.9271833896636963
Step 464 | learning_rate: 4.0641025641025645e-05
Step 464 | epoch: 1.7865060240963855
Step 465 | loss: 0.6275966167449951
Step 465 | grad_norm: 4.616451263427734
Step 465 | learning_rate: 4.051282051282052e-05
Step 465 | epoch: 1.7903614457831325
Step 466 | loss: 0.6421717405319214
Step 466 | grad_norm: 5.196774005889893
Step 466 | learning_rate: 4.038461538461539e-05
Step 466 | epoch: 1.7942168674698795
Step 467 | loss: 0.971478283405304
Step 467 | grad_norm: 5.7253642082214355
Step 467 | learning_rate: 4.025641025641026e-05
Step 467 | epoch: 1.7980722891566265
Step 468 | loss: 0.7703247666358948
Step 468 | grad_norm: 4.895947456359863
Step 468 | learning_rate: 4.012820512820513e-05
Step 468 | epoch: 1.8019277108433736
Step 469 | loss: 0.8324973583221436
Step 469 | grad_norm: 5.182398796081543
Step 469 | learning_rate: 4e-05
Step 469 | epoch: 1.8057831325301206
Step 470 | loss: 0.9332417249679565
Step 470 | grad_norm: 5.650416374206543
Step 470 | learning_rate: 3.9871794871794875e-05
Step 470 | epoch: 1.8096385542168676
Step 471 | loss: 0.6819545030593872
Step 471 | grad_norm: 4.84782075881958
Step 471 | learning_rate: 3.974358974358974e-05
Step 471 | epoch: 1.8134939759036146
Step 472 | loss: 0.7731294631958008
Step 472 | grad_norm: 4.803489685058594
Step 472 | learning_rate: 3.961538461538462e-05
Step 472 | epoch: 1.8173493975903614
Step 473 | loss: 0.7967846989631653
Step 473 | grad_norm: 5.3919501304626465
Step 473 | learning_rate: 3.948717948717949e-05
Step 473 | epoch: 1.8212048192771084
Step 474 | loss: 0.7216955423355103
Step 474 | grad_norm: 5.126811504364014
Step 474 | learning_rate: 3.935897435897436e-05
Step 474 | epoch: 1.8250602409638554
Step 475 | loss: 0.6715394258499146
Step 475 | grad_norm: 4.731848239898682
Step 475 | learning_rate: 3.923076923076923e-05
Step 475 | epoch: 1.8289156626506025
Step 476 | loss: 0.6210336685180664
Step 476 | grad_norm: 4.636487007141113
Step 476 | learning_rate: 3.9102564102564105e-05
Step 476 | epoch: 1.8327710843373493
Step 477 | loss: 0.7581440806388855
Step 477 | grad_norm: 5.064418315887451
Step 477 | learning_rate: 3.8974358974358976e-05
Step 477 | epoch: 1.8366265060240963
Step 478 | loss: 0.7114556431770325
Step 478 | grad_norm: 5.751341819763184
Step 478 | learning_rate: 3.884615384615385e-05
Step 478 | epoch: 1.8404819277108433
Step 479 | loss: 0.7030566930770874
Step 479 | grad_norm: 6.041203022003174
Step 479 | learning_rate: 3.871794871794872e-05
Step 479 | epoch: 1.8443373493975903
Step 480 | loss: 0.4451732635498047
Step 480 | grad_norm: 4.097445964813232
Step 480 | learning_rate: 3.858974358974359e-05
Step 480 | epoch: 1.8481927710843373
Step 481 | loss: 0.7477889657020569
Step 481 | grad_norm: 4.781813144683838
Step 481 | learning_rate: 3.846153846153846e-05
Step 481 | epoch: 1.8520481927710843
Step 482 | loss: 0.9167757034301758
Step 482 | grad_norm: 4.806715965270996
Step 482 | learning_rate: 3.8333333333333334e-05
Step 482 | epoch: 1.8559036144578314
Step 483 | loss: 0.6322520971298218
Step 483 | grad_norm: 4.377440929412842
Step 483 | learning_rate: 3.8205128205128206e-05
Step 483 | epoch: 1.8597590361445784
Step 484 | loss: 0.8166347146034241
Step 484 | grad_norm: 6.197656631469727
Step 484 | learning_rate: 3.807692307692308e-05
Step 484 | epoch: 1.8636144578313254
Step 485 | loss: 0.6776001453399658
Step 485 | grad_norm: 4.457208156585693
Step 485 | learning_rate: 3.794871794871795e-05
Step 485 | epoch: 1.8674698795180724
Step 486 | loss: 0.8257378339767456
Step 486 | grad_norm: 5.982085704803467
Step 486 | learning_rate: 3.782051282051282e-05
Step 486 | epoch: 1.8713253012048194
Step 487 | loss: 0.8911901712417603
Step 487 | grad_norm: 4.977482795715332
Step 487 | learning_rate: 3.769230769230769e-05
Step 487 | epoch: 1.8751807228915662
Step 488 | loss: 0.5644707083702087
Step 488 | grad_norm: 4.499222755432129
Step 488 | learning_rate: 3.7564102564102564e-05
Step 488 | epoch: 1.8790361445783132
Step 489 | loss: 0.9178618788719177
Step 489 | grad_norm: 5.3245930671691895
Step 489 | learning_rate: 3.7435897435897436e-05
Step 489 | epoch: 1.8828915662650603
Step 490 | loss: 0.5977732539176941
Step 490 | grad_norm: 5.4814300537109375
Step 490 | learning_rate: 3.730769230769231e-05
Step 490 | epoch: 1.886746987951807
Step 491 | loss: 0.6262009739875793
Step 491 | grad_norm: 4.485780715942383
Step 491 | learning_rate: 3.717948717948718e-05
Step 491 | epoch: 1.890602409638554
Step 492 | loss: 0.6862199306488037
Step 492 | grad_norm: 5.22730827331543
Step 492 | learning_rate: 3.705128205128206e-05
Step 492 | epoch: 1.894457831325301
Step 493 | loss: 0.8145763874053955
Step 493 | grad_norm: 5.686051368713379
Step 493 | learning_rate: 3.692307692307693e-05
Step 493 | epoch: 1.8983132530120481
Step 494 | loss: 0.9191887974739075
Step 494 | grad_norm: 5.895735263824463
Step 494 | learning_rate: 3.67948717948718e-05
Step 494 | epoch: 1.9021686746987951
Step 495 | loss: 0.8489360213279724
Step 495 | grad_norm: 5.027251243591309
Step 495 | learning_rate: 3.6666666666666666e-05
Step 495 | epoch: 1.9060240963855422
Step 496 | loss: 0.6447924971580505
Step 496 | grad_norm: 4.66211462020874
Step 496 | learning_rate: 3.653846153846154e-05
Step 496 | epoch: 1.9098795180722892
Step 497 | loss: 0.7902037501335144
Step 497 | grad_norm: 4.699656963348389
Step 497 | learning_rate: 3.641025641025641e-05
Step 497 | epoch: 1.9137349397590362
Step 498 | loss: 0.8956214189529419
Step 498 | grad_norm: 4.980558395385742
Step 498 | learning_rate: 3.628205128205128e-05
Step 498 | epoch: 1.9175903614457832
Step 499 | loss: 0.7088475227355957
Step 499 | grad_norm: 5.55615758895874
Step 499 | learning_rate: 3.615384615384615e-05
Step 499 | epoch: 1.9214457831325302
Step 500 | loss: 0.8134387731552124
Step 500 | grad_norm: 4.754222393035889
Step 500 | learning_rate: 3.6025641025641024e-05
Step 500 | epoch: 1.9253012048192772
Step 501 | loss: 0.8311319947242737
Step 501 | grad_norm: 5.027345657348633
Step 501 | learning_rate: 3.58974358974359e-05
Step 501 | epoch: 1.929156626506024
Step 502 | loss: 0.9102064967155457
Step 502 | grad_norm: 5.215268135070801
Step 502 | learning_rate: 3.5769230769230774e-05
Step 502 | epoch: 1.933012048192771
Step 503 | loss: 0.7006209492683411
Step 503 | grad_norm: 4.633725643157959
Step 503 | learning_rate: 3.5641025641025646e-05
Step 503 | epoch: 1.936867469879518
Step 504 | loss: 1.0850729942321777
Step 504 | grad_norm: 5.2309160232543945
Step 504 | learning_rate: 3.551282051282052e-05
Step 504 | epoch: 1.940722891566265
Step 505 | loss: 0.6134527921676636
Step 505 | grad_norm: 4.3197245597839355
Step 505 | learning_rate: 3.538461538461539e-05
Step 505 | epoch: 1.944578313253012
Step 506 | loss: 0.5390884876251221
Step 506 | grad_norm: 4.814069747924805
Step 506 | learning_rate: 3.525641025641026e-05
Step 506 | epoch: 1.948433734939759
Step 507 | loss: 0.6540929079055786
Step 507 | grad_norm: 5.2991533279418945
Step 507 | learning_rate: 3.5128205128205125e-05
Step 507 | epoch: 1.952289156626506
Step 508 | loss: 0.7685287594795227
Step 508 | grad_norm: 4.987936019897461
Step 508 | learning_rate: 3.5e-05
Step 508 | epoch: 1.956144578313253
Step 509 | loss: 0.6635140180587769
Step 509 | grad_norm: 6.049130916595459
Step 509 | learning_rate: 3.487179487179487e-05
Step 509 | epoch: 1.96
Step 510 | loss: 0.6925814747810364
Step 510 | grad_norm: 6.36752986907959
Step 510 | learning_rate: 3.474358974358975e-05
Step 510 | epoch: 1.963855421686747
Step 511 | loss: 0.8145028352737427
Step 511 | grad_norm: 5.183272361755371
Step 511 | learning_rate: 3.461538461538462e-05
Step 511 | epoch: 1.967710843373494
Step 512 | loss: 0.6802260279655457
Step 512 | grad_norm: 4.502434730529785
Step 512 | learning_rate: 3.448717948717949e-05
Step 512 | epoch: 1.971566265060241
Step 513 | loss: 0.7600406408309937
Step 513 | grad_norm: 5.401870250701904
Step 513 | learning_rate: 3.435897435897436e-05
Step 513 | epoch: 1.975421686746988
Step 514 | loss: 0.8242415189743042
Step 514 | grad_norm: 5.483624458312988
Step 514 | learning_rate: 3.4230769230769234e-05
Step 514 | epoch: 1.979277108433735
Step 515 | loss: 0.708123505115509
Step 515 | grad_norm: 4.454649448394775
Step 515 | learning_rate: 3.4102564102564105e-05
Step 515 | epoch: 1.983132530120482
Step 516 | loss: 0.665159285068512
Step 516 | grad_norm: 4.623865127563477
Step 516 | learning_rate: 3.397435897435898e-05
Step 516 | epoch: 1.9869879518072289
Step 517 | loss: 0.8075551986694336
Step 517 | grad_norm: 5.3198065757751465
Step 517 | learning_rate: 3.384615384615385e-05
Step 517 | epoch: 1.9908433734939759
Step 518 | loss: 0.5101699829101562
Step 518 | grad_norm: 4.8517584800720215
Step 518 | learning_rate: 3.371794871794872e-05
Step 518 | epoch: 1.994698795180723
Step 519 | loss: 0.9360641837120056
Step 519 | grad_norm: 5.424812316894531
Step 519 | learning_rate: 3.358974358974359e-05
Step 519 | epoch: 1.99855421686747
Step 520 | loss: 0.9346104264259338
Step 520 | grad_norm: 10.249584197998047
Step 520 | learning_rate: 3.346153846153846e-05
Step 520 | epoch: 2.0
Step 521 | loss: 0.5912014842033386
Step 521 | grad_norm: 4.117559432983398
Step 521 | learning_rate: 3.3333333333333335e-05
Step 521 | epoch: 2.003855421686747
Step 522 | loss: 0.551400363445282
Step 522 | grad_norm: 3.861362934112549
Step 522 | learning_rate: 3.3205128205128207e-05
Step 522 | epoch: 2.007710843373494
Step 523 | loss: 0.35727232694625854
Step 523 | grad_norm: 3.502697229385376
Step 523 | learning_rate: 3.307692307692308e-05
Step 523 | epoch: 2.011566265060241
Step 524 | loss: 0.5616297125816345
Step 524 | grad_norm: 4.105241298675537
Step 524 | learning_rate: 3.294871794871795e-05
Step 524 | epoch: 2.015421686746988
Step 525 | loss: 0.7869916558265686
Step 525 | grad_norm: 4.524939060211182
Step 525 | learning_rate: 3.282051282051282e-05
Step 525 | epoch: 2.019277108433735
Step 526 | loss: 0.678977370262146
Step 526 | grad_norm: 4.517131805419922
Step 526 | learning_rate: 3.269230769230769e-05
Step 526 | epoch: 2.023132530120482
Step 527 | loss: 0.5406476855278015
Step 527 | grad_norm: 4.034510612487793
Step 527 | learning_rate: 3.2564102564102565e-05
Step 527 | epoch: 2.026987951807229
Step 528 | loss: 0.5639787316322327
Step 528 | grad_norm: 3.9388043880462646
Step 528 | learning_rate: 3.2435897435897436e-05
Step 528 | epoch: 2.0308433734939757
Step 529 | loss: 0.5127039551734924
Step 529 | grad_norm: 3.7688117027282715
Step 529 | learning_rate: 3.230769230769231e-05
Step 529 | epoch: 2.0346987951807227
Step 530 | loss: 0.4612334370613098
Step 530 | grad_norm: 4.506747245788574
Step 530 | learning_rate: 3.2179487179487186e-05
Step 530 | epoch: 2.0385542168674697
Step 531 | loss: 0.5543884038925171
Step 531 | grad_norm: 4.392569541931152
Step 531 | learning_rate: 3.205128205128206e-05
Step 531 | epoch: 2.0424096385542168
Step 532 | loss: 0.6169107556343079
Step 532 | grad_norm: 4.403848171234131
Step 532 | learning_rate: 3.192307692307692e-05
Step 532 | epoch: 2.0462650602409638
Step 533 | loss: 0.5276557803153992
Step 533 | grad_norm: 4.428406238555908
Step 533 | learning_rate: 3.1794871794871795e-05
Step 533 | epoch: 2.050120481927711
Step 534 | loss: 0.5539615154266357
Step 534 | grad_norm: 5.580643177032471
Step 534 | learning_rate: 3.1666666666666666e-05
Step 534 | epoch: 2.053975903614458
Step 535 | loss: 0.4284185469150543
Step 535 | grad_norm: 5.398129940032959
Step 535 | learning_rate: 3.153846153846154e-05
Step 535 | epoch: 2.057831325301205
Step 536 | loss: 0.5119053721427917
Step 536 | grad_norm: 5.763516426086426
Step 536 | learning_rate: 3.141025641025641e-05
Step 536 | epoch: 2.061686746987952
Step 537 | loss: 0.6792646050453186
Step 537 | grad_norm: 6.712789535522461
Step 537 | learning_rate: 3.128205128205128e-05
Step 537 | epoch: 2.065542168674699
Step 538 | loss: 0.5034157633781433
Step 538 | grad_norm: 6.172177314758301
Step 538 | learning_rate: 3.115384615384615e-05
Step 538 | epoch: 2.069397590361446
Step 539 | loss: 0.49131983518600464
Step 539 | grad_norm: 5.789994716644287
Step 539 | learning_rate: 3.102564102564103e-05
Step 539 | epoch: 2.073253012048193
Step 540 | loss: 0.5643146634101868
Step 540 | grad_norm: 5.9060797691345215
Step 540 | learning_rate: 3.08974358974359e-05
Step 540 | epoch: 2.07710843373494
Step 541 | loss: 0.48796701431274414
Step 541 | grad_norm: 5.223215579986572
Step 541 | learning_rate: 3.0769230769230774e-05
Step 541 | epoch: 2.080963855421687
Step 542 | loss: 0.5907905697822571
Step 542 | grad_norm: 5.9423956871032715
Step 542 | learning_rate: 3.0641025641025646e-05
Step 542 | epoch: 2.0848192771084335
Step 543 | loss: 0.6094768047332764
Step 543 | grad_norm: 6.584835529327393
Step 543 | learning_rate: 3.0512820512820518e-05
Step 543 | epoch: 2.0886746987951805
Step 544 | loss: 0.3572540581226349
Step 544 | grad_norm: 5.214837074279785
Step 544 | learning_rate: 3.0384615384615382e-05
Step 544 | epoch: 2.0925301204819275
Step 545 | loss: 0.6185732483863831
Step 545 | grad_norm: 5.9049391746521
Step 545 | learning_rate: 3.0256410256410257e-05
Step 545 | epoch: 2.0963855421686746
Step 546 | loss: 0.5369314551353455
Step 546 | grad_norm: 5.124236583709717
Step 546 | learning_rate: 3.012820512820513e-05
Step 546 | epoch: 2.1002409638554216
Step 547 | loss: 0.6158018112182617
Step 547 | grad_norm: 6.598998069763184
Step 547 | learning_rate: 3e-05
Step 547 | epoch: 2.1040963855421686
Step 548 | loss: 0.6581613421440125
Step 548 | grad_norm: 6.967655181884766
Step 548 | learning_rate: 2.9871794871794872e-05
Step 548 | epoch: 2.1079518072289156
Step 549 | loss: 0.45052826404571533
Step 549 | grad_norm: 5.302619457244873
Step 549 | learning_rate: 2.9743589743589744e-05
Step 549 | epoch: 2.1118072289156626
Step 550 | loss: 0.37792596220970154
Step 550 | grad_norm: 4.972234725952148
Step 550 | learning_rate: 2.9615384615384616e-05
Step 550 | epoch: 2.1156626506024097
Step 551 | loss: 0.562609851360321
Step 551 | grad_norm: 4.615170478820801
Step 551 | learning_rate: 2.948717948717949e-05
Step 551 | epoch: 2.1195180722891567
Step 552 | loss: 0.35133516788482666
Step 552 | grad_norm: 4.234630584716797
Step 552 | learning_rate: 2.9358974358974362e-05
Step 552 | epoch: 2.1233734939759037
Step 553 | loss: 0.3994683027267456
Step 553 | grad_norm: 5.050514221191406
Step 553 | learning_rate: 2.9230769230769234e-05
Step 553 | epoch: 2.1272289156626507
Step 554 | loss: 0.5447883605957031
Step 554 | grad_norm: 6.275166988372803
Step 554 | learning_rate: 2.9102564102564106e-05
Step 554 | epoch: 2.1310843373493977
Step 555 | loss: 0.364009827375412
Step 555 | grad_norm: 5.255950927734375
Step 555 | learning_rate: 2.8974358974358977e-05
Step 555 | epoch: 2.1349397590361447
Step 556 | loss: 0.4079715609550476
Step 556 | grad_norm: 5.0104899406433105
Step 556 | learning_rate: 2.8846153846153845e-05
Step 556 | epoch: 2.1387951807228918
Step 557 | loss: 0.4944695234298706
Step 557 | grad_norm: 5.9523606300354
Step 557 | learning_rate: 2.8717948717948717e-05
Step 557 | epoch: 2.1426506024096383
Step 558 | loss: 0.3956790864467621
Step 558 | grad_norm: 5.240602016448975
Step 558 | learning_rate: 2.858974358974359e-05
Step 558 | epoch: 2.1465060240963854
Step 559 | loss: 0.5677288770675659
Step 559 | grad_norm: 5.449791431427002
Step 559 | learning_rate: 2.846153846153846e-05
Step 559 | epoch: 2.1503614457831324
Step 560 | loss: 0.56247878074646
Step 560 | grad_norm: 4.950850963592529
Step 560 | learning_rate: 2.8333333333333335e-05
Step 560 | epoch: 2.1542168674698794
Step 561 | loss: 0.6338803172111511
Step 561 | grad_norm: 6.371428966522217
Step 561 | learning_rate: 2.8205128205128207e-05
Step 561 | epoch: 2.1580722891566264
Step 562 | loss: 0.6087050437927246
Step 562 | grad_norm: 6.572353363037109
Step 562 | learning_rate: 2.807692307692308e-05
Step 562 | epoch: 2.1619277108433734
Step 563 | loss: 0.37711572647094727
Step 563 | grad_norm: 5.077463150024414
Step 563 | learning_rate: 2.794871794871795e-05
Step 563 | epoch: 2.1657831325301204
Step 564 | loss: 0.7414207458496094
Step 564 | grad_norm: 8.356400489807129
Step 564 | learning_rate: 2.7820512820512822e-05
Step 564 | epoch: 2.1696385542168675
Step 565 | loss: 0.45665085315704346
Step 565 | grad_norm: 5.733005046844482
Step 565 | learning_rate: 2.7692307692307694e-05
Step 565 | epoch: 2.1734939759036145
Step 566 | loss: 0.6624301671981812
Step 566 | grad_norm: 5.3722825050354
Step 566 | learning_rate: 2.756410256410257e-05
Step 566 | epoch: 2.1773493975903615
Step 567 | loss: 0.4331710934638977
Step 567 | grad_norm: 6.395384788513184
Step 567 | learning_rate: 2.743589743589744e-05
Step 567 | epoch: 2.1812048192771085
Step 568 | loss: 0.5245120525360107
Step 568 | grad_norm: 5.777350902557373
Step 568 | learning_rate: 2.7307692307692305e-05
Step 568 | epoch: 2.1850602409638555
Step 569 | loss: 0.4691837728023529
Step 569 | grad_norm: 5.696437358856201
Step 569 | learning_rate: 2.717948717948718e-05
Step 569 | epoch: 2.1889156626506026
Step 570 | loss: 0.5239256024360657
Step 570 | grad_norm: 5.124771595001221
Step 570 | learning_rate: 2.705128205128205e-05
Step 570 | epoch: 2.1927710843373496
Step 571 | loss: 0.49737319350242615
Step 571 | grad_norm: 5.831719398498535
Step 571 | learning_rate: 2.6923076923076923e-05
Step 571 | epoch: 2.1966265060240966
Step 572 | loss: 0.4249442219734192
Step 572 | grad_norm: 6.0821404457092285
Step 572 | learning_rate: 2.6794871794871795e-05
Step 572 | epoch: 2.200481927710843
Step 573 | loss: 0.4106360077857971
Step 573 | grad_norm: 5.363677024841309
Step 573 | learning_rate: 2.6666666666666667e-05
Step 573 | epoch: 2.20433734939759
Step 574 | loss: 0.4380335211753845
Step 574 | grad_norm: 5.635576248168945
Step 574 | learning_rate: 2.6538461538461538e-05
Step 574 | epoch: 2.208192771084337
Step 575 | loss: 0.4226301908493042
Step 575 | grad_norm: 5.392091274261475
Step 575 | learning_rate: 2.6410256410256413e-05
Step 575 | epoch: 2.212048192771084
Step 576 | loss: 0.39967262744903564
Step 576 | grad_norm: 4.902142524719238
Step 576 | learning_rate: 2.6282051282051285e-05
Step 576 | epoch: 2.2159036144578312
Step 577 | loss: 0.5049641728401184
Step 577 | grad_norm: 5.455925941467285
Step 577 | learning_rate: 2.6153846153846157e-05
Step 577 | epoch: 2.2197590361445783
Step 578 | loss: 0.40897464752197266
Step 578 | grad_norm: 5.111059665679932
Step 578 | learning_rate: 2.6025641025641028e-05
Step 578 | epoch: 2.2236144578313253
Step 579 | loss: 0.5363670587539673
Step 579 | grad_norm: 7.3736701011657715
Step 579 | learning_rate: 2.58974358974359e-05
Step 579 | epoch: 2.2274698795180723
Step 580 | loss: 0.4152596890926361
Step 580 | grad_norm: 5.618178367614746
Step 580 | learning_rate: 2.5769230769230768e-05
Step 580 | epoch: 2.2313253012048193
Step 581 | loss: 0.5570169687271118
Step 581 | grad_norm: 5.760558128356934
Step 581 | learning_rate: 2.564102564102564e-05
Step 581 | epoch: 2.2351807228915663
Step 582 | loss: 0.6024489998817444
Step 582 | grad_norm: 5.726415634155273
Step 582 | learning_rate: 2.551282051282051e-05
Step 582 | epoch: 2.2390361445783133
Step 583 | loss: 0.5823825597763062
Step 583 | grad_norm: 5.549136161804199
Step 583 | learning_rate: 2.5384615384615383e-05
Step 583 | epoch: 2.2428915662650604
Step 584 | loss: 0.44364994764328003
Step 584 | grad_norm: 5.80392599105835
Step 584 | learning_rate: 2.5256410256410258e-05
Step 584 | epoch: 2.2467469879518074
Step 585 | loss: 0.7312657833099365
Step 585 | grad_norm: 6.469478607177734
Step 585 | learning_rate: 2.512820512820513e-05
Step 585 | epoch: 2.2506024096385544
Step 586 | loss: 0.5389180779457092
Step 586 | grad_norm: 7.066313743591309
Step 586 | learning_rate: 2.5e-05
Step 586 | epoch: 2.2544578313253014
Step 587 | loss: 0.4917103052139282
Step 587 | grad_norm: 6.569628715515137
Step 587 | learning_rate: 2.4871794871794873e-05
Step 587 | epoch: 2.258313253012048
Step 588 | loss: 0.5804553627967834
Step 588 | grad_norm: 7.076032638549805
Step 588 | learning_rate: 2.4743589743589744e-05
Step 588 | epoch: 2.262168674698795
Step 589 | loss: 0.6292718648910522
Step 589 | grad_norm: 6.961218357086182
Step 589 | learning_rate: 2.461538461538462e-05
Step 589 | epoch: 2.266024096385542
Step 590 | loss: 0.361884206533432
Step 590 | grad_norm: 6.618408203125
Step 590 | learning_rate: 2.4487179487179488e-05
Step 590 | epoch: 2.269879518072289
Step 591 | loss: 0.454568475484848
Step 591 | grad_norm: 7.304601669311523
Step 591 | learning_rate: 2.435897435897436e-05
Step 591 | epoch: 2.273734939759036
Step 592 | loss: 0.4336719810962677
Step 592 | grad_norm: 6.093729019165039
Step 592 | learning_rate: 2.423076923076923e-05
Step 592 | epoch: 2.277590361445783
Step 593 | loss: 0.6084058880805969
Step 593 | grad_norm: 5.896700859069824
Step 593 | learning_rate: 2.4102564102564103e-05
Step 593 | epoch: 2.28144578313253
Step 594 | loss: 0.42267927527427673
Step 594 | grad_norm: 5.2977294921875
Step 594 | learning_rate: 2.3974358974358978e-05
Step 594 | epoch: 2.285301204819277
Step 595 | loss: 0.4186374247074127
Step 595 | grad_norm: 6.249612331390381
Step 595 | learning_rate: 2.384615384615385e-05
Step 595 | epoch: 2.289156626506024
Step 596 | loss: 0.5481505393981934
Step 596 | grad_norm: 5.684957027435303
Step 596 | learning_rate: 2.3717948717948718e-05
Step 596 | epoch: 2.293012048192771
Step 597 | loss: 0.45872411131858826
Step 597 | grad_norm: 6.18916130065918
Step 597 | learning_rate: 2.358974358974359e-05
Step 597 | epoch: 2.296867469879518
Step 598 | loss: 0.7607665061950684
Step 598 | grad_norm: 7.524812698364258
Step 598 | learning_rate: 2.3461538461538464e-05
Step 598 | epoch: 2.300722891566265
Step 599 | loss: 0.40196260809898376
Step 599 | grad_norm: 5.186753749847412
Step 599 | learning_rate: 2.3333333333333336e-05
Step 599 | epoch: 2.304578313253012
Step 600 | loss: 0.347402423620224
Step 600 | grad_norm: 4.685627460479736
Step 600 | learning_rate: 2.3205128205128207e-05
Step 600 | epoch: 2.3084337349397592
Step 601 | loss: 0.6606467962265015
Step 601 | grad_norm: 6.248222827911377
Step 601 | learning_rate: 2.307692307692308e-05
Step 601 | epoch: 2.3122891566265062
Step 602 | loss: 0.518771231174469
Step 602 | grad_norm: 6.161194801330566
Step 602 | learning_rate: 2.2948717948717947e-05
Step 602 | epoch: 2.316144578313253
Step 603 | loss: 0.539092481136322
Step 603 | grad_norm: 5.864007949829102
Step 603 | learning_rate: 2.2820512820512822e-05
Step 603 | epoch: 2.32
Step 604 | loss: 0.585258960723877
Step 604 | grad_norm: 6.380211353302002
Step 604 | learning_rate: 2.2692307692307694e-05
Step 604 | epoch: 2.323855421686747
Step 605 | loss: 0.5874388813972473
Step 605 | grad_norm: 6.232498645782471
Step 605 | learning_rate: 2.2564102564102566e-05
Step 605 | epoch: 2.327710843373494
Step 606 | loss: 0.4482637941837311
Step 606 | grad_norm: 5.307700157165527
Step 606 | learning_rate: 2.2435897435897437e-05
Step 606 | epoch: 2.331566265060241
Step 607 | loss: 0.552592396736145
Step 607 | grad_norm: 5.882238388061523
Step 607 | learning_rate: 2.230769230769231e-05
Step 607 | epoch: 2.335421686746988
Step 608 | loss: 0.4657285809516907
Step 608 | grad_norm: 5.893327236175537
Step 608 | learning_rate: 2.217948717948718e-05
Step 608 | epoch: 2.339277108433735
Step 609 | loss: 0.48159319162368774
Step 609 | grad_norm: 5.572006702423096
Step 609 | learning_rate: 2.2051282051282052e-05
Step 609 | epoch: 2.343132530120482
Step 610 | loss: 0.5774019360542297
Step 610 | grad_norm: 6.629551887512207
Step 610 | learning_rate: 2.1923076923076924e-05
Step 610 | epoch: 2.346987951807229
Step 611 | loss: 0.5561672449111938
Step 611 | grad_norm: 5.495358467102051
Step 611 | learning_rate: 2.1794871794871795e-05
Step 611 | epoch: 2.350843373493976
Step 612 | loss: 0.417437344789505
Step 612 | grad_norm: 4.987422943115234
Step 612 | learning_rate: 2.1666666666666667e-05
Step 612 | epoch: 2.354698795180723
Step 613 | loss: 0.6132215857505798
Step 613 | grad_norm: 6.477438926696777
Step 613 | learning_rate: 2.1538461538461542e-05
Step 613 | epoch: 2.35855421686747
Step 614 | loss: 0.5076972842216492
Step 614 | grad_norm: 5.333950519561768
Step 614 | learning_rate: 2.141025641025641e-05
Step 614 | epoch: 2.362409638554217
Step 615 | loss: 0.45535510778427124
Step 615 | grad_norm: 4.982067584991455
Step 615 | learning_rate: 2.1282051282051282e-05
Step 615 | epoch: 2.3662650602409636
Step 616 | loss: 0.5641018152236938
Step 616 | grad_norm: 5.903636455535889
Step 616 | learning_rate: 2.1153846153846154e-05
Step 616 | epoch: 2.370120481927711
Step 617 | loss: 0.5761065483093262
Step 617 | grad_norm: 6.233426570892334
Step 617 | learning_rate: 2.102564102564103e-05
Step 617 | epoch: 2.3739759036144576
Step 618 | loss: 0.38812774419784546
Step 618 | grad_norm: 4.851718902587891
Step 618 | learning_rate: 2.08974358974359e-05
Step 618 | epoch: 2.3778313253012047
Step 619 | loss: 0.4577886760234833
Step 619 | grad_norm: 4.792514801025391
Step 619 | learning_rate: 2.0769230769230772e-05
Step 619 | epoch: 2.3816867469879517
Step 620 | loss: 0.5958594083786011
Step 620 | grad_norm: 5.52467679977417
Step 620 | learning_rate: 2.064102564102564e-05
Step 620 | epoch: 2.3855421686746987
Step 621 | loss: 0.48501625657081604
Step 621 | grad_norm: 6.123157024383545
Step 621 | learning_rate: 2.0512820512820512e-05
Step 621 | epoch: 2.3893975903614457
Step 622 | loss: 0.3943389058113098
Step 622 | grad_norm: 5.714745998382568
Step 622 | learning_rate: 2.0384615384615387e-05
Step 622 | epoch: 2.3932530120481927
Step 623 | loss: 0.5894173979759216
Step 623 | grad_norm: 5.169650077819824
Step 623 | learning_rate: 2.025641025641026e-05
Step 623 | epoch: 2.3971084337349398
Step 624 | loss: 0.5396572947502136
Step 624 | grad_norm: 7.368788242340088
Step 624 | learning_rate: 2.012820512820513e-05
Step 624 | epoch: 2.4009638554216868
Step 625 | loss: 0.6261105537414551
Step 625 | grad_norm: 6.337373733520508
Step 625 | learning_rate: 2e-05
Step 625 | epoch: 2.404819277108434
Step 626 | loss: 0.49198856949806213
Step 626 | grad_norm: 5.84147310256958
Step 626 | learning_rate: 1.987179487179487e-05
Step 626 | epoch: 2.408674698795181
Step 627 | loss: 0.42883509397506714
Step 627 | grad_norm: 4.94849157333374
Step 627 | learning_rate: 1.9743589743589745e-05
Step 627 | epoch: 2.412530120481928
Step 628 | loss: 0.5474948287010193
Step 628 | grad_norm: 6.8463897705078125
Step 628 | learning_rate: 1.9615384615384617e-05
Step 628 | epoch: 2.416385542168675
Step 629 | loss: 0.5399447083473206
Step 629 | grad_norm: 5.387273788452148
Step 629 | learning_rate: 1.9487179487179488e-05
Step 629 | epoch: 2.420240963855422
Step 630 | loss: 0.33713313937187195
Step 630 | grad_norm: 5.052593231201172
Step 630 | learning_rate: 1.935897435897436e-05
Step 630 | epoch: 2.4240963855421684
Step 631 | loss: 0.5175109505653381
Step 631 | grad_norm: 5.477169036865234
Step 631 | learning_rate: 1.923076923076923e-05
Step 631 | epoch: 2.427951807228916
Step 632 | loss: 0.5761952996253967
Step 632 | grad_norm: 6.21051025390625
Step 632 | learning_rate: 1.9102564102564103e-05
Step 632 | epoch: 2.4318072289156625
Step 633 | loss: 0.6851773262023926
Step 633 | grad_norm: 6.722020149230957
Step 633 | learning_rate: 1.8974358974358975e-05
Step 633 | epoch: 2.4356626506024095
Step 634 | loss: 0.5279184579849243
Step 634 | grad_norm: 7.3405022621154785
Step 634 | learning_rate: 1.8846153846153846e-05
Step 634 | epoch: 2.4395180722891565
Step 635 | loss: 0.7803739309310913
Step 635 | grad_norm: 6.16085958480835
Step 635 | learning_rate: 1.8717948717948718e-05
Step 635 | epoch: 2.4433734939759035
Step 636 | loss: 0.3224346935749054
Step 636 | grad_norm: 4.458934783935547
Step 636 | learning_rate: 1.858974358974359e-05
Step 636 | epoch: 2.4472289156626506
Step 637 | loss: 0.5945824384689331
Step 637 | grad_norm: 7.2903265953063965
Step 637 | learning_rate: 1.8461538461538465e-05
Step 637 | epoch: 2.4510843373493976
Step 638 | loss: 0.47983935475349426
Step 638 | grad_norm: 6.219257831573486
Step 638 | learning_rate: 1.8333333333333333e-05
Step 638 | epoch: 2.4549397590361446
Step 639 | loss: 0.5699945688247681
Step 639 | grad_norm: 6.421538829803467
Step 639 | learning_rate: 1.8205128205128204e-05
Step 639 | epoch: 2.4587951807228916
Step 640 | loss: 0.4806171953678131
Step 640 | grad_norm: 6.3572821617126465
Step 640 | learning_rate: 1.8076923076923076e-05
Step 640 | epoch: 2.4626506024096386
Step 641 | loss: 0.5472562909126282
Step 641 | grad_norm: 5.8278584480285645
Step 641 | learning_rate: 1.794871794871795e-05
Step 641 | epoch: 2.4665060240963856
Step 642 | loss: 0.4550703167915344
Step 642 | grad_norm: 6.549455165863037
Step 642 | learning_rate: 1.7820512820512823e-05
Step 642 | epoch: 2.4703614457831327
Step 643 | loss: 0.542004406452179
Step 643 | grad_norm: 5.949909687042236
Step 643 | learning_rate: 1.7692307692307694e-05
Step 643 | epoch: 2.4742168674698797
Step 644 | loss: 0.38659992814064026
Step 644 | grad_norm: 5.939744472503662
Step 644 | learning_rate: 1.7564102564102563e-05
Step 644 | epoch: 2.4780722891566267
Step 645 | loss: 0.5437167286872864
Step 645 | grad_norm: 7.2374067306518555
Step 645 | learning_rate: 1.7435897435897434e-05
Step 645 | epoch: 2.4819277108433733
Step 646 | loss: 0.41970086097717285
Step 646 | grad_norm: 4.789544105529785
Step 646 | learning_rate: 1.730769230769231e-05
Step 646 | epoch: 2.4857831325301207
Step 647 | loss: 0.5343768000602722
Step 647 | grad_norm: 6.011941432952881
Step 647 | learning_rate: 1.717948717948718e-05
Step 647 | epoch: 2.4896385542168673
Step 648 | loss: 0.558066189289093
Step 648 | grad_norm: 6.315720558166504
Step 648 | learning_rate: 1.7051282051282053e-05
Step 648 | epoch: 2.4934939759036143
Step 649 | loss: 0.461495578289032
Step 649 | grad_norm: 5.650228500366211
Step 649 | learning_rate: 1.6923076923076924e-05
Step 649 | epoch: 2.4973493975903613
Step 650 | loss: 0.45494240522384644
Step 650 | grad_norm: 5.93996000289917
Step 650 | learning_rate: 1.6794871794871796e-05
Step 650 | epoch: 2.5012048192771084
Step 651 | loss: 0.4311447739601135
Step 651 | grad_norm: 5.643831729888916
Step 651 | learning_rate: 1.6666666666666667e-05
Step 651 | epoch: 2.5050602409638554
Step 652 | loss: 0.6285046339035034
Step 652 | grad_norm: 7.333366394042969
Step 652 | learning_rate: 1.653846153846154e-05
Step 652 | epoch: 2.5089156626506024
Step 653 | loss: 0.4084378182888031
Step 653 | grad_norm: 5.347655296325684
Step 653 | learning_rate: 1.641025641025641e-05
Step 653 | epoch: 2.5127710843373494
Step 654 | loss: 0.5762813687324524
Step 654 | grad_norm: 6.062308311462402
Step 654 | learning_rate: 1.6282051282051282e-05
Step 654 | epoch: 2.5166265060240964
Step 655 | loss: 0.46847304701805115
Step 655 | grad_norm: 5.38265323638916
Step 655 | learning_rate: 1.6153846153846154e-05
Step 655 | epoch: 2.5204819277108435
Step 656 | loss: 0.4277084767818451
Step 656 | grad_norm: 6.263278007507324
Step 656 | learning_rate: 1.602564102564103e-05
Step 656 | epoch: 2.5243373493975905
Step 657 | loss: 0.7703430652618408
Step 657 | grad_norm: 5.8598246574401855
Step 657 | learning_rate: 1.5897435897435897e-05
Step 657 | epoch: 2.5281927710843375
Step 658 | loss: 0.48644590377807617
Step 658 | grad_norm: 5.275670051574707
Step 658 | learning_rate: 1.576923076923077e-05
Step 658 | epoch: 2.532048192771084
Step 659 | loss: 0.5963408350944519
Step 659 | grad_norm: 5.638346195220947
Step 659 | learning_rate: 1.564102564102564e-05
Step 659 | epoch: 2.5359036144578315
Step 660 | loss: 0.7023493051528931
Step 660 | grad_norm: 7.8836588859558105
Step 660 | learning_rate: 1.5512820512820516e-05
Step 660 | epoch: 2.539759036144578
Step 661 | loss: 0.5431659817695618
Step 661 | grad_norm: 6.45048713684082
Step 661 | learning_rate: 1.5384615384615387e-05
Step 661 | epoch: 2.5436144578313256
Step 662 | loss: 0.3637317419052124
Step 662 | grad_norm: 5.898955345153809
Step 662 | learning_rate: 1.5256410256410259e-05
Step 662 | epoch: 2.547469879518072
Step 663 | loss: 0.5659875869750977
Step 663 | grad_norm: 5.981388568878174
Step 663 | learning_rate: 1.5128205128205129e-05
Step 663 | epoch: 2.551325301204819
Step 664 | loss: 0.5401892066001892
Step 664 | grad_norm: 6.190900802612305
Step 664 | learning_rate: 1.5e-05
Step 664 | epoch: 2.555180722891566
Step 665 | loss: 0.4637035131454468
Step 665 | grad_norm: 7.103878498077393
Step 665 | learning_rate: 1.4871794871794872e-05
Step 665 | epoch: 2.559036144578313
Step 666 | loss: 0.4836712181568146
Step 666 | grad_norm: 5.1997389793396
Step 666 | learning_rate: 1.4743589743589745e-05
Step 666 | epoch: 2.56289156626506
Step 667 | loss: 0.5343396663665771
Step 667 | grad_norm: 6.8123321533203125
Step 667 | learning_rate: 1.4615384615384617e-05
Step 667 | epoch: 2.5667469879518072
Step 668 | loss: 0.5313922166824341
Step 668 | grad_norm: 6.713468074798584
Step 668 | learning_rate: 1.4487179487179489e-05
Step 668 | epoch: 2.5706024096385542
Step 669 | loss: 0.6812628507614136
Step 669 | grad_norm: 6.705225944519043
Step 669 | learning_rate: 1.4358974358974359e-05
Step 669 | epoch: 2.5744578313253013
Step 670 | loss: 0.40580275654792786
Step 670 | grad_norm: 6.531304836273193
Step 670 | learning_rate: 1.423076923076923e-05
Step 670 | epoch: 2.5783132530120483
Step 671 | loss: 0.6661607027053833
Step 671 | grad_norm: 7.602097511291504
Step 671 | learning_rate: 1.4102564102564104e-05
Step 671 | epoch: 2.5821686746987953
Step 672 | loss: 0.5045981407165527
Step 672 | grad_norm: 7.606727123260498
Step 672 | learning_rate: 1.3974358974358975e-05
Step 672 | epoch: 2.5860240963855423
Step 673 | loss: 0.4953136742115021
Step 673 | grad_norm: 5.672438144683838
Step 673 | learning_rate: 1.3846153846153847e-05
Step 673 | epoch: 2.589879518072289
Step 674 | loss: 0.5374162197113037
Step 674 | grad_norm: 7.4937968254089355
Step 674 | learning_rate: 1.371794871794872e-05
Step 674 | epoch: 2.5937349397590364
Step 675 | loss: 0.46952441334724426
Step 675 | grad_norm: 7.078612804412842
Step 675 | learning_rate: 1.358974358974359e-05
Step 675 | epoch: 2.597590361445783
Step 676 | loss: 0.4790341258049011
Step 676 | grad_norm: 5.796417713165283
Step 676 | learning_rate: 1.3461538461538462e-05
Step 676 | epoch: 2.6014457831325304
Step 677 | loss: 0.5488226413726807
Step 677 | grad_norm: 5.91493558883667
Step 677 | learning_rate: 1.3333333333333333e-05
Step 677 | epoch: 2.605301204819277
Step 678 | loss: 0.37770384550094604
Step 678 | grad_norm: 5.190472602844238
Step 678 | learning_rate: 1.3205128205128207e-05
Step 678 | epoch: 2.609156626506024
Step 679 | loss: 0.41841331124305725
Step 679 | grad_norm: 5.682589054107666
Step 679 | learning_rate: 1.3076923076923078e-05
Step 679 | epoch: 2.613012048192771
Step 680 | loss: 0.43023914098739624
Step 680 | grad_norm: 5.1155805587768555
Step 680 | learning_rate: 1.294871794871795e-05
Step 680 | epoch: 2.616867469879518
Step 681 | loss: 0.6626147627830505
Step 681 | grad_norm: 8.013834953308105
Step 681 | learning_rate: 1.282051282051282e-05
Step 681 | epoch: 2.620722891566265
Step 682 | loss: 0.6408172249794006
Step 682 | grad_norm: 7.052871227264404
Step 682 | learning_rate: 1.2692307692307691e-05
Step 682 | epoch: 2.624578313253012
Step 683 | loss: 0.46671023964881897
Step 683 | grad_norm: 6.321361064910889
Step 683 | learning_rate: 1.2564102564102565e-05
Step 683 | epoch: 2.628433734939759
Step 684 | loss: 0.45777571201324463
Step 684 | grad_norm: 5.53416633605957
Step 684 | learning_rate: 1.2435897435897436e-05
Step 684 | epoch: 2.632289156626506
Step 685 | loss: 0.6748605966567993
Step 685 | grad_norm: 6.057403564453125
Step 685 | learning_rate: 1.230769230769231e-05
Step 685 | epoch: 2.636144578313253
Step 686 | loss: 0.4414689540863037
Step 686 | grad_norm: 5.775137424468994
Step 686 | learning_rate: 1.217948717948718e-05
Step 686 | epoch: 2.64
Step 687 | loss: 0.620494544506073
Step 687 | grad_norm: 6.820052623748779
Step 687 | learning_rate: 1.2051282051282051e-05
Step 687 | epoch: 2.643855421686747
Step 688 | loss: 0.4246061146259308
Step 688 | grad_norm: 5.638983726501465
Step 688 | learning_rate: 1.1923076923076925e-05
Step 688 | epoch: 2.6477108433734937
Step 689 | loss: 0.6345842480659485
Step 689 | grad_norm: 6.890333652496338
Step 689 | learning_rate: 1.1794871794871795e-05
Step 689 | epoch: 2.651566265060241
Step 690 | loss: 0.5399806499481201
Step 690 | grad_norm: 5.700545787811279
Step 690 | learning_rate: 1.1666666666666668e-05
Step 690 | epoch: 2.6554216867469878
Step 691 | loss: 0.40811869502067566
Step 691 | grad_norm: 5.047532081604004
Step 691 | learning_rate: 1.153846153846154e-05
Step 691 | epoch: 2.659277108433735
Step 692 | loss: 0.4773966372013092
Step 692 | grad_norm: 5.786593914031982
Step 692 | learning_rate: 1.1410256410256411e-05
Step 692 | epoch: 2.663132530120482
Step 693 | loss: 0.536447286605835
Step 693 | grad_norm: 6.769904136657715
Step 693 | learning_rate: 1.1282051282051283e-05
Step 693 | epoch: 2.666987951807229
Step 694 | loss: 0.614743709564209
Step 694 | grad_norm: 7.077455520629883
Step 694 | learning_rate: 1.1153846153846154e-05
Step 694 | epoch: 2.670843373493976
Step 695 | loss: 0.48852860927581787
Step 695 | grad_norm: 4.903765678405762
Step 695 | learning_rate: 1.1025641025641026e-05
Step 695 | epoch: 2.674698795180723
Step 696 | loss: 0.5810662508010864
Step 696 | grad_norm: 5.443995952606201
Step 696 | learning_rate: 1.0897435897435898e-05
Step 696 | epoch: 2.67855421686747
Step 697 | loss: 0.5464062094688416
Step 697 | grad_norm: 5.7555036544799805
Step 697 | learning_rate: 1.0769230769230771e-05
Step 697 | epoch: 2.682409638554217
Step 698 | loss: 0.4023210108280182
Step 698 | grad_norm: 4.522510528564453
Step 698 | learning_rate: 1.0641025641025641e-05
Step 698 | epoch: 2.686265060240964
Step 699 | loss: 0.6679911613464355
Step 699 | grad_norm: 6.3900532722473145
Step 699 | learning_rate: 1.0512820512820514e-05
Step 699 | epoch: 2.690120481927711
Step 700 | loss: 0.4519430994987488
Step 700 | grad_norm: 6.365255832672119
Step 700 | learning_rate: 1.0384615384615386e-05
Step 700 | epoch: 2.693975903614458
Step 701 | loss: 0.510094404220581
Step 701 | grad_norm: 5.799079895019531
Step 701 | learning_rate: 1.0256410256410256e-05
Step 701 | epoch: 2.697831325301205
Step 702 | loss: 0.5665627717971802
Step 702 | grad_norm: 5.9039106369018555
Step 702 | learning_rate: 1.012820512820513e-05
Step 702 | epoch: 2.701686746987952
Step 703 | loss: 0.6892873644828796
Step 703 | grad_norm: 6.857451438903809
Step 703 | learning_rate: 1e-05
Step 703 | epoch: 2.7055421686746985
Step 704 | loss: 0.5579389929771423
Step 704 | grad_norm: 7.337246894836426
Step 704 | learning_rate: 9.871794871794872e-06
Step 704 | epoch: 2.709397590361446
Step 705 | loss: 0.39563408493995667
Step 705 | grad_norm: 5.073374271392822
Step 705 | learning_rate: 9.743589743589744e-06
Step 705 | epoch: 2.7132530120481926
Step 706 | loss: 0.667945921421051
Step 706 | grad_norm: 7.023962020874023
Step 706 | learning_rate: 9.615384615384616e-06
Step 706 | epoch: 2.7171084337349396
Step 707 | loss: 0.5448529720306396
Step 707 | grad_norm: 5.359299659729004
Step 707 | learning_rate: 9.487179487179487e-06
Step 707 | epoch: 2.7209638554216866
Step 708 | loss: 0.560019314289093
Step 708 | grad_norm: 8.575542449951172
Step 708 | learning_rate: 9.358974358974359e-06
Step 708 | epoch: 2.7248192771084336
Step 709 | loss: 0.6479067206382751
Step 709 | grad_norm: 6.404506683349609
Step 709 | learning_rate: 9.230769230769232e-06
Step 709 | epoch: 2.7286746987951807
Step 710 | loss: 0.4144934117794037
Step 710 | grad_norm: 5.298825740814209
Step 710 | learning_rate: 9.102564102564102e-06
Step 710 | epoch: 2.7325301204819277
Step 711 | loss: 0.43547523021698
Step 711 | grad_norm: 4.811910152435303
Step 711 | learning_rate: 8.974358974358976e-06
Step 711 | epoch: 2.7363855421686747
Step 712 | loss: 0.6905001997947693
Step 712 | grad_norm: 5.845088481903076
Step 712 | learning_rate: 8.846153846153847e-06
Step 712 | epoch: 2.7402409638554217
Step 713 | loss: 0.5782302618026733
Step 713 | grad_norm: 6.423101425170898
Step 713 | learning_rate: 8.717948717948717e-06
Step 713 | epoch: 2.7440963855421687
Step 714 | loss: 0.8352858424186707
Step 714 | grad_norm: 5.596395492553711
Step 714 | learning_rate: 8.58974358974359e-06
Step 714 | epoch: 2.7479518072289157
Step 715 | loss: 0.5532664656639099
Step 715 | grad_norm: 5.515008926391602
Step 715 | learning_rate: 8.461538461538462e-06
Step 715 | epoch: 2.7518072289156628
Step 716 | loss: 0.48110517859458923
Step 716 | grad_norm: 6.140764236450195
Step 716 | learning_rate: 8.333333333333334e-06
Step 716 | epoch: 2.75566265060241
Step 717 | loss: 0.41149091720581055
Step 717 | grad_norm: 5.248068332672119
Step 717 | learning_rate: 8.205128205128205e-06
Step 717 | epoch: 2.759518072289157
Step 718 | loss: 0.6392447352409363
Step 718 | grad_norm: 7.434009552001953
Step 718 | learning_rate: 8.076923076923077e-06
Step 718 | epoch: 2.7633734939759034
Step 719 | loss: 0.5232388973236084
Step 719 | grad_norm: 6.733310222625732
Step 719 | learning_rate: 7.948717948717949e-06
Step 719 | epoch: 2.767228915662651
Step 720 | loss: 0.5681458711624146
Step 720 | grad_norm: 5.9805097579956055
Step 720 | learning_rate: 7.82051282051282e-06
Step 720 | epoch: 2.7710843373493974
Step 721 | loss: 0.5093182921409607
Step 721 | grad_norm: 6.642275810241699
Step 721 | learning_rate: 7.692307692307694e-06
Step 721 | epoch: 2.7749397590361444
Step 722 | loss: 0.6062793731689453
Step 722 | grad_norm: 6.043087005615234
Step 722 | learning_rate: 7.564102564102564e-06
Step 722 | epoch: 2.7787951807228914
Step 723 | loss: 0.46415406465530396
Step 723 | grad_norm: 5.88455867767334
Step 723 | learning_rate: 7.435897435897436e-06
Step 723 | epoch: 2.7826506024096385
Step 724 | loss: 0.47117194533348083
Step 724 | grad_norm: 6.0621657371521
Step 724 | learning_rate: 7.3076923076923085e-06
Step 724 | epoch: 2.7865060240963855
Step 725 | loss: 0.5999831557273865
Step 725 | grad_norm: 6.551662445068359
Step 725 | learning_rate: 7.179487179487179e-06
Step 725 | epoch: 2.7903614457831325
Step 726 | loss: 0.6649187803268433
Step 726 | grad_norm: 6.664231300354004
Step 726 | learning_rate: 7.051282051282052e-06
Step 726 | epoch: 2.7942168674698795
Step 727 | loss: 0.5622175335884094
Step 727 | grad_norm: 6.369731903076172
Step 727 | learning_rate: 6.923076923076923e-06
Step 727 | epoch: 2.7980722891566265
Step 728 | loss: 0.3207991123199463
Step 728 | grad_norm: 4.700287818908691
Step 728 | learning_rate: 6.794871794871795e-06
Step 728 | epoch: 2.8019277108433736
Step 729 | loss: 0.365597128868103
Step 729 | grad_norm: 5.6856689453125
Step 729 | learning_rate: 6.666666666666667e-06
Step 729 | epoch: 2.8057831325301206
Step 730 | loss: 0.47581231594085693
Step 730 | grad_norm: 5.77103853225708
Step 730 | learning_rate: 6.538461538461539e-06
Step 730 | epoch: 2.8096385542168676
Step 731 | loss: 0.4553976058959961
Step 731 | grad_norm: 6.163636207580566
Step 731 | learning_rate: 6.41025641025641e-06
Step 731 | epoch: 2.8134939759036146
Step 732 | loss: 0.4632112681865692
Step 732 | grad_norm: 6.540274143218994
Step 732 | learning_rate: 6.282051282051282e-06
Step 732 | epoch: 2.8173493975903616
Step 733 | loss: 0.39230069518089294
Step 733 | grad_norm: 4.603175163269043
Step 733 | learning_rate: 6.153846153846155e-06
Step 733 | epoch: 2.821204819277108
Step 734 | loss: 0.44898101687431335
Step 734 | grad_norm: 5.687411308288574
Step 734 | learning_rate: 6.025641025641026e-06
Step 734 | epoch: 2.8250602409638557
Step 735 | loss: 0.5505102872848511
Step 735 | grad_norm: 6.83689022064209
Step 735 | learning_rate: 5.897435897435897e-06
Step 735 | epoch: 2.8289156626506022
Step 736 | loss: 0.8150660991668701
Step 736 | grad_norm: 8.225578308105469
Step 736 | learning_rate: 5.76923076923077e-06
Step 736 | epoch: 2.8327710843373493
Step 737 | loss: 0.6250539422035217
Step 737 | grad_norm: 6.953012466430664
Step 737 | learning_rate: 5.641025641025641e-06
Step 737 | epoch: 2.8366265060240963
Step 738 | loss: 0.49813202023506165
Step 738 | grad_norm: 5.079864501953125
Step 738 | learning_rate: 5.512820512820513e-06
Step 738 | epoch: 2.8404819277108433
Step 739 | loss: 0.466094434261322
Step 739 | grad_norm: 6.722691059112549
Step 739 | learning_rate: 5.3846153846153855e-06
Step 739 | epoch: 2.8443373493975903
Step 740 | loss: 0.4961930513381958
Step 740 | grad_norm: 6.064580917358398
Step 740 | learning_rate: 5.256410256410257e-06
Step 740 | epoch: 2.8481927710843373
Step 741 | loss: 0.6634376049041748
Step 741 | grad_norm: 7.651566505432129
Step 741 | learning_rate: 5.128205128205128e-06
Step 741 | epoch: 2.8520481927710843
Step 742 | loss: 0.5966659188270569
Step 742 | grad_norm: 6.469323635101318
Step 742 | learning_rate: 5e-06
Step 742 | epoch: 2.8559036144578314
Step 743 | loss: 0.582402765750885
Step 743 | grad_norm: 5.85287618637085
Step 743 | learning_rate: 4.871794871794872e-06
Step 743 | epoch: 2.8597590361445784
Step 744 | loss: 0.5946980714797974
Step 744 | grad_norm: 5.70905065536499
Step 744 | learning_rate: 4.743589743589744e-06
Step 744 | epoch: 2.8636144578313254
Step 745 | loss: 0.32120394706726074
Step 745 | grad_norm: 5.64695405960083
Step 745 | learning_rate: 4.615384615384616e-06
Step 745 | epoch: 2.8674698795180724
Step 746 | loss: 0.469662070274353
Step 746 | grad_norm: 6.298923969268799
Step 746 | learning_rate: 4.487179487179488e-06
Step 746 | epoch: 2.8713253012048194
Step 747 | loss: 0.701189398765564
Step 747 | grad_norm: 7.346505641937256
Step 747 | learning_rate: 4.3589743589743586e-06
Step 747 | epoch: 2.8751807228915665
Step 748 | loss: 0.6563445329666138
Step 748 | grad_norm: 7.110892295837402
Step 748 | learning_rate: 4.230769230769231e-06
Step 748 | epoch: 2.879036144578313
Step 749 | loss: 0.7268018126487732
Step 749 | grad_norm: 6.677953243255615
Step 749 | learning_rate: 4.102564102564103e-06
Step 749 | epoch: 2.8828915662650605
Step 750 | loss: 0.42840009927749634
Step 750 | grad_norm: 5.647487640380859
Step 750 | learning_rate: 3.974358974358974e-06
Step 750 | epoch: 2.886746987951807
Step 751 | loss: 0.49904555082321167
Step 751 | grad_norm: 6.120769500732422
Step 751 | learning_rate: 3.846153846153847e-06
Step 751 | epoch: 2.890602409638554
Step 752 | loss: 0.6589336395263672
Step 752 | grad_norm: 6.553628921508789
Step 752 | learning_rate: 3.717948717948718e-06
Step 752 | epoch: 2.894457831325301
Step 753 | loss: 0.4112154245376587
Step 753 | grad_norm: 4.937770843505859
Step 753 | learning_rate: 3.5897435897435896e-06
Step 753 | epoch: 2.898313253012048
Step 754 | loss: 0.4249092638492584
Step 754 | grad_norm: 6.747220516204834
Step 754 | learning_rate: 3.4615384615384617e-06
Step 754 | epoch: 2.902168674698795
Step 755 | loss: 0.4892130494117737
Step 755 | grad_norm: 5.393825054168701
Step 755 | learning_rate: 3.3333333333333333e-06
Step 755 | epoch: 2.906024096385542
Step 756 | loss: 0.648293673992157
Step 756 | grad_norm: 7.391607761383057
Step 756 | learning_rate: 3.205128205128205e-06
Step 756 | epoch: 2.909879518072289
Step 757 | loss: 0.43392086029052734
Step 757 | grad_norm: 5.228054046630859
Step 757 | learning_rate: 3.0769230769230774e-06
Step 757 | epoch: 2.913734939759036
Step 758 | loss: 0.6065525412559509
Step 758 | grad_norm: 6.022542476654053
Step 758 | learning_rate: 2.9487179487179486e-06
Step 758 | epoch: 2.917590361445783
Step 759 | loss: 0.5815021395683289
Step 759 | grad_norm: 6.044818878173828
Step 759 | learning_rate: 2.8205128205128207e-06
Step 759 | epoch: 2.9214457831325302
Step 760 | loss: 0.5294162034988403
Step 760 | grad_norm: 6.3498921394348145
Step 760 | learning_rate: 2.6923076923076928e-06
Step 760 | epoch: 2.9253012048192772
Step 761 | loss: 0.34293732047080994
Step 761 | grad_norm: 5.146255016326904
Step 761 | learning_rate: 2.564102564102564e-06
Step 761 | epoch: 2.929156626506024
Step 762 | loss: 0.4054836332798004
Step 762 | grad_norm: 5.081286907196045
Step 762 | learning_rate: 2.435897435897436e-06
Step 762 | epoch: 2.9330120481927713
Step 763 | loss: 0.5884734988212585
Step 763 | grad_norm: 5.70375394821167
Step 763 | learning_rate: 2.307692307692308e-06
Step 763 | epoch: 2.936867469879518
Step 764 | loss: 0.4830045998096466
Step 764 | grad_norm: 5.69012975692749
Step 764 | learning_rate: 2.1794871794871793e-06
Step 764 | epoch: 2.9407228915662653
Step 765 | loss: 0.5480276346206665
Step 765 | grad_norm: 6.588500499725342
Step 765 | learning_rate: 2.0512820512820513e-06
Step 765 | epoch: 2.944578313253012
Step 766 | loss: 0.48468366265296936
Step 766 | grad_norm: 5.991011142730713
Step 766 | learning_rate: 1.9230769230769234e-06
Step 766 | epoch: 2.948433734939759
Step 767 | loss: 0.579906165599823
Step 767 | grad_norm: 6.775475025177002
Step 767 | learning_rate: 1.7948717948717948e-06
Step 767 | epoch: 2.952289156626506
Step 768 | loss: 0.5090189576148987
Step 768 | grad_norm: 4.904364585876465
Step 768 | learning_rate: 1.6666666666666667e-06
Step 768 | epoch: 2.956144578313253
Step 769 | loss: 0.6633594036102295
Step 769 | grad_norm: 5.712657451629639
Step 769 | learning_rate: 1.5384615384615387e-06
Step 769 | epoch: 2.96
Step 770 | loss: 0.46431654691696167
Step 770 | grad_norm: 7.43498420715332
Step 770 | learning_rate: 1.4102564102564104e-06
Step 770 | epoch: 2.963855421686747
Step 771 | loss: 0.6673997640609741
Step 771 | grad_norm: 7.0124945640563965
Step 771 | learning_rate: 1.282051282051282e-06
Step 771 | epoch: 2.967710843373494
Step 772 | loss: 0.7291615009307861
Step 772 | grad_norm: 6.707787990570068
Step 772 | learning_rate: 1.153846153846154e-06
Step 772 | epoch: 2.971566265060241
Step 773 | loss: 0.45803558826446533
Step 773 | grad_norm: 6.010883808135986
Step 773 | learning_rate: 1.0256410256410257e-06
Step 773 | epoch: 2.975421686746988
Step 774 | loss: 0.5997650027275085
Step 774 | grad_norm: 6.276174068450928
Step 774 | learning_rate: 8.974358974358974e-07
Step 774 | epoch: 2.979277108433735
Step 775 | loss: 0.4403557777404785
Step 775 | grad_norm: 5.979161262512207
Step 775 | learning_rate: 7.692307692307694e-07
Step 775 | epoch: 2.983132530120482
Step 776 | loss: 0.5126230716705322
Step 776 | grad_norm: 5.442997932434082
Step 776 | learning_rate: 6.41025641025641e-07
Step 776 | epoch: 2.9869879518072286
Step 777 | loss: 0.5278588533401489
Step 777 | grad_norm: 6.332852363586426
Step 777 | learning_rate: 5.128205128205128e-07
Step 777 | epoch: 2.990843373493976
Step 778 | loss: 0.4864215552806854
Step 778 | grad_norm: 5.703034400939941
Step 778 | learning_rate: 3.846153846153847e-07
Step 778 | epoch: 2.9946987951807227
Step 779 | loss: 0.43793153762817383
Step 779 | grad_norm: 5.43112850189209
Step 779 | learning_rate: 2.564102564102564e-07
Step 779 | epoch: 2.99855421686747
Step 780 | loss: 0.5996729731559753
Step 780 | grad_norm: 8.374414443969727
Step 780 | learning_rate: 1.282051282051282e-07
Step 780 | epoch: 3.0
Step 780 | train_runtime: 6320.2372
Step 780 | train_samples_per_second: 0.985
Step 780 | train_steps_per_second: 0.123
Step 780 | total_flos: 1.097724028317696e+16
Step 780 | train_loss: 0.7904378342704895
Step 780 | epoch: 3.0
