Experiment: Mistral Experiment
Timestamp: 20260126_151514
============================================================


============================================================
MODEL
============================================================

Model name: mistralai/Mistral-7B-v0.1
Tokenizer vocab size: 32000
Pad token: </s>
EOS token: </s>

============================================================
DATASET STATS
============================================================

Train size: 180
Test size: 20

============================================================
LORA CONFIGURATION
============================================================

LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.18.1', base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'q_proj', 'v_proj', 'k_proj', 'o_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, alora_invocation_tokens=None, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None, ensure_weight_tying=False)
Trainable parameters: 6,815,744
Total parameters: 7,248,547,840
Trainable %: 0.0940%

============================================================
TRAINING ARGUMENTS
============================================================

output_dir: ./mistral_qa
overwrite_output_dir: False
do_train: False
do_eval: False
do_predict: False
eval_strategy: no
prediction_loss_only: False
per_device_train_batch_size: 1
per_device_eval_batch_size: 8
per_gpu_train_batch_size: None
per_gpu_eval_batch_size: None
gradient_accumulation_steps: 8
eval_accumulation_steps: None
eval_delay: 0
torch_empty_cache_steps: None
learning_rate: 0.0001
weight_decay: 0.0
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-08
max_grad_norm: 1.0
num_train_epochs: 3
max_steps: -1
lr_scheduler_type: linear
lr_scheduler_kwargs: None
warmup_ratio: 0.0
warmup_steps: 0
log_level: passive
log_level_replica: warning
log_on_each_node: True
logging_dir: ./mistral_qa/runs/Jan26_15-15-14_b7fe885c61bb
logging_strategy: steps
logging_first_step: False
logging_steps: 1
logging_nan_inf_filter: True
save_strategy: no
save_steps: 100
save_total_limit: None
save_safetensors: True
save_on_each_node: False
save_only_model: False
restore_callback_states_from_checkpoint: False
no_cuda: False
use_cpu: False
use_mps_device: False
seed: 42
data_seed: None
jit_mode_eval: False
bf16: True
fp16: False
fp16_opt_level: O1
half_precision_backend: auto
bf16_full_eval: False
fp16_full_eval: False
tf32: None
local_rank: 0
ddp_backend: None
tpu_num_cores: None
tpu_metrics_debug: False
debug: []
dataloader_drop_last: False
eval_steps: None
dataloader_num_workers: 0
dataloader_prefetch_factor: None
past_index: -1
run_name: None
disable_tqdm: False
remove_unused_columns: True
label_names: None
load_best_model_at_end: False
metric_for_best_model: None
greater_is_better: None
ignore_data_skip: False
fsdp: []
fsdp_min_num_params: 0
fsdp_config: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
fsdp_transformer_layer_cls_to_wrap: None
accelerator_config: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
parallelism_config: None
deepspeed: None
label_smoothing_factor: 0.0
optim: adamw_torch_fused
optim_args: None
adafactor: False
group_by_length: False
length_column_name: length
report_to: []
project: huggingface
trackio_space_id: trackio
ddp_find_unused_parameters: None
ddp_bucket_cap_mb: None
ddp_broadcast_buffers: None
dataloader_pin_memory: True
dataloader_persistent_workers: False
skip_memory_metrics: True
use_legacy_prediction_loop: False
push_to_hub: False
resume_from_checkpoint: None
hub_model_id: None
hub_strategy: every_save
hub_token: <HUB_TOKEN>
hub_private_repo: None
hub_always_push: False
hub_revision: None
gradient_checkpointing: False
gradient_checkpointing_kwargs: None
include_inputs_for_metrics: False
include_for_metrics: []
eval_do_concat_batches: True
fp16_backend: auto
push_to_hub_model_id: None
push_to_hub_organization: None
push_to_hub_token: <PUSH_TO_HUB_TOKEN>
mp_parameters: 
auto_find_batch_size: False
full_determinism: False
torchdynamo: None
ray_scope: last
ddp_timeout: 1800
torch_compile: False
torch_compile_backend: None
torch_compile_mode: None
include_tokens_per_second: False
include_num_input_tokens_seen: no
neftune_noise_alpha: None
optim_target_modules: None
batch_eval_metrics: False
eval_on_start: False
use_liger_kernel: False
liger_kernel_config: None
eval_use_gather_object: False
average_tokens_across_devices: True
Step 1 | loss: 1.6238
Step 1 | grad_norm: 2.7011401653289795
Step 1 | learning_rate: 0.0001
Step 1 | epoch: 0.044444444444444446
Step 2 | loss: 1.8618
Step 2 | grad_norm: 2.9464495182037354
Step 2 | learning_rate: 9.855072463768117e-05
Step 2 | epoch: 0.08888888888888889
Step 3 | loss: 1.5379
Step 3 | grad_norm: 2.964646339416504
Step 3 | learning_rate: 9.710144927536232e-05
Step 3 | epoch: 0.13333333333333333
Step 4 | loss: 1.5024
Step 4 | grad_norm: 3.4520039558410645
Step 4 | learning_rate: 9.565217391304348e-05
Step 4 | epoch: 0.17777777777777778
Step 5 | loss: 1.1376
Step 5 | grad_norm: 2.806730270385742
Step 5 | learning_rate: 9.420289855072463e-05
Step 5 | epoch: 0.2222222222222222
Step 6 | loss: 1.6413
Step 6 | grad_norm: 3.123512029647827
Step 6 | learning_rate: 9.27536231884058e-05
Step 6 | epoch: 0.26666666666666666
Step 7 | loss: 1.3825
Step 7 | grad_norm: 3.1712687015533447
Step 7 | learning_rate: 9.130434782608696e-05
Step 7 | epoch: 0.3111111111111111
Step 8 | loss: 1.4894
Step 8 | grad_norm: 3.835301160812378
Step 8 | learning_rate: 8.985507246376813e-05
Step 8 | epoch: 0.35555555555555557
Step 9 | loss: 1.1364
Step 9 | grad_norm: 3.3369948863983154
Step 9 | learning_rate: 8.840579710144929e-05
Step 9 | epoch: 0.4
Step 10 | loss: 1.211
Step 10 | grad_norm: 3.8955421447753906
Step 10 | learning_rate: 8.695652173913044e-05
Step 10 | epoch: 0.4444444444444444
Step 11 | loss: 1.1325
Step 11 | grad_norm: 3.6702005863189697
Step 11 | learning_rate: 8.55072463768116e-05
Step 11 | epoch: 0.4888888888888889
Step 12 | loss: 1.2542
Step 12 | grad_norm: 3.974503755569458
Step 12 | learning_rate: 8.405797101449276e-05
Step 12 | epoch: 0.5333333333333333
Step 13 | loss: 1.4789
Step 13 | grad_norm: 3.6151788234710693
Step 13 | learning_rate: 8.260869565217392e-05
Step 13 | epoch: 0.5777777777777777
Step 14 | loss: 1.2797
Step 14 | grad_norm: 3.9518380165100098
Step 14 | learning_rate: 8.115942028985508e-05
Step 14 | epoch: 0.6222222222222222
Step 15 | loss: 1.1716
Step 15 | grad_norm: 3.247360944747925
Step 15 | learning_rate: 7.971014492753623e-05
Step 15 | epoch: 0.6666666666666666
Step 16 | loss: 1.3767
Step 16 | grad_norm: 3.2937705516815186
Step 16 | learning_rate: 7.82608695652174e-05
Step 16 | epoch: 0.7111111111111111
Step 17 | loss: 1.3019
Step 17 | grad_norm: 3.3652596473693848
Step 17 | learning_rate: 7.681159420289855e-05
Step 17 | epoch: 0.7555555555555555
Step 18 | loss: 1.0327
Step 18 | grad_norm: 3.6558871269226074
Step 18 | learning_rate: 7.536231884057971e-05
Step 18 | epoch: 0.8
Step 19 | loss: 1.1542
Step 19 | grad_norm: 3.3272087574005127
Step 19 | learning_rate: 7.391304347826086e-05
Step 19 | epoch: 0.8444444444444444
Step 20 | loss: 1.2064
Step 20 | grad_norm: 3.1466569900512695
Step 20 | learning_rate: 7.246376811594203e-05
Step 20 | epoch: 0.8888888888888888
Step 21 | loss: 1.1873
Step 21 | grad_norm: 3.1043434143066406
Step 21 | learning_rate: 7.101449275362319e-05
Step 21 | epoch: 0.9333333333333333
Step 22 | loss: 1.2549
Step 22 | grad_norm: 2.9874284267425537
Step 22 | learning_rate: 6.956521739130436e-05
Step 22 | epoch: 0.9777777777777777
Step 23 | loss: 1.4484
Step 23 | grad_norm: 4.70849609375
Step 23 | learning_rate: 6.811594202898552e-05
Step 23 | epoch: 1.0
Step 24 | loss: 0.9148
Step 24 | grad_norm: 2.965768575668335
Step 24 | learning_rate: 6.666666666666667e-05
Step 24 | epoch: 1.0444444444444445
Step 25 | loss: 1.3434
Step 25 | grad_norm: 3.2767531871795654
Step 25 | learning_rate: 6.521739130434783e-05
Step 25 | epoch: 1.0888888888888888
Step 26 | loss: 0.765
Step 26 | grad_norm: 2.8664660453796387
Step 26 | learning_rate: 6.376811594202898e-05
Step 26 | epoch: 1.1333333333333333
Step 27 | loss: 1.1509
Step 27 | grad_norm: 3.110140800476074
Step 27 | learning_rate: 6.231884057971015e-05
Step 27 | epoch: 1.1777777777777778
Step 28 | loss: 0.9511
Step 28 | grad_norm: 3.517279624938965
Step 28 | learning_rate: 6.086956521739131e-05
Step 28 | epoch: 1.2222222222222223
Step 29 | loss: 0.9354
Step 29 | grad_norm: 3.3051295280456543
Step 29 | learning_rate: 5.942028985507246e-05
Step 29 | epoch: 1.2666666666666666
Step 30 | loss: 0.801
Step 30 | grad_norm: 3.478814125061035
Step 30 | learning_rate: 5.797101449275363e-05
Step 30 | epoch: 1.3111111111111111
Step 31 | loss: 0.9107
Step 31 | grad_norm: 3.6457738876342773
Step 31 | learning_rate: 5.652173913043478e-05
Step 31 | epoch: 1.3555555555555556
Step 32 | loss: 0.9523
Step 32 | grad_norm: 3.331852674484253
Step 32 | learning_rate: 5.507246376811594e-05
Step 32 | epoch: 1.4
Step 33 | loss: 1.1531
Step 33 | grad_norm: 3.3146047592163086
Step 33 | learning_rate: 5.3623188405797106e-05
Step 33 | epoch: 1.4444444444444444
Step 34 | loss: 0.9779
Step 34 | grad_norm: 3.561206817626953
Step 34 | learning_rate: 5.217391304347826e-05
Step 34 | epoch: 1.488888888888889
Step 35 | loss: 0.9787
Step 35 | grad_norm: 3.824192523956299
Step 35 | learning_rate: 5.072463768115943e-05
Step 35 | epoch: 1.5333333333333332
Step 36 | loss: 0.7771
Step 36 | grad_norm: 2.978856325149536
Step 36 | learning_rate: 4.9275362318840584e-05
Step 36 | epoch: 1.5777777777777777
Step 37 | loss: 0.7835
Step 37 | grad_norm: 3.311734437942505
Step 37 | learning_rate: 4.782608695652174e-05
Step 37 | epoch: 1.6222222222222222
Step 38 | loss: 0.9515
Step 38 | grad_norm: 4.389540672302246
Step 38 | learning_rate: 4.63768115942029e-05
Step 38 | epoch: 1.6666666666666665
Step 39 | loss: 1.0861
Step 39 | grad_norm: 4.089732646942139
Step 39 | learning_rate: 4.492753623188406e-05
Step 39 | epoch: 1.7111111111111112
Step 40 | loss: 0.6565
Step 40 | grad_norm: 3.1563315391540527
Step 40 | learning_rate: 4.347826086956522e-05
Step 40 | epoch: 1.7555555555555555
Step 41 | loss: 0.8642
Step 41 | grad_norm: 4.0477728843688965
Step 41 | learning_rate: 4.202898550724638e-05
Step 41 | epoch: 1.8
Step 42 | loss: 0.939
Step 42 | grad_norm: 4.733451843261719
Step 42 | learning_rate: 4.057971014492754e-05
Step 42 | epoch: 1.8444444444444446
Step 43 | loss: 0.9477
Step 43 | grad_norm: 4.116669178009033
Step 43 | learning_rate: 3.91304347826087e-05
Step 43 | epoch: 1.8888888888888888
Step 44 | loss: 0.9591
Step 44 | grad_norm: 4.489012241363525
Step 44 | learning_rate: 3.7681159420289856e-05
Step 44 | epoch: 1.9333333333333333
Step 45 | loss: 1.0216
Step 45 | grad_norm: 4.251640796661377
Step 45 | learning_rate: 3.6231884057971014e-05
Step 45 | epoch: 1.9777777777777779
Step 46 | loss: 0.9173
Step 46 | grad_norm: 5.176657676696777
Step 46 | learning_rate: 3.478260869565218e-05
Step 46 | epoch: 2.0
Step 47 | loss: 0.7565
Step 47 | grad_norm: 4.046121120452881
Step 47 | learning_rate: 3.3333333333333335e-05
Step 47 | epoch: 2.0444444444444443
Step 48 | loss: 0.8499
Step 48 | grad_norm: 4.6504082679748535
Step 48 | learning_rate: 3.188405797101449e-05
Step 48 | epoch: 2.088888888888889
Step 49 | loss: 0.7214
Step 49 | grad_norm: 3.8114852905273438
Step 49 | learning_rate: 3.0434782608695656e-05
Step 49 | epoch: 2.1333333333333333
Step 50 | loss: 0.8093
Step 50 | grad_norm: 3.814110279083252
Step 50 | learning_rate: 2.8985507246376814e-05
Step 50 | epoch: 2.1777777777777776
Step 51 | loss: 0.7541
Step 51 | grad_norm: 4.082866668701172
Step 51 | learning_rate: 2.753623188405797e-05
Step 51 | epoch: 2.2222222222222223
Step 52 | loss: 0.6309
Step 52 | grad_norm: 4.427696228027344
Step 52 | learning_rate: 2.608695652173913e-05
Step 52 | epoch: 2.2666666666666666
Step 53 | loss: 0.6359
Step 53 | grad_norm: 3.674933433532715
Step 53 | learning_rate: 2.4637681159420292e-05
Step 53 | epoch: 2.311111111111111
Step 54 | loss: 0.5565
Step 54 | grad_norm: 3.1537539958953857
Step 54 | learning_rate: 2.318840579710145e-05
Step 54 | epoch: 2.3555555555555556
Step 55 | loss: 0.7241
Step 55 | grad_norm: 4.013396263122559
Step 55 | learning_rate: 2.173913043478261e-05
Step 55 | epoch: 2.4
Step 56 | loss: 0.7591
Step 56 | grad_norm: 4.608163833618164
Step 56 | learning_rate: 2.028985507246377e-05
Step 56 | epoch: 2.4444444444444446
Step 57 | loss: 0.6323
Step 57 | grad_norm: 3.3835721015930176
Step 57 | learning_rate: 1.8840579710144928e-05
Step 57 | epoch: 2.488888888888889
Step 58 | loss: 0.8624
Step 58 | grad_norm: 4.668309688568115
Step 58 | learning_rate: 1.739130434782609e-05
Step 58 | epoch: 2.533333333333333
Step 59 | loss: 0.8235
Step 59 | grad_norm: 5.072546482086182
Step 59 | learning_rate: 1.5942028985507246e-05
Step 59 | epoch: 2.5777777777777775
Step 60 | loss: 0.7527
Step 60 | grad_norm: 4.07208251953125
Step 60 | learning_rate: 1.4492753623188407e-05
Step 60 | epoch: 2.6222222222222222
Step 61 | loss: 0.8252
Step 61 | grad_norm: 5.45235538482666
Step 61 | learning_rate: 1.3043478260869566e-05
Step 61 | epoch: 2.6666666666666665
Step 62 | loss: 0.6533
Step 62 | grad_norm: 4.176161766052246
Step 62 | learning_rate: 1.1594202898550725e-05
Step 62 | epoch: 2.7111111111111112
Step 63 | loss: 0.6395
Step 63 | grad_norm: 4.36715841293335
Step 63 | learning_rate: 1.0144927536231885e-05
Step 63 | epoch: 2.7555555555555555
Step 64 | loss: 0.7328
Step 64 | grad_norm: 5.380721092224121
Step 64 | learning_rate: 8.695652173913044e-06
Step 64 | epoch: 2.8
Step 65 | loss: 0.8669
Step 65 | grad_norm: 6.05657434463501
Step 65 | learning_rate: 7.246376811594203e-06
Step 65 | epoch: 2.8444444444444446
Step 66 | loss: 0.8174
Step 66 | grad_norm: 4.540040016174316
Step 66 | learning_rate: 5.797101449275362e-06
Step 66 | epoch: 2.888888888888889
Step 67 | loss: 0.7987
Step 67 | grad_norm: 4.643572807312012
Step 67 | learning_rate: 4.347826086956522e-06
Step 67 | epoch: 2.9333333333333336
Step 68 | loss: 0.4172
Step 68 | grad_norm: 3.381854295730591
Step 68 | learning_rate: 2.898550724637681e-06
Step 68 | epoch: 2.977777777777778
Step 69 | loss: 1.03
Step 69 | grad_norm: 10.06923770904541
Step 69 | learning_rate: 1.4492753623188406e-06
Step 69 | epoch: 3.0
Step 69 | train_runtime: 498.7684
Step 69 | train_samples_per_second: 1.083
Step 69 | train_steps_per_second: 0.138
Step 69 | total_flos: 976745444474880.0
Step 69 | train_loss: 1.0085614867832349
Step 69 | epoch: 3.0

============================================================
SAMPLE GENERATIONS
============================================================

Q: What is the difference between compilation and interpretation?
OUTPUT:
Q: What is the difference between compilation and interpretation?
A: Compilation translates source code into machine code, producing an executable file. Interpretation executes code line by line, translating at runtime.
----------------------------------------
Q: Explain the concept of polymorphism.
OUTPUT:
Q: Explain the concept of polymorphism.
A: Polymorphism allows objects of different classes to be treated as objects of a common base class, enabling method overriding and dynamic behavior.
----------------------------------------
