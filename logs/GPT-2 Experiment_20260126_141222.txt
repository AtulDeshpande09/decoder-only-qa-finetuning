Experiment: GPT-2 Experiment
Timestamp: 20260126_141222

============================================================
MODEL
============================================================

Model name: gpt2
Tokenizer vocab size: 50257
Pad token: <|endoftext|>
EOS token: <|endoftext|>

============================================================
DATASET STATS
============================================================

Train size: 180
Test size: 20

============================================================
LORA CONFIGURATION
============================================================

LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.18.1', base_model_name_or_path='gpt2', revision=None, inference_mode=False, r=8, target_modules={'c_attn'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=True, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, alora_invocation_tokens=None, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None, ensure_weight_tying=False)
Trainable parameters: 294,912
Total parameters: 124,734,720
Trainable %: 0.2364%

============================================================
TRAINING ARGUMENTS
============================================================

output_dir: ./gpt-2-qa
overwrite_output_dir: False
do_train: False
do_eval: False
do_predict: False
eval_strategy: no
prediction_loss_only: False
per_device_train_batch_size: 2
per_device_eval_batch_size: 8
per_gpu_train_batch_size: None
per_gpu_eval_batch_size: None
gradient_accumulation_steps: 1
eval_accumulation_steps: None
eval_delay: 0
torch_empty_cache_steps: None
learning_rate: 0.0001
weight_decay: 0.0
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-08
max_grad_norm: 1.0
num_train_epochs: 3
max_steps: -1
lr_scheduler_type: linear
lr_scheduler_kwargs: None
warmup_ratio: 0.0
warmup_steps: 0
log_level: passive
log_level_replica: warning
log_on_each_node: True
logging_dir: ./gpt-2-qa/runs/Jan26_14-12-22_b46367e8115f
logging_strategy: steps
logging_first_step: False
logging_steps: 1
logging_nan_inf_filter: True
save_strategy: steps
save_steps: 100
save_total_limit: None
save_safetensors: True
save_on_each_node: False
save_only_model: False
restore_callback_states_from_checkpoint: False
no_cuda: False
use_cpu: False
use_mps_device: False
seed: 42
data_seed: None
jit_mode_eval: False
bf16: False
fp16: False
fp16_opt_level: O1
half_precision_backend: auto
bf16_full_eval: False
fp16_full_eval: False
tf32: None
local_rank: 0
ddp_backend: None
tpu_num_cores: None
tpu_metrics_debug: False
debug: []
dataloader_drop_last: False
eval_steps: None
dataloader_num_workers: 0
dataloader_prefetch_factor: None
past_index: -1
run_name: None
disable_tqdm: False
remove_unused_columns: True
label_names: None
load_best_model_at_end: False
metric_for_best_model: None
greater_is_better: None
ignore_data_skip: False
fsdp: []
fsdp_min_num_params: 0
fsdp_config: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
fsdp_transformer_layer_cls_to_wrap: None
accelerator_config: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}
parallelism_config: None
deepspeed: None
label_smoothing_factor: 0.0
optim: adamw_torch_fused
optim_args: None
adafactor: False
group_by_length: False
length_column_name: length
report_to: []
project: huggingface
trackio_space_id: trackio
ddp_find_unused_parameters: None
ddp_bucket_cap_mb: None
ddp_broadcast_buffers: None
dataloader_pin_memory: True
dataloader_persistent_workers: False
skip_memory_metrics: True
use_legacy_prediction_loop: False
push_to_hub: False
resume_from_checkpoint: None
hub_model_id: None
hub_strategy: every_save
hub_token: <HUB_TOKEN>
hub_private_repo: None
hub_always_push: False
hub_revision: None
gradient_checkpointing: False
gradient_checkpointing_kwargs: None
include_inputs_for_metrics: False
include_for_metrics: []
eval_do_concat_batches: True
fp16_backend: auto
push_to_hub_model_id: None
push_to_hub_organization: None
push_to_hub_token: <PUSH_TO_HUB_TOKEN>
mp_parameters: 
auto_find_batch_size: False
full_determinism: False
torchdynamo: None
ray_scope: last
ddp_timeout: 1800
torch_compile: False
torch_compile_backend: None
torch_compile_mode: None
include_tokens_per_second: False
include_num_input_tokens_seen: no
neftune_noise_alpha: None
optim_target_modules: None
batch_eval_metrics: False
eval_on_start: False
use_liger_kernel: False
liger_kernel_config: None
eval_use_gather_object: False
average_tokens_across_devices: True

============================================================
SAMPLE GENERATIONS
============================================================

Q: What is the difference between compilation and interpretation?
OUTPUT:
Q: What is the difference between compilation and interpretation?
A: Compilation is the process of converting a file into a binary file. Interpretation is the process of converting a file into a binary file.
----------------------------------------
Q: Explain the concept of polymorphism.
OUTPUT:
Q: Explain the concept of polymorphism.
A: It is a type system that allows polymorphism to be implemented in a single function.

============================================================

Step 1 | loss: 4.2221
Step 1 | grad_norm: 0.8215928673744202
Step 1 | learning_rate: 0.0001
Step 1 | epoch: 0.011111111111111112
Step 10 | loss: 4.0839
Step 10 | grad_norm: 1.0864005088806152
Step 10 | learning_rate: 9.666666666666667e-05
Step 10 | epoch: 0.1111111111111111
Step 20 | loss: 3.7854
Step 20 | grad_norm: 0.7645605206489563
Step 20 | learning_rate: 9.296296296296296e-05
Step 20 | epoch: 0.2222222222222222
Step 30 | loss: 3.5842
Step 30 | grad_norm: 1.2239598035812378
Step 30 | learning_rate: 8.925925925925926e-05
Step 30 | epoch: 0.3333333333333333
Step 40 | loss: 3.3646
Step 40 | grad_norm: 1.1245876550674438
Step 40 | learning_rate: 8.555555555555556e-05
Step 40 | epoch: 0.4444444444444444
Step 50 | loss: 4.0408
Step 50 | grad_norm: 0.9936841130256653
Step 50 | learning_rate: 8.185185185185186e-05
Step 50 | epoch: 0.5555555555555556
Step 60 | loss: 3.6306
Step 60 | grad_norm: 1.205355167388916
Step 60 | learning_rate: 7.814814814814815e-05
Step 60 | epoch: 0.6666666666666666
Step 70 | loss: 3.4626
Step 70 | grad_norm: 1.0171531438827515
Step 70 | learning_rate: 7.444444444444444e-05
Step 70 | epoch: 0.7777777777777778
Step 80 | loss: 2.6539
Step 80 | grad_norm: 1.201641321182251
Step 80 | learning_rate: 7.074074074074074e-05
Step 80 | epoch: 0.8888888888888888
Step 90 | loss: 3.3762
Step 90 | grad_norm: 1.108102798461914
Step 90 | learning_rate: 6.703703703703704e-05
Step 90 | epoch: 1.0
Step 100 | loss: 2.9635
Step 100 | grad_norm: 1.2089074850082397
Step 100 | learning_rate: 6.333333333333333e-05
Step 100 | epoch: 1.1111111111111112
Step 110 | loss: 3.2157
Step 110 | grad_norm: 1.3215992450714111
Step 110 | learning_rate: 5.962962962962964e-05
Step 110 | epoch: 1.2222222222222223
Step 120 | loss: 3.5841
Step 120 | grad_norm: 1.5803587436676025
Step 120 | learning_rate: 5.592592592592593e-05
Step 120 | epoch: 1.3333333333333333
Step 130 | loss: 3.4301
Step 130 | grad_norm: 1.3477274179458618
Step 130 | learning_rate: 5.222222222222223e-05
Step 130 | epoch: 1.4444444444444444
Step 140 | loss: 2.9944
Step 140 | grad_norm: 1.4336947202682495
Step 140 | learning_rate: 4.851851851851852e-05
Step 140 | epoch: 1.5555555555555556
Step 150 | loss: 3.7591
Step 150 | grad_norm: 2.09378719329834
Step 150 | learning_rate: 4.481481481481482e-05
Step 150 | epoch: 1.6666666666666665
Step 160 | loss: 3.2198
Step 160 | grad_norm: 2.182729721069336
Step 160 | learning_rate: 4.111111111111111e-05
Step 160 | epoch: 1.7777777777777777
Step 170 | loss: 3.609
Step 170 | grad_norm: 2.027505874633789
Step 170 | learning_rate: 3.740740740740741e-05
Step 170 | epoch: 1.8888888888888888
Step 180 | loss: 2.9189
Step 180 | grad_norm: 1.9490066766738892
Step 180 | learning_rate: 3.3703703703703706e-05
Step 180 | epoch: 2.0
Step 190 | loss: 2.7806
Step 190 | grad_norm: 2.6293599605560303
Step 190 | learning_rate: 3e-05
Step 190 | epoch: 2.111111111111111
Step 200 | loss: 2.5618
Step 200 | grad_norm: 1.639744758605957
Step 200 | learning_rate: 2.6296296296296296e-05
Step 200 | epoch: 2.2222222222222223
Step 210 | loss: 3.8696
Step 210 | grad_norm: 1.7140603065490723
Step 210 | learning_rate: 2.2592592592592594e-05
Step 210 | epoch: 2.3333333333333335
Step 220 | loss: 2.773
Step 220 | grad_norm: 1.7696179151535034
Step 220 | learning_rate: 1.888888888888889e-05
Step 220 | epoch: 2.4444444444444446
Step 230 | loss: 3.2443
Step 230 | grad_norm: 2.025507926940918
Step 230 | learning_rate: 1.5185185185185186e-05
Step 230 | epoch: 2.5555555555555554
Step 240 | loss: 3.0858
Step 240 | grad_norm: 1.488105297088623
Step 240 | learning_rate: 1.1481481481481482e-05
Step 240 | epoch: 2.6666666666666665
Step 250 | loss: 3.587
Step 250 | grad_norm: 3.085805654525757
Step 250 | learning_rate: 7.777777777777777e-06
Step 250 | epoch: 2.7777777777777777
Step 260 | loss: 3.6641
Step 260 | grad_norm: 1.4282522201538086
Step 260 | learning_rate: 4.074074074074075e-06
Step 260 | epoch: 2.888888888888889
Step 270 | loss: 2.6648
Step 270 | grad_norm: 1.6734437942504883
Step 270 | learning_rate: 3.703703703703704e-07
Step 270 | epoch: 3.0
Step 270 | train_runtime: 30.9694
Step 270 | train_samples_per_second: 17.437
Step 270 | train_steps_per_second: 8.718
Step 270 | total_flos: 11999655419904.0
Step 270 | train_loss: 3.4769845123644227
Step 270 | epoch: 3.0

----------------------------------------
