Step 10 | loss: 2.110194969177246
Step 10 | grad_norm: 1.0026559829711914
Step 10 | learning_rate: 9.734513274336283e-05
Step 10 | epoch: 0.08888888888888889
Step 20 | loss: 1.5699456214904786
Step 20 | grad_norm: 1.7530033588409424
Step 20 | learning_rate: 9.43952802359882e-05
Step 20 | epoch: 0.17777777777777778
Step 30 | loss: 1.4024084091186524
Step 30 | grad_norm: 1.8843806982040405
Step 30 | learning_rate: 9.144542772861357e-05
Step 30 | epoch: 0.26666666666666666
Step 40 | loss: 1.4373777389526368
Step 40 | grad_norm: 1.136586308479309
Step 40 | learning_rate: 8.849557522123895e-05
Step 40 | epoch: 0.35555555555555557
Step 50 | loss: 1.3783482551574706
Step 50 | grad_norm: 1.9320117235183716
Step 50 | learning_rate: 8.554572271386431e-05
Step 50 | epoch: 0.4444444444444444
Step 60 | loss: 1.3101397514343263
Step 60 | grad_norm: 1.1030948162078857
Step 60 | learning_rate: 8.259587020648968e-05
Step 60 | epoch: 0.5333333333333333
Step 70 | loss: 1.31027250289917
Step 70 | grad_norm: 1.3294771909713745
Step 70 | learning_rate: 7.964601769911504e-05
Step 70 | epoch: 0.6222222222222222
Step 80 | loss: 1.3598440170288086
Step 80 | grad_norm: 1.3840038776397705
Step 80 | learning_rate: 7.669616519174043e-05
Step 80 | epoch: 0.7111111111111111
Step 90 | loss: 1.2235549926757812
Step 90 | grad_norm: 1.4845905303955078
Step 90 | learning_rate: 7.374631268436578e-05
Step 90 | epoch: 0.8
Step 100 | loss: 1.3654984474182128
Step 100 | grad_norm: 1.474751353263855
Step 100 | learning_rate: 7.079646017699115e-05
Step 100 | epoch: 0.8888888888888888
Step 110 | loss: 1.2119356155395509
Step 110 | grad_norm: 1.3725255727767944
Step 110 | learning_rate: 6.784660766961653e-05
Step 110 | epoch: 0.9777777777777777
Step 120 | loss: 1.17074556350708
Step 120 | grad_norm: 1.5527688264846802
Step 120 | learning_rate: 6.48967551622419e-05
Step 120 | epoch: 1.0622222222222222
Step 130 | loss: 1.1451645851135255
Step 130 | grad_norm: 1.5567824840545654
Step 130 | learning_rate: 6.194690265486725e-05
Step 130 | epoch: 1.1511111111111112
Step 140 | loss: 1.1650636672973633
Step 140 | grad_norm: 1.655608892440796
Step 140 | learning_rate: 5.899705014749263e-05
Step 140 | epoch: 1.24
Step 150 | loss: 1.1071586608886719
Step 150 | grad_norm: 1.3289963006973267
Step 150 | learning_rate: 5.6047197640118e-05
Step 150 | epoch: 1.3288888888888888
Step 160 | loss: 1.1568051338195802
Step 160 | grad_norm: 1.6803419589996338
Step 160 | learning_rate: 5.309734513274337e-05
Step 160 | epoch: 1.4177777777777778
Step 170 | loss: 1.1139984130859375
Step 170 | grad_norm: 1.528822898864746
Step 170 | learning_rate: 5.014749262536873e-05
Step 170 | epoch: 1.5066666666666668
Step 180 | loss: 1.1527241706848144
Step 180 | grad_norm: 1.8854022026062012
Step 180 | learning_rate: 4.71976401179941e-05
Step 180 | epoch: 1.5955555555555554
Step 190 | loss: 1.188328742980957
Step 190 | grad_norm: 1.6689687967300415
Step 190 | learning_rate: 4.4247787610619477e-05
Step 190 | epoch: 1.6844444444444444
Step 200 | loss: 1.171045684814453
Step 200 | grad_norm: 1.9249502420425415
Step 200 | learning_rate: 4.129793510324484e-05
Step 200 | epoch: 1.7733333333333334
Step 210 | loss: 1.1572996139526368
Step 210 | grad_norm: 1.7060378789901733
Step 210 | learning_rate: 3.834808259587021e-05
Step 210 | epoch: 1.8622222222222222
Step 220 | loss: 1.1313616752624511
Step 220 | grad_norm: 2.0715506076812744
Step 220 | learning_rate: 3.5398230088495574e-05
Step 220 | epoch: 1.951111111111111
Step 230 | loss: 1.1021099090576172
Step 230 | grad_norm: 1.9374898672103882
Step 230 | learning_rate: 3.244837758112095e-05
Step 230 | epoch: 2.0355555555555553
Step 240 | loss: 1.046864891052246
Step 240 | grad_norm: 2.4078540802001953
Step 240 | learning_rate: 2.9498525073746314e-05
Step 240 | epoch: 2.1244444444444444
Step 250 | loss: 1.0273332595825195
Step 250 | grad_norm: 2.066420078277588
Step 250 | learning_rate: 2.6548672566371686e-05
Step 250 | epoch: 2.2133333333333334
Step 260 | loss: 0.9389033317565918
Step 260 | grad_norm: 2.0657107830047607
Step 260 | learning_rate: 2.359882005899705e-05
Step 260 | epoch: 2.3022222222222224
Step 270 | loss: 1.083403968811035
Step 270 | grad_norm: 2.1406984329223633
Step 270 | learning_rate: 2.064896755162242e-05
Step 270 | epoch: 2.391111111111111
Step 280 | loss: 1.036687183380127
Step 280 | grad_norm: 2.1968696117401123
Step 280 | learning_rate: 1.7699115044247787e-05
Step 280 | epoch: 2.48
Step 290 | loss: 1.054927444458008
Step 290 | grad_norm: 1.943524718284607
Step 290 | learning_rate: 1.4749262536873157e-05
Step 290 | epoch: 2.568888888888889
Step 300 | loss: 1.0187339782714844
Step 300 | grad_norm: 2.1501352787017822
Step 300 | learning_rate: 1.1799410029498525e-05
Step 300 | epoch: 2.6577777777777776
Step 310 | loss: 1.0468032836914063
Step 310 | grad_norm: 2.037677764892578
Step 310 | learning_rate: 8.849557522123894e-06
Step 310 | epoch: 2.7466666666666666
Step 320 | loss: 0.9587090492248536
Step 320 | grad_norm: 1.9224207401275635
Step 320 | learning_rate: 5.899705014749263e-06
Step 320 | epoch: 2.8355555555555556
Step 330 | loss: 1.0843765258789062
Step 330 | grad_norm: 1.932281494140625
Step 330 | learning_rate: 2.9498525073746313e-06
Step 330 | epoch: 2.924444444444444
Step 339 | train_runtime: 571.0447
Step 339 | train_samples_per_second: 4.728
Step 339 | train_steps_per_second: 0.594
Step 339 | total_flos: 4643767688527872.0
Step 339 | train_loss: 1.1983602602573271
Step 339 | epoch: 3.0
