Step 1 | loss: 3.8825724124908447
Step 1 | grad_norm: 0.6205847859382629
Step 1 | learning_rate: 0.0001
Step 1 | epoch: 0.0022222222222222222
Step 2 | loss: 5.186185359954834
Step 2 | grad_norm: 0.6523362398147583
Step 2 | learning_rate: 9.992592592592593e-05
Step 2 | epoch: 0.0044444444444444444
Step 3 | loss: 4.174501895904541
Step 3 | grad_norm: 1.0111955404281616
Step 3 | learning_rate: 9.985185185185185e-05
Step 3 | epoch: 0.006666666666666667
Step 4 | loss: 3.873100996017456
Step 4 | grad_norm: 0.5863860249519348
Step 4 | learning_rate: 9.977777777777779e-05
Step 4 | epoch: 0.008888888888888889
Step 5 | loss: 4.201606750488281
Step 5 | grad_norm: 0.7873097658157349
Step 5 | learning_rate: 9.970370370370371e-05
Step 5 | epoch: 0.011111111111111112
Step 6 | loss: 3.7268354892730713
Step 6 | grad_norm: 0.7873550653457642
Step 6 | learning_rate: 9.962962962962963e-05
Step 6 | epoch: 0.013333333333333334
Step 7 | loss: 3.9995837211608887
Step 7 | grad_norm: 0.8807162642478943
Step 7 | learning_rate: 9.955555555555556e-05
Step 7 | epoch: 0.015555555555555555
Step 8 | loss: 4.59577751159668
Step 8 | grad_norm: 0.6210141181945801
Step 8 | learning_rate: 9.948148148148148e-05
Step 8 | epoch: 0.017777777777777778
Step 9 | loss: 3.853419780731201
Step 9 | grad_norm: 1.611197590827942
Step 9 | learning_rate: 9.940740740740742e-05
Step 9 | epoch: 0.02
Step 10 | loss: 4.270786762237549
Step 10 | grad_norm: 0.83351731300354
Step 10 | learning_rate: 9.933333333333334e-05
Step 10 | epoch: 0.022222222222222223
Step 11 | loss: 4.0815839767456055
Step 11 | grad_norm: 0.8098944425582886
Step 11 | learning_rate: 9.925925925925926e-05
Step 11 | epoch: 0.024444444444444446
Step 12 | loss: 4.0156073570251465
Step 12 | grad_norm: 0.7396718859672546
Step 12 | learning_rate: 9.918518518518519e-05
Step 12 | epoch: 0.02666666666666667
Step 13 | loss: 4.347691535949707
Step 13 | grad_norm: 0.6328076124191284
Step 13 | learning_rate: 9.911111111111112e-05
Step 13 | epoch: 0.028888888888888888
Step 14 | loss: 3.7777063846588135
Step 14 | grad_norm: 0.8034061789512634
Step 14 | learning_rate: 9.903703703703705e-05
Step 14 | epoch: 0.03111111111111111
Step 15 | loss: 4.2076520919799805
Step 15 | grad_norm: 0.809776246547699
Step 15 | learning_rate: 9.896296296296297e-05
Step 15 | epoch: 0.03333333333333333
Step 16 | loss: 3.8421552181243896
Step 16 | grad_norm: 0.7145114541053772
Step 16 | learning_rate: 9.888888888888889e-05
Step 16 | epoch: 0.035555555555555556
Step 17 | loss: 4.755592346191406
Step 17 | grad_norm: 0.8693427443504333
Step 17 | learning_rate: 9.881481481481482e-05
Step 17 | epoch: 0.03777777777777778
Step 18 | loss: 3.4725964069366455
Step 18 | grad_norm: 0.6617355346679688
Step 18 | learning_rate: 9.874074074074074e-05
Step 18 | epoch: 0.04
Step 19 | loss: 3.8242406845092773
Step 19 | grad_norm: 1.1502190828323364
Step 19 | learning_rate: 9.866666666666668e-05
Step 19 | epoch: 0.042222222222222223
Step 20 | loss: 3.560502290725708
Step 20 | grad_norm: 0.7810291647911072
Step 20 | learning_rate: 9.85925925925926e-05
Step 20 | epoch: 0.044444444444444446
Step 21 | loss: 4.79761266708374
Step 21 | grad_norm: 1.2062958478927612
Step 21 | learning_rate: 9.851851851851852e-05
Step 21 | epoch: 0.04666666666666667
Step 22 | loss: 4.351813793182373
Step 22 | grad_norm: 1.240935206413269
Step 22 | learning_rate: 9.844444444444444e-05
Step 22 | epoch: 0.04888888888888889
Step 23 | loss: 5.638345718383789
Step 23 | grad_norm: 1.0484440326690674
Step 23 | learning_rate: 9.837037037037038e-05
Step 23 | epoch: 0.051111111111111114
Step 24 | loss: 3.532668113708496
Step 24 | grad_norm: 0.6677753925323486
Step 24 | learning_rate: 9.829629629629629e-05
Step 24 | epoch: 0.05333333333333334
Step 25 | loss: 3.998908281326294
Step 25 | grad_norm: 0.7564359307289124
Step 25 | learning_rate: 9.822222222222223e-05
Step 25 | epoch: 0.05555555555555555
Step 26 | loss: 3.7290306091308594
Step 26 | grad_norm: 0.8538809418678284
Step 26 | learning_rate: 9.814814814814815e-05
Step 26 | epoch: 0.057777777777777775
Step 27 | loss: 4.318568229675293
Step 27 | grad_norm: 0.8092991709709167
Step 27 | learning_rate: 9.807407407407407e-05
Step 27 | epoch: 0.06
Step 28 | loss: 3.5699524879455566
Step 28 | grad_norm: 0.9995025396347046
Step 28 | learning_rate: 9.8e-05
Step 28 | epoch: 0.06222222222222222
Step 29 | loss: 4.668148994445801
Step 29 | grad_norm: 1.275427222251892
Step 29 | learning_rate: 9.792592592592593e-05
Step 29 | epoch: 0.06444444444444444
Step 30 | loss: 4.94503927230835
Step 30 | grad_norm: 1.2772005796432495
Step 30 | learning_rate: 9.785185185185186e-05
Step 30 | epoch: 0.06666666666666667
Step 31 | loss: 3.610184907913208
Step 31 | grad_norm: 1.064988136291504
Step 31 | learning_rate: 9.777777777777778e-05
Step 31 | epoch: 0.06888888888888889
Step 32 | loss: 4.252486705780029
Step 32 | grad_norm: 1.0260618925094604
Step 32 | learning_rate: 9.77037037037037e-05
Step 32 | epoch: 0.07111111111111111
Step 33 | loss: 3.5182807445526123
Step 33 | grad_norm: 0.8214297294616699
Step 33 | learning_rate: 9.762962962962964e-05
Step 33 | epoch: 0.07333333333333333
Step 34 | loss: 4.101268291473389
Step 34 | grad_norm: 0.8861885070800781
Step 34 | learning_rate: 9.755555555555555e-05
Step 34 | epoch: 0.07555555555555556
Step 35 | loss: 4.646877288818359
Step 35 | grad_norm: 1.1290384531021118
Step 35 | learning_rate: 9.748148148148149e-05
Step 35 | epoch: 0.07777777777777778
Step 36 | loss: 5.178895473480225
Step 36 | grad_norm: 1.0592589378356934
Step 36 | learning_rate: 9.740740740740741e-05
Step 36 | epoch: 0.08
Step 37 | loss: 4.908387184143066
Step 37 | grad_norm: 1.7822515964508057
Step 37 | learning_rate: 9.733333333333335e-05
Step 37 | epoch: 0.08222222222222222
Step 38 | loss: 3.600625514984131
Step 38 | grad_norm: 1.0063505172729492
Step 38 | learning_rate: 9.725925925925926e-05
Step 38 | epoch: 0.08444444444444445
Step 39 | loss: 3.6366562843322754
Step 39 | grad_norm: 1.1389929056167603
Step 39 | learning_rate: 9.718518518518519e-05
Step 39 | epoch: 0.08666666666666667
Step 40 | loss: 3.7940890789031982
Step 40 | grad_norm: 0.9699360132217407
Step 40 | learning_rate: 9.711111111111111e-05
Step 40 | epoch: 0.08888888888888889
Step 41 | loss: 4.030343055725098
Step 41 | grad_norm: 1.0546607971191406
Step 41 | learning_rate: 9.703703703703704e-05
Step 41 | epoch: 0.09111111111111111
Step 42 | loss: 4.783731937408447
Step 42 | grad_norm: 1.262649416923523
Step 42 | learning_rate: 9.696296296296296e-05
Step 42 | epoch: 0.09333333333333334
Step 43 | loss: 4.654078006744385
Step 43 | grad_norm: 1.0184215307235718
Step 43 | learning_rate: 9.68888888888889e-05
Step 43 | epoch: 0.09555555555555556
Step 44 | loss: 4.339478015899658
Step 44 | grad_norm: 1.439906358718872
Step 44 | learning_rate: 9.681481481481482e-05
Step 44 | epoch: 0.09777777777777778
Step 45 | loss: 3.9464917182922363
Step 45 | grad_norm: 1.120229721069336
Step 45 | learning_rate: 9.674074074074074e-05
Step 45 | epoch: 0.1
Step 46 | loss: 4.534289836883545
Step 46 | grad_norm: 1.212032437324524
Step 46 | learning_rate: 9.666666666666667e-05
Step 46 | epoch: 0.10222222222222223
Step 47 | loss: 4.814061164855957
Step 47 | grad_norm: 1.1190013885498047
Step 47 | learning_rate: 9.65925925925926e-05
Step 47 | epoch: 0.10444444444444445
Step 48 | loss: 4.027233600616455
Step 48 | grad_norm: 1.7840704917907715
Step 48 | learning_rate: 9.651851851851851e-05
Step 48 | epoch: 0.10666666666666667
Step 49 | loss: 3.891744613647461
Step 49 | grad_norm: 2.2390565872192383
Step 49 | learning_rate: 9.644444444444445e-05
Step 49 | epoch: 0.10888888888888888
Step 50 | loss: 3.6581733226776123
Step 50 | grad_norm: 1.154137372970581
Step 50 | learning_rate: 9.637037037037037e-05
Step 50 | epoch: 0.1111111111111111
Step 51 | loss: 4.108232021331787
Step 51 | grad_norm: 1.6643890142440796
Step 51 | learning_rate: 9.62962962962963e-05
Step 51 | epoch: 0.11333333333333333
Step 52 | loss: 3.849377155303955
Step 52 | grad_norm: 1.0835741758346558
Step 52 | learning_rate: 9.622222222222222e-05
Step 52 | epoch: 0.11555555555555555
Step 53 | loss: 2.828341484069824
Step 53 | grad_norm: 1.157331109046936
Step 53 | learning_rate: 9.614814814814816e-05
Step 53 | epoch: 0.11777777777777777
Step 54 | loss: 4.349862575531006
Step 54 | grad_norm: 1.374178171157837
Step 54 | learning_rate: 9.607407407407408e-05
Step 54 | epoch: 0.12
Step 55 | loss: 3.975872278213501
Step 55 | grad_norm: 1.6332379579544067
Step 55 | learning_rate: 9.6e-05
Step 55 | epoch: 0.12222222222222222
Step 56 | loss: 3.568183183670044
Step 56 | grad_norm: 1.3546451330184937
Step 56 | learning_rate: 9.592592592592593e-05
Step 56 | epoch: 0.12444444444444444
Step 57 | loss: 4.274105072021484
Step 57 | grad_norm: 1.2447818517684937
Step 57 | learning_rate: 9.585185185185186e-05
Step 57 | epoch: 0.12666666666666668
Step 58 | loss: 4.43926477432251
Step 58 | grad_norm: 1.1668039560317993
Step 58 | learning_rate: 9.577777777777777e-05
Step 58 | epoch: 0.1288888888888889
Step 59 | loss: 4.182045936584473
Step 59 | grad_norm: 1.3581849336624146
Step 59 | learning_rate: 9.570370370370371e-05
Step 59 | epoch: 0.13111111111111112
Step 60 | loss: 4.47755765914917
Step 60 | grad_norm: 1.455063819885254
Step 60 | learning_rate: 9.562962962962963e-05
Step 60 | epoch: 0.13333333333333333
Step 61 | loss: 3.8266899585723877
Step 61 | grad_norm: 1.399250864982605
Step 61 | learning_rate: 9.555555555555557e-05
Step 61 | epoch: 0.13555555555555557
Step 62 | loss: 3.7535400390625
Step 62 | grad_norm: 1.3973151445388794
Step 62 | learning_rate: 9.548148148148148e-05
Step 62 | epoch: 0.13777777777777778
Step 63 | loss: 3.3450980186462402
Step 63 | grad_norm: 1.3400243520736694
Step 63 | learning_rate: 9.540740740740741e-05
Step 63 | epoch: 0.14
Step 64 | loss: 3.757127285003662
Step 64 | grad_norm: 1.2216986417770386
Step 64 | learning_rate: 9.533333333333334e-05
Step 64 | epoch: 0.14222222222222222
Step 65 | loss: 3.721895933151245
Step 65 | grad_norm: 1.22819185256958
Step 65 | learning_rate: 9.525925925925926e-05
Step 65 | epoch: 0.14444444444444443
Step 66 | loss: 3.354269504547119
Step 66 | grad_norm: 1.566773772239685
Step 66 | learning_rate: 9.518518518518518e-05
Step 66 | epoch: 0.14666666666666667
Step 67 | loss: 3.992697238922119
Step 67 | grad_norm: 1.3139585256576538
Step 67 | learning_rate: 9.511111111111112e-05
Step 67 | epoch: 0.14888888888888888
Step 68 | loss: 3.8138370513916016
Step 68 | grad_norm: 1.2285187244415283
Step 68 | learning_rate: 9.503703703703704e-05
Step 68 | epoch: 0.1511111111111111
Step 69 | loss: 4.066997528076172
Step 69 | grad_norm: 1.6318150758743286
Step 69 | learning_rate: 9.496296296296297e-05
Step 69 | epoch: 0.15333333333333332
Step 70 | loss: 4.02439022064209
Step 70 | grad_norm: 4.557684421539307
Step 70 | learning_rate: 9.488888888888889e-05
Step 70 | epoch: 0.15555555555555556
Step 71 | loss: 3.7974257469177246
Step 71 | grad_norm: 1.4097802639007568
Step 71 | learning_rate: 9.481481481481483e-05
Step 71 | epoch: 0.15777777777777777
Step 72 | loss: 4.203771591186523
Step 72 | grad_norm: 1.5307354927062988
Step 72 | learning_rate: 9.474074074074074e-05
Step 72 | epoch: 0.16
Step 73 | loss: 3.4423439502716064
Step 73 | grad_norm: 1.2311533689498901
Step 73 | learning_rate: 9.466666666666667e-05
Step 73 | epoch: 0.1622222222222222
Step 74 | loss: 3.5653882026672363
Step 74 | grad_norm: 1.1143124103546143
Step 74 | learning_rate: 9.45925925925926e-05
Step 74 | epoch: 0.16444444444444445
Step 75 | loss: 3.4396770000457764
Step 75 | grad_norm: 1.5754648447036743
Step 75 | learning_rate: 9.451851851851853e-05
Step 75 | epoch: 0.16666666666666666
Step 76 | loss: 3.6380443572998047
Step 76 | grad_norm: 1.5151336193084717
Step 76 | learning_rate: 9.444444444444444e-05
Step 76 | epoch: 0.1688888888888889
Step 77 | loss: 3.8757874965667725
Step 77 | grad_norm: 1.571885585784912
Step 77 | learning_rate: 9.437037037037038e-05
Step 77 | epoch: 0.1711111111111111
Step 78 | loss: 3.991131544113159
Step 78 | grad_norm: 1.6385451555252075
Step 78 | learning_rate: 9.42962962962963e-05
Step 78 | epoch: 0.17333333333333334
Step 79 | loss: 3.943796396255493
Step 79 | grad_norm: 1.5897687673568726
Step 79 | learning_rate: 9.422222222222223e-05
Step 79 | epoch: 0.17555555555555555
Step 80 | loss: 4.667328357696533
Step 80 | grad_norm: 2.2171921730041504
Step 80 | learning_rate: 9.414814814814815e-05
Step 80 | epoch: 0.17777777777777778
Step 81 | loss: 3.723485231399536
Step 81 | grad_norm: 1.8995674848556519
Step 81 | learning_rate: 9.407407407407408e-05
Step 81 | epoch: 0.18
Step 82 | loss: 4.146180629730225
Step 82 | grad_norm: 1.464826226234436
Step 82 | learning_rate: 9.4e-05
Step 82 | epoch: 0.18222222222222223
Step 83 | loss: 3.539597988128662
Step 83 | grad_norm: 1.3028985261917114
Step 83 | learning_rate: 9.392592592592593e-05
Step 83 | epoch: 0.18444444444444444
Step 84 | loss: 3.8142526149749756
Step 84 | grad_norm: 1.7592628002166748
Step 84 | learning_rate: 9.385185185185185e-05
Step 84 | epoch: 0.18666666666666668
Step 85 | loss: 3.5494673252105713
Step 85 | grad_norm: 1.734412670135498
Step 85 | learning_rate: 9.377777777777779e-05
Step 85 | epoch: 0.18888888888888888
Step 86 | loss: 3.486077070236206
Step 86 | grad_norm: 1.6967214345932007
Step 86 | learning_rate: 9.37037037037037e-05
Step 86 | epoch: 0.19111111111111112
Step 87 | loss: 3.7985832691192627
Step 87 | grad_norm: 1.6521555185317993
Step 87 | learning_rate: 9.362962962962964e-05
Step 87 | epoch: 0.19333333333333333
Step 88 | loss: 3.7112646102905273
Step 88 | grad_norm: 1.3830304145812988
Step 88 | learning_rate: 9.355555555555556e-05
Step 88 | epoch: 0.19555555555555557
Step 89 | loss: 3.7696311473846436
Step 89 | grad_norm: 2.8073995113372803
Step 89 | learning_rate: 9.348148148148148e-05
Step 89 | epoch: 0.19777777777777777
Step 90 | loss: 3.3731696605682373
Step 90 | grad_norm: 1.5180929899215698
Step 90 | learning_rate: 9.34074074074074e-05
Step 90 | epoch: 0.2
Step 91 | loss: 2.8009896278381348
Step 91 | grad_norm: 1.6272759437561035
Step 91 | learning_rate: 9.333333333333334e-05
Step 91 | epoch: 0.20222222222222222
Step 92 | loss: 4.161782264709473
Step 92 | grad_norm: 1.5595366954803467
Step 92 | learning_rate: 9.325925925925927e-05
Step 92 | epoch: 0.20444444444444446
Step 93 | loss: 3.7465336322784424
Step 93 | grad_norm: 1.5317718982696533
Step 93 | learning_rate: 9.318518518518519e-05
Step 93 | epoch: 0.20666666666666667
Step 94 | loss: 4.205402851104736
Step 94 | grad_norm: 1.4327483177185059
Step 94 | learning_rate: 9.311111111111111e-05
Step 94 | epoch: 0.2088888888888889
Step 95 | loss: 4.219191551208496
Step 95 | grad_norm: 1.919640302658081
Step 95 | learning_rate: 9.303703703703705e-05
Step 95 | epoch: 0.2111111111111111
Step 96 | loss: 3.209543228149414
Step 96 | grad_norm: 1.2846076488494873
Step 96 | learning_rate: 9.296296296296296e-05
Step 96 | epoch: 0.21333333333333335
Step 97 | loss: 4.252536773681641
Step 97 | grad_norm: 1.5924679040908813
Step 97 | learning_rate: 9.28888888888889e-05
Step 97 | epoch: 0.21555555555555556
Step 98 | loss: 3.7931740283966064
Step 98 | grad_norm: 1.596725344657898
Step 98 | learning_rate: 9.281481481481482e-05
Step 98 | epoch: 0.21777777777777776
Step 99 | loss: 4.909218788146973
Step 99 | grad_norm: 2.0177884101867676
Step 99 | learning_rate: 9.274074074074076e-05
Step 99 | epoch: 0.22
Step 100 | loss: 4.542860507965088
Step 100 | grad_norm: 1.8439149856567383
Step 100 | learning_rate: 9.266666666666666e-05
Step 100 | epoch: 0.2222222222222222
Step 101 | loss: 3.473583936691284
Step 101 | grad_norm: 1.3138693571090698
Step 101 | learning_rate: 9.25925925925926e-05
Step 101 | epoch: 0.22444444444444445
Step 102 | loss: 3.4396753311157227
Step 102 | grad_norm: 1.4860813617706299
Step 102 | learning_rate: 9.251851851851852e-05
Step 102 | epoch: 0.22666666666666666
Step 103 | loss: 3.256979465484619
Step 103 | grad_norm: 1.4108541011810303
Step 103 | learning_rate: 9.244444444444445e-05
Step 103 | epoch: 0.2288888888888889
Step 104 | loss: 3.37676739692688
Step 104 | grad_norm: 1.582767367362976
Step 104 | learning_rate: 9.237037037037037e-05
Step 104 | epoch: 0.2311111111111111
Step 105 | loss: 3.945695161819458
Step 105 | grad_norm: 1.5436115264892578
Step 105 | learning_rate: 9.229629629629631e-05
Step 105 | epoch: 0.23333333333333334
Step 106 | loss: 3.7575583457946777
Step 106 | grad_norm: 1.6197072267532349
Step 106 | learning_rate: 9.222222222222223e-05
Step 106 | epoch: 0.23555555555555555
Step 107 | loss: 3.9457080364227295
Step 107 | grad_norm: 1.8088905811309814
Step 107 | learning_rate: 9.214814814814815e-05
Step 107 | epoch: 0.23777777777777778
Step 108 | loss: 3.5145480632781982
Step 108 | grad_norm: 1.5010429620742798
Step 108 | learning_rate: 9.207407407407408e-05
Step 108 | epoch: 0.24
Step 109 | loss: 4.13616943359375
Step 109 | grad_norm: 1.5339466333389282
Step 109 | learning_rate: 9.200000000000001e-05
Step 109 | epoch: 0.24222222222222223
Step 110 | loss: 2.813347101211548
Step 110 | grad_norm: 2.8928897380828857
Step 110 | learning_rate: 9.192592592592592e-05
Step 110 | epoch: 0.24444444444444444
Step 111 | loss: 3.41109037399292
Step 111 | grad_norm: 1.4339656829833984
Step 111 | learning_rate: 9.185185185185186e-05
Step 111 | epoch: 0.24666666666666667
Step 112 | loss: 3.2702369689941406
Step 112 | grad_norm: 1.2423213720321655
Step 112 | learning_rate: 9.177777777777778e-05
Step 112 | epoch: 0.24888888888888888
Step 113 | loss: 3.783851146697998
Step 113 | grad_norm: 1.8282607793807983
Step 113 | learning_rate: 9.17037037037037e-05
Step 113 | epoch: 0.2511111111111111
Step 114 | loss: 4.015108585357666
Step 114 | grad_norm: 1.8450230360031128
Step 114 | learning_rate: 9.162962962962963e-05
Step 114 | epoch: 0.25333333333333335
Step 115 | loss: 4.054244041442871
Step 115 | grad_norm: 1.9458129405975342
Step 115 | learning_rate: 9.155555555555557e-05
Step 115 | epoch: 0.25555555555555554
Step 116 | loss: 3.941964626312256
Step 116 | grad_norm: 2.717425584793091
Step 116 | learning_rate: 9.148148148148149e-05
Step 116 | epoch: 0.2577777777777778
Step 117 | loss: 3.463134527206421
Step 117 | grad_norm: 1.7099952697753906
Step 117 | learning_rate: 9.140740740740741e-05
Step 117 | epoch: 0.26
Step 118 | loss: 3.8351588249206543
Step 118 | grad_norm: 1.7286981344223022
Step 118 | learning_rate: 9.133333333333334e-05
Step 118 | epoch: 0.26222222222222225
Step 119 | loss: 3.7258341312408447
Step 119 | grad_norm: 1.9945889711380005
Step 119 | learning_rate: 9.125925925925927e-05
Step 119 | epoch: 0.2644444444444444
Step 120 | loss: 4.22238826751709
Step 120 | grad_norm: 2.325970411300659
Step 120 | learning_rate: 9.118518518518518e-05
Step 120 | epoch: 0.26666666666666666
Step 121 | loss: 3.8218259811401367
Step 121 | grad_norm: 1.9527544975280762
Step 121 | learning_rate: 9.111111111111112e-05
Step 121 | epoch: 0.2688888888888889
Step 122 | loss: 3.3282642364501953
Step 122 | grad_norm: 2.8687820434570312
Step 122 | learning_rate: 9.103703703703704e-05
Step 122 | epoch: 0.27111111111111114
Step 123 | loss: 4.228704452514648
Step 123 | grad_norm: 2.2554843425750732
Step 123 | learning_rate: 9.096296296296298e-05
Step 123 | epoch: 0.2733333333333333
Step 124 | loss: 3.388274669647217
Step 124 | grad_norm: 1.6045533418655396
Step 124 | learning_rate: 9.088888888888889e-05
Step 124 | epoch: 0.27555555555555555
Step 125 | loss: 3.630641460418701
Step 125 | grad_norm: 1.6498388051986694
Step 125 | learning_rate: 9.081481481481482e-05
Step 125 | epoch: 0.2777777777777778
Step 126 | loss: 3.663536787033081
Step 126 | grad_norm: 1.3742479085922241
Step 126 | learning_rate: 9.074074074074075e-05
Step 126 | epoch: 0.28
Step 127 | loss: 3.663724660873413
Step 127 | grad_norm: 1.930742621421814
Step 127 | learning_rate: 9.066666666666667e-05
Step 127 | epoch: 0.2822222222222222
Step 128 | loss: 4.153759002685547
Step 128 | grad_norm: 1.8968437910079956
Step 128 | learning_rate: 9.05925925925926e-05
Step 128 | epoch: 0.28444444444444444
Step 129 | loss: 3.254028081893921
Step 129 | grad_norm: 1.4341228008270264
Step 129 | learning_rate: 9.051851851851853e-05
Step 129 | epoch: 0.2866666666666667
Step 130 | loss: 4.424694538116455
Step 130 | grad_norm: 1.7346123456954956
Step 130 | learning_rate: 9.044444444444445e-05
Step 130 | epoch: 0.28888888888888886
Step 131 | loss: 4.243151664733887
Step 131 | grad_norm: 1.7702841758728027
Step 131 | learning_rate: 9.037037037037038e-05
Step 131 | epoch: 0.2911111111111111
Step 132 | loss: 3.420604705810547
Step 132 | grad_norm: 1.902524709701538
Step 132 | learning_rate: 9.02962962962963e-05
Step 132 | epoch: 0.29333333333333333
Step 133 | loss: 3.7971417903900146
Step 133 | grad_norm: 2.3736460208892822
Step 133 | learning_rate: 9.022222222222224e-05
Step 133 | epoch: 0.29555555555555557
Step 134 | loss: 3.7888641357421875
Step 134 | grad_norm: 2.007096529006958
Step 134 | learning_rate: 9.014814814814815e-05
Step 134 | epoch: 0.29777777777777775
Step 135 | loss: 3.730861186981201
Step 135 | grad_norm: 2.1028120517730713
Step 135 | learning_rate: 9.007407407407408e-05
Step 135 | epoch: 0.3
Step 136 | loss: 3.926933765411377
Step 136 | grad_norm: 2.257469654083252
Step 136 | learning_rate: 9e-05
Step 136 | epoch: 0.3022222222222222
Step 137 | loss: 3.2597217559814453
Step 137 | grad_norm: 1.9299591779708862
Step 137 | learning_rate: 8.992592592592594e-05
Step 137 | epoch: 0.30444444444444446
Step 138 | loss: 4.154287338256836
Step 138 | grad_norm: 1.7906233072280884
Step 138 | learning_rate: 8.985185185185185e-05
Step 138 | epoch: 0.30666666666666664
Step 139 | loss: 3.81929612159729
Step 139 | grad_norm: 1.724912405014038
Step 139 | learning_rate: 8.977777777777779e-05
Step 139 | epoch: 0.3088888888888889
Step 140 | loss: 2.8118624687194824
Step 140 | grad_norm: 1.5866148471832275
Step 140 | learning_rate: 8.970370370370371e-05
Step 140 | epoch: 0.3111111111111111
Step 141 | loss: 3.349393844604492
Step 141 | grad_norm: 1.9168304204940796
Step 141 | learning_rate: 8.962962962962963e-05
Step 141 | epoch: 0.31333333333333335
Step 142 | loss: 3.628068208694458
Step 142 | grad_norm: 2.149007558822632
Step 142 | learning_rate: 8.955555555555556e-05
Step 142 | epoch: 0.31555555555555553
Step 143 | loss: 3.61525297164917
Step 143 | grad_norm: 1.701284646987915
Step 143 | learning_rate: 8.94814814814815e-05
Step 143 | epoch: 0.31777777777777777
Step 144 | loss: 3.7280380725860596
Step 144 | grad_norm: 2.139664888381958
Step 144 | learning_rate: 8.94074074074074e-05
Step 144 | epoch: 0.32
Step 145 | loss: 3.8945534229278564
Step 145 | grad_norm: 1.6791183948516846
Step 145 | learning_rate: 8.933333333333334e-05
Step 145 | epoch: 0.32222222222222224
Step 146 | loss: 3.112375497817993
Step 146 | grad_norm: 3.1573855876922607
Step 146 | learning_rate: 8.925925925925926e-05
Step 146 | epoch: 0.3244444444444444
Step 147 | loss: 3.575579881668091
Step 147 | grad_norm: 1.977605938911438
Step 147 | learning_rate: 8.918518518518519e-05
Step 147 | epoch: 0.32666666666666666
Step 148 | loss: 4.55814790725708
Step 148 | grad_norm: 2.4477365016937256
Step 148 | learning_rate: 8.911111111111111e-05
Step 148 | epoch: 0.3288888888888889
Step 149 | loss: 3.550509214401245
Step 149 | grad_norm: 1.8805502653121948
Step 149 | learning_rate: 8.903703703703705e-05
Step 149 | epoch: 0.33111111111111113
Step 150 | loss: 4.304312229156494
Step 150 | grad_norm: 2.0862693786621094
Step 150 | learning_rate: 8.896296296296297e-05
Step 150 | epoch: 0.3333333333333333
Step 151 | loss: 3.837393283843994
Step 151 | grad_norm: 1.8830775022506714
Step 151 | learning_rate: 8.888888888888889e-05
Step 151 | epoch: 0.33555555555555555
Step 152 | loss: 3.8158833980560303
Step 152 | grad_norm: 3.9857535362243652
Step 152 | learning_rate: 8.881481481481482e-05
Step 152 | epoch: 0.3377777777777778
Step 153 | loss: 2.9119131565093994
Step 153 | grad_norm: 1.5220487117767334
Step 153 | learning_rate: 8.874074074074075e-05
Step 153 | epoch: 0.34
Step 154 | loss: 4.084537982940674
Step 154 | grad_norm: 2.186084508895874
Step 154 | learning_rate: 8.866666666666668e-05
Step 154 | epoch: 0.3422222222222222
Step 155 | loss: 2.9483795166015625
Step 155 | grad_norm: 1.59071683883667
Step 155 | learning_rate: 8.85925925925926e-05
Step 155 | epoch: 0.34444444444444444
Step 156 | loss: 3.241762161254883
Step 156 | grad_norm: 1.4914376735687256
Step 156 | learning_rate: 8.851851851851852e-05
Step 156 | epoch: 0.3466666666666667
Step 157 | loss: 3.876000165939331
Step 157 | grad_norm: 2.1844189167022705
Step 157 | learning_rate: 8.844444444444445e-05
Step 157 | epoch: 0.3488888888888889
Step 158 | loss: 3.3671910762786865
Step 158 | grad_norm: 1.7551939487457275
Step 158 | learning_rate: 8.837037037037037e-05
Step 158 | epoch: 0.3511111111111111
Step 159 | loss: 3.7029831409454346
Step 159 | grad_norm: 1.9578931331634521
Step 159 | learning_rate: 8.82962962962963e-05
Step 159 | epoch: 0.35333333333333333
Step 160 | loss: 3.0703494548797607
Step 160 | grad_norm: 1.5658938884735107
Step 160 | learning_rate: 8.822222222222223e-05
Step 160 | epoch: 0.35555555555555557
Step 161 | loss: 3.3451664447784424
Step 161 | grad_norm: 1.869288444519043
Step 161 | learning_rate: 8.814814814814815e-05
Step 161 | epoch: 0.35777777777777775
Step 162 | loss: 3.111438035964966
Step 162 | grad_norm: 1.483860731124878
Step 162 | learning_rate: 8.807407407407407e-05
Step 162 | epoch: 0.36
Step 163 | loss: 3.8304338455200195
Step 163 | grad_norm: 2.660231828689575
Step 163 | learning_rate: 8.800000000000001e-05
Step 163 | epoch: 0.3622222222222222
Step 164 | loss: 2.7526113986968994
Step 164 | grad_norm: 1.5973950624465942
Step 164 | learning_rate: 8.792592592592593e-05
Step 164 | epoch: 0.36444444444444446
Step 165 | loss: 3.6930654048919678
Step 165 | grad_norm: 2.1673479080200195
Step 165 | learning_rate: 8.785185185185186e-05
Step 165 | epoch: 0.36666666666666664
Step 166 | loss: 4.157426357269287
Step 166 | grad_norm: 2.169955253601074
Step 166 | learning_rate: 8.777777777777778e-05
Step 166 | epoch: 0.3688888888888889
Step 167 | loss: 3.4928019046783447
Step 167 | grad_norm: 2.256622314453125
Step 167 | learning_rate: 8.77037037037037e-05
Step 167 | epoch: 0.3711111111111111
Step 168 | loss: 3.7676942348480225
Step 168 | grad_norm: 1.8199100494384766
Step 168 | learning_rate: 8.762962962962964e-05
Step 168 | epoch: 0.37333333333333335
Step 169 | loss: 3.473005771636963
Step 169 | grad_norm: 1.8079618215560913
Step 169 | learning_rate: 8.755555555555556e-05
Step 169 | epoch: 0.37555555555555553
Step 170 | loss: 4.198610305786133
Step 170 | grad_norm: 1.9101269245147705
Step 170 | learning_rate: 8.748148148148149e-05
Step 170 | epoch: 0.37777777777777777
Step 171 | loss: 3.411405563354492
Step 171 | grad_norm: 1.6587530374526978
Step 171 | learning_rate: 8.740740740740741e-05
Step 171 | epoch: 0.38
Step 172 | loss: 2.873215913772583
Step 172 | grad_norm: 1.6865870952606201
Step 172 | learning_rate: 8.733333333333333e-05
Step 172 | epoch: 0.38222222222222224
Step 173 | loss: 3.569882392883301
Step 173 | grad_norm: 1.9947197437286377
Step 173 | learning_rate: 8.725925925925927e-05
Step 173 | epoch: 0.3844444444444444
Step 174 | loss: 3.4267351627349854
Step 174 | grad_norm: 2.2638771533966064
Step 174 | learning_rate: 8.718518518518519e-05
Step 174 | epoch: 0.38666666666666666
Step 175 | loss: 3.4120442867279053
Step 175 | grad_norm: 2.489149808883667
Step 175 | learning_rate: 8.711111111111112e-05
Step 175 | epoch: 0.3888888888888889
Step 176 | loss: 3.558195114135742
Step 176 | grad_norm: 2.5042836666107178
Step 176 | learning_rate: 8.703703703703704e-05
Step 176 | epoch: 0.39111111111111113
Step 177 | loss: 3.8410425186157227
Step 177 | grad_norm: 2.732442617416382
Step 177 | learning_rate: 8.696296296296296e-05
Step 177 | epoch: 0.3933333333333333
Step 178 | loss: 3.797183036804199
Step 178 | grad_norm: 1.742365837097168
Step 178 | learning_rate: 8.68888888888889e-05
Step 178 | epoch: 0.39555555555555555
Step 179 | loss: 4.412693500518799
Step 179 | grad_norm: 1.9385913610458374
Step 179 | learning_rate: 8.681481481481482e-05
Step 179 | epoch: 0.3977777777777778
Step 180 | loss: 4.0716471672058105
Step 180 | grad_norm: 2.190680742263794
Step 180 | learning_rate: 8.674074074074074e-05
Step 180 | epoch: 0.4
Step 181 | loss: 2.904782772064209
Step 181 | grad_norm: 1.880652904510498
Step 181 | learning_rate: 8.666666666666667e-05
Step 181 | epoch: 0.4022222222222222
Step 182 | loss: 3.464653253555298
Step 182 | grad_norm: 1.4349937438964844
Step 182 | learning_rate: 8.659259259259259e-05
Step 182 | epoch: 0.40444444444444444
Step 183 | loss: 3.9134082794189453
Step 183 | grad_norm: 1.7860972881317139
Step 183 | learning_rate: 8.651851851851851e-05
Step 183 | epoch: 0.4066666666666667
Step 184 | loss: 4.30164909362793
Step 184 | grad_norm: 2.1553032398223877
Step 184 | learning_rate: 8.644444444444445e-05
Step 184 | epoch: 0.4088888888888889
Step 185 | loss: 2.699826955795288
Step 185 | grad_norm: 2.1271815299987793
Step 185 | learning_rate: 8.637037037037037e-05
Step 185 | epoch: 0.4111111111111111
Step 186 | loss: 3.92917537689209
Step 186 | grad_norm: 2.3292124271392822
Step 186 | learning_rate: 8.62962962962963e-05
Step 186 | epoch: 0.41333333333333333
Step 187 | loss: 4.096043586730957
Step 187 | grad_norm: 2.1225080490112305
Step 187 | learning_rate: 8.622222222222222e-05
Step 187 | epoch: 0.41555555555555557
Step 188 | loss: 3.4398396015167236
Step 188 | grad_norm: 2.4305527210235596
Step 188 | learning_rate: 8.614814814814816e-05
Step 188 | epoch: 0.4177777777777778
Step 189 | loss: 3.9380340576171875
Step 189 | grad_norm: 2.2347426414489746
Step 189 | learning_rate: 8.607407407407408e-05
Step 189 | epoch: 0.42
Step 190 | loss: 3.344714641571045
Step 190 | grad_norm: 1.8903911113739014
Step 190 | learning_rate: 8.6e-05
Step 190 | epoch: 0.4222222222222222
Step 191 | loss: 3.057857036590576
Step 191 | grad_norm: 1.5062639713287354
Step 191 | learning_rate: 8.592592592592593e-05
Step 191 | epoch: 0.42444444444444446
Step 192 | loss: 3.473668098449707
Step 192 | grad_norm: 2.4352364540100098
Step 192 | learning_rate: 8.585185185185186e-05
Step 192 | epoch: 0.4266666666666667
Step 193 | loss: 3.2062559127807617
Step 193 | grad_norm: 2.0307600498199463
Step 193 | learning_rate: 8.577777777777777e-05
Step 193 | epoch: 0.4288888888888889
Step 194 | loss: 3.6397616863250732
Step 194 | grad_norm: 1.6277440786361694
Step 194 | learning_rate: 8.570370370370371e-05
Step 194 | epoch: 0.4311111111111111
Step 195 | loss: 3.553342819213867
Step 195 | grad_norm: 2.8570079803466797
Step 195 | learning_rate: 8.562962962962963e-05
Step 195 | epoch: 0.43333333333333335
Step 196 | loss: 3.6444814205169678
Step 196 | grad_norm: 2.065480947494507
Step 196 | learning_rate: 8.555555555555556e-05
Step 196 | epoch: 0.43555555555555553
Step 197 | loss: 3.167041540145874
Step 197 | grad_norm: 2.11257266998291
Step 197 | learning_rate: 8.548148148148148e-05
Step 197 | epoch: 0.43777777777777777
Step 198 | loss: 3.52533221244812
Step 198 | grad_norm: 2.337191104888916
Step 198 | learning_rate: 8.540740740740742e-05
Step 198 | epoch: 0.44
Step 199 | loss: 3.3460798263549805
Step 199 | grad_norm: 2.0637340545654297
Step 199 | learning_rate: 8.533333333333334e-05
Step 199 | epoch: 0.44222222222222224
Step 200 | loss: 3.6907079219818115
Step 200 | grad_norm: 2.9459567070007324
Step 200 | learning_rate: 8.525925925925926e-05
Step 200 | epoch: 0.4444444444444444
Step 201 | loss: 3.5725669860839844
Step 201 | grad_norm: 2.9432806968688965
Step 201 | learning_rate: 8.518518518518518e-05
Step 201 | epoch: 0.44666666666666666
Step 202 | loss: 3.781322479248047
Step 202 | grad_norm: 2.2773382663726807
Step 202 | learning_rate: 8.511111111111112e-05
Step 202 | epoch: 0.4488888888888889
Step 203 | loss: 3.5290210247039795
Step 203 | grad_norm: 1.793588638305664
Step 203 | learning_rate: 8.503703703703703e-05
Step 203 | epoch: 0.45111111111111113
Step 204 | loss: 3.5332448482513428
Step 204 | grad_norm: 1.6166521310806274
Step 204 | learning_rate: 8.496296296296297e-05
Step 204 | epoch: 0.4533333333333333
Step 205 | loss: 2.891298294067383
Step 205 | grad_norm: 1.6547791957855225
Step 205 | learning_rate: 8.488888888888889e-05
Step 205 | epoch: 0.45555555555555555
Step 206 | loss: 4.049564361572266
Step 206 | grad_norm: 2.356221914291382
Step 206 | learning_rate: 8.481481481481481e-05
Step 206 | epoch: 0.4577777777777778
Step 207 | loss: 2.4474399089813232
Step 207 | grad_norm: 2.865446090698242
Step 207 | learning_rate: 8.474074074074074e-05
Step 207 | epoch: 0.46
Step 208 | loss: 3.7487878799438477
Step 208 | grad_norm: 2.3053812980651855
Step 208 | learning_rate: 8.466666666666667e-05
Step 208 | epoch: 0.4622222222222222
Step 209 | loss: 3.471963405609131
Step 209 | grad_norm: 2.3420305252075195
Step 209 | learning_rate: 8.45925925925926e-05
Step 209 | epoch: 0.46444444444444444
Step 210 | loss: 2.8943023681640625
Step 210 | grad_norm: 1.5527534484863281
Step 210 | learning_rate: 8.451851851851852e-05
Step 210 | epoch: 0.4666666666666667
Step 211 | loss: 3.782667636871338
Step 211 | grad_norm: 2.1323165893554688
Step 211 | learning_rate: 8.444444444444444e-05
Step 211 | epoch: 0.4688888888888889
Step 212 | loss: 3.2524921894073486
Step 212 | grad_norm: 1.87763249874115
Step 212 | learning_rate: 8.437037037037038e-05
Step 212 | epoch: 0.4711111111111111
Step 213 | loss: 2.329909563064575
Step 213 | grad_norm: 1.7230545282363892
Step 213 | learning_rate: 8.429629629629629e-05
Step 213 | epoch: 0.47333333333333333
Step 214 | loss: 3.9897046089172363
Step 214 | grad_norm: 1.8144892454147339
Step 214 | learning_rate: 8.422222222222223e-05
Step 214 | epoch: 0.47555555555555556
Step 215 | loss: 3.7774412631988525
Step 215 | grad_norm: 2.419067144393921
Step 215 | learning_rate: 8.414814814814815e-05
Step 215 | epoch: 0.4777777777777778
Step 216 | loss: 2.9419920444488525
Step 216 | grad_norm: 1.8686796426773071
Step 216 | learning_rate: 8.407407407407409e-05
Step 216 | epoch: 0.48
Step 217 | loss: 3.875501871109009
Step 217 | grad_norm: 2.3303885459899902
Step 217 | learning_rate: 8.4e-05
Step 217 | epoch: 0.4822222222222222
Step 218 | loss: 4.149895668029785
Step 218 | grad_norm: 2.6039445400238037
Step 218 | learning_rate: 8.392592592592593e-05
Step 218 | epoch: 0.48444444444444446
Step 219 | loss: 2.9412336349487305
Step 219 | grad_norm: 2.0005431175231934
Step 219 | learning_rate: 8.385185185185186e-05
Step 219 | epoch: 0.4866666666666667
Step 220 | loss: 3.4491236209869385
Step 220 | grad_norm: 1.8935730457305908
Step 220 | learning_rate: 8.377777777777778e-05
Step 220 | epoch: 0.4888888888888889
Step 221 | loss: 3.9650611877441406
Step 221 | grad_norm: 2.083920478820801
Step 221 | learning_rate: 8.37037037037037e-05
Step 221 | epoch: 0.4911111111111111
Step 222 | loss: 3.7345380783081055
Step 222 | grad_norm: 2.2406582832336426
Step 222 | learning_rate: 8.362962962962964e-05
Step 222 | epoch: 0.49333333333333335
Step 223 | loss: 2.9392662048339844
Step 223 | grad_norm: 2.1742937564849854
Step 223 | learning_rate: 8.355555555555556e-05
Step 223 | epoch: 0.4955555555555556
Step 224 | loss: 3.9422924518585205
Step 224 | grad_norm: 2.675449848175049
Step 224 | learning_rate: 8.348148148148148e-05
Step 224 | epoch: 0.49777777777777776
Step 225 | loss: 4.277321815490723
Step 225 | grad_norm: 2.1942498683929443
Step 225 | learning_rate: 8.340740740740741e-05
Step 225 | epoch: 0.5
Step 226 | loss: 3.214268922805786
Step 226 | grad_norm: 1.9101011753082275
Step 226 | learning_rate: 8.333333333333334e-05
Step 226 | epoch: 0.5022222222222222
Step 227 | loss: 4.060608863830566
Step 227 | grad_norm: 2.404444694519043
Step 227 | learning_rate: 8.325925925925925e-05
Step 227 | epoch: 0.5044444444444445
Step 228 | loss: 3.207637310028076
Step 228 | grad_norm: 3.266690492630005
Step 228 | learning_rate: 8.318518518518519e-05
Step 228 | epoch: 0.5066666666666667
Step 229 | loss: 3.0357863903045654
Step 229 | grad_norm: 2.045490264892578
Step 229 | learning_rate: 8.311111111111111e-05
Step 229 | epoch: 0.5088888888888888
Step 230 | loss: 3.627332925796509
Step 230 | grad_norm: 1.9292539358139038
Step 230 | learning_rate: 8.303703703703705e-05
Step 230 | epoch: 0.5111111111111111
Step 231 | loss: 3.901398181915283
Step 231 | grad_norm: 2.188422203063965
Step 231 | learning_rate: 8.296296296296296e-05
Step 231 | epoch: 0.5133333333333333
Step 232 | loss: 4.256648063659668
Step 232 | grad_norm: 2.1805007457733154
Step 232 | learning_rate: 8.28888888888889e-05
Step 232 | epoch: 0.5155555555555555
Step 233 | loss: 3.8839128017425537
Step 233 | grad_norm: 2.4465160369873047
Step 233 | learning_rate: 8.281481481481482e-05
Step 233 | epoch: 0.5177777777777778
Step 234 | loss: 3.43174409866333
Step 234 | grad_norm: 2.933852195739746
Step 234 | learning_rate: 8.274074074074074e-05
Step 234 | epoch: 0.52
Step 235 | loss: 3.608215093612671
Step 235 | grad_norm: 1.658294439315796
Step 235 | learning_rate: 8.266666666666667e-05
Step 235 | epoch: 0.5222222222222223
Step 236 | loss: 3.3551764488220215
Step 236 | grad_norm: 3.290499448776245
Step 236 | learning_rate: 8.25925925925926e-05
Step 236 | epoch: 0.5244444444444445
Step 237 | loss: 3.6863760948181152
Step 237 | grad_norm: 2.938469648361206
Step 237 | learning_rate: 8.251851851851851e-05
Step 237 | epoch: 0.5266666666666666
Step 238 | loss: 3.3559279441833496
Step 238 | grad_norm: 1.8569389581680298
Step 238 | learning_rate: 8.244444444444445e-05
Step 238 | epoch: 0.5288888888888889
Step 239 | loss: 3.1958816051483154
Step 239 | grad_norm: 2.709351062774658
Step 239 | learning_rate: 8.237037037037037e-05
Step 239 | epoch: 0.5311111111111111
Step 240 | loss: 3.2936556339263916
Step 240 | grad_norm: 1.935111165046692
Step 240 | learning_rate: 8.229629629629631e-05
Step 240 | epoch: 0.5333333333333333
Step 241 | loss: 3.3768203258514404
Step 241 | grad_norm: 1.6176331043243408
Step 241 | learning_rate: 8.222222222222222e-05
Step 241 | epoch: 0.5355555555555556
Step 242 | loss: 4.031064033508301
Step 242 | grad_norm: 2.785815715789795
Step 242 | learning_rate: 8.214814814814815e-05
Step 242 | epoch: 0.5377777777777778
Step 243 | loss: 3.0068109035491943
Step 243 | grad_norm: 1.6188974380493164
Step 243 | learning_rate: 8.207407407407408e-05
Step 243 | epoch: 0.54
Step 244 | loss: 3.018658399581909
Step 244 | grad_norm: 1.6757137775421143
Step 244 | learning_rate: 8.2e-05
Step 244 | epoch: 0.5422222222222223
Step 245 | loss: 4.321426868438721
Step 245 | grad_norm: 1.9602516889572144
Step 245 | learning_rate: 8.192592592592592e-05
Step 245 | epoch: 0.5444444444444444
Step 246 | loss: 3.600226640701294
Step 246 | grad_norm: 1.9593983888626099
Step 246 | learning_rate: 8.185185185185186e-05
Step 246 | epoch: 0.5466666666666666
Step 247 | loss: 3.4237184524536133
Step 247 | grad_norm: 1.7104300260543823
Step 247 | learning_rate: 8.177777777777778e-05
Step 247 | epoch: 0.5488888888888889
Step 248 | loss: 3.565088987350464
Step 248 | grad_norm: 2.9566917419433594
Step 248 | learning_rate: 8.170370370370371e-05
Step 248 | epoch: 0.5511111111111111
Step 249 | loss: 3.370361328125
Step 249 | grad_norm: 1.7929887771606445
Step 249 | learning_rate: 8.162962962962963e-05
Step 249 | epoch: 0.5533333333333333
Step 250 | loss: 3.266284227371216
Step 250 | grad_norm: 1.8827587366104126
Step 250 | learning_rate: 8.155555555555557e-05
Step 250 | epoch: 0.5555555555555556
Step 251 | loss: 3.686267375946045
Step 251 | grad_norm: 2.5746586322784424
Step 251 | learning_rate: 8.148148148148148e-05
Step 251 | epoch: 0.5577777777777778
Step 252 | loss: 3.555720329284668
Step 252 | grad_norm: 1.5218156576156616
Step 252 | learning_rate: 8.140740740740741e-05
Step 252 | epoch: 0.56
Step 253 | loss: 3.964078903198242
Step 253 | grad_norm: 2.0153191089630127
Step 253 | learning_rate: 8.133333333333334e-05
Step 253 | epoch: 0.5622222222222222
Step 254 | loss: 3.2448644638061523
Step 254 | grad_norm: 3.3653199672698975
Step 254 | learning_rate: 8.125925925925927e-05
Step 254 | epoch: 0.5644444444444444
Step 255 | loss: 3.8120954036712646
Step 255 | grad_norm: 2.141793966293335
Step 255 | learning_rate: 8.118518518518518e-05
Step 255 | epoch: 0.5666666666666667
Step 256 | loss: 3.6444623470306396
Step 256 | grad_norm: 2.075058937072754
Step 256 | learning_rate: 8.111111111111112e-05
Step 256 | epoch: 0.5688888888888889
Step 257 | loss: 2.5552244186401367
Step 257 | grad_norm: 1.8123828172683716
Step 257 | learning_rate: 8.103703703703704e-05
Step 257 | epoch: 0.5711111111111111
Step 258 | loss: 3.8447306156158447
Step 258 | grad_norm: 2.3646092414855957
Step 258 | learning_rate: 8.096296296296297e-05
Step 258 | epoch: 0.5733333333333334
Step 259 | loss: 3.255032539367676
Step 259 | grad_norm: 1.9047633409500122
Step 259 | learning_rate: 8.088888888888889e-05
Step 259 | epoch: 0.5755555555555556
Step 260 | loss: 3.7001953125
Step 260 | grad_norm: 2.0208442211151123
Step 260 | learning_rate: 8.081481481481483e-05
Step 260 | epoch: 0.5777777777777777
Step 261 | loss: 3.7203452587127686
Step 261 | grad_norm: 2.892371654510498
Step 261 | learning_rate: 8.074074074074075e-05
Step 261 | epoch: 0.58
Step 262 | loss: 3.9473226070404053
Step 262 | grad_norm: 2.191819429397583
Step 262 | learning_rate: 8.066666666666667e-05
Step 262 | epoch: 0.5822222222222222
Step 263 | loss: 3.1795198917388916
Step 263 | grad_norm: 2.0408496856689453
Step 263 | learning_rate: 8.05925925925926e-05
Step 263 | epoch: 0.5844444444444444
Step 264 | loss: 3.1513283252716064
Step 264 | grad_norm: 3.764719247817993
Step 264 | learning_rate: 8.051851851851853e-05
Step 264 | epoch: 0.5866666666666667
Step 265 | loss: 3.1508543491363525
Step 265 | grad_norm: 2.3808822631835938
Step 265 | learning_rate: 8.044444444444444e-05
Step 265 | epoch: 0.5888888888888889
Step 266 | loss: 3.239750385284424
Step 266 | grad_norm: 2.2470457553863525
Step 266 | learning_rate: 8.037037037037038e-05
Step 266 | epoch: 0.5911111111111111
Step 267 | loss: 3.0622708797454834
Step 267 | grad_norm: 2.0989997386932373
Step 267 | learning_rate: 8.02962962962963e-05
Step 267 | epoch: 0.5933333333333334
Step 268 | loss: 3.552262544631958
Step 268 | grad_norm: 1.917762041091919
Step 268 | learning_rate: 8.022222222222222e-05
Step 268 | epoch: 0.5955555555555555
Step 269 | loss: 3.951007604598999
Step 269 | grad_norm: 2.419233798980713
Step 269 | learning_rate: 8.014814814814815e-05
Step 269 | epoch: 0.5977777777777777
Step 270 | loss: 3.8060081005096436
Step 270 | grad_norm: 2.8795924186706543
Step 270 | learning_rate: 8.007407407407408e-05
Step 270 | epoch: 0.6
Step 271 | loss: 3.3459644317626953
Step 271 | grad_norm: 2.085780382156372
Step 271 | learning_rate: 8e-05
Step 271 | epoch: 0.6022222222222222
Step 272 | loss: 4.585097789764404
Step 272 | grad_norm: 2.133378505706787
Step 272 | learning_rate: 7.992592592592593e-05
Step 272 | epoch: 0.6044444444444445
Step 273 | loss: 2.826251268386841
Step 273 | grad_norm: 2.2265284061431885
Step 273 | learning_rate: 7.985185185185185e-05
Step 273 | epoch: 0.6066666666666667
Step 274 | loss: 3.1712276935577393
Step 274 | grad_norm: 1.986140251159668
Step 274 | learning_rate: 7.977777777777779e-05
Step 274 | epoch: 0.6088888888888889
Step 275 | loss: 3.579233169555664
Step 275 | grad_norm: 2.87312912940979
Step 275 | learning_rate: 7.97037037037037e-05
Step 275 | epoch: 0.6111111111111112
Step 276 | loss: 3.839205503463745
Step 276 | grad_norm: 2.2052433490753174
Step 276 | learning_rate: 7.962962962962964e-05
Step 276 | epoch: 0.6133333333333333
Step 277 | loss: 3.20632266998291
Step 277 | grad_norm: 2.249070405960083
Step 277 | learning_rate: 7.955555555555556e-05
Step 277 | epoch: 0.6155555555555555
Step 278 | loss: 2.915062427520752
Step 278 | grad_norm: 2.29189395904541
Step 278 | learning_rate: 7.94814814814815e-05
Step 278 | epoch: 0.6177777777777778
Step 279 | loss: 3.7765610218048096
Step 279 | grad_norm: 2.5772485733032227
Step 279 | learning_rate: 7.94074074074074e-05
Step 279 | epoch: 0.62
Step 280 | loss: 3.5770068168640137
Step 280 | grad_norm: 2.171602725982666
Step 280 | learning_rate: 7.933333333333334e-05
Step 280 | epoch: 0.6222222222222222
Step 281 | loss: 3.1057190895080566
Step 281 | grad_norm: 1.9124605655670166
Step 281 | learning_rate: 7.925925925925926e-05
Step 281 | epoch: 0.6244444444444445
Step 282 | loss: 2.874422311782837
Step 282 | grad_norm: 1.7711529731750488
Step 282 | learning_rate: 7.918518518518519e-05
Step 282 | epoch: 0.6266666666666667
Step 283 | loss: 3.995978355407715
Step 283 | grad_norm: 2.686326026916504
Step 283 | learning_rate: 7.911111111111111e-05
Step 283 | epoch: 0.6288888888888889
Step 284 | loss: 3.3675529956817627
Step 284 | grad_norm: 2.175832748413086
Step 284 | learning_rate: 7.903703703703705e-05
Step 284 | epoch: 0.6311111111111111
Step 285 | loss: 3.9734647274017334
Step 285 | grad_norm: 2.4604122638702393
Step 285 | learning_rate: 7.896296296296297e-05
Step 285 | epoch: 0.6333333333333333
Step 286 | loss: 3.76750111579895
Step 286 | grad_norm: 2.4250423908233643
Step 286 | learning_rate: 7.88888888888889e-05
Step 286 | epoch: 0.6355555555555555
Step 287 | loss: 3.0440757274627686
Step 287 | grad_norm: 1.8014991283416748
Step 287 | learning_rate: 7.881481481481482e-05
Step 287 | epoch: 0.6377777777777778
Step 288 | loss: 2.73309063911438
Step 288 | grad_norm: 1.7600104808807373
Step 288 | learning_rate: 7.874074074074075e-05
Step 288 | epoch: 0.64
Step 289 | loss: 3.194457769393921
Step 289 | grad_norm: 1.920631766319275
Step 289 | learning_rate: 7.866666666666666e-05
Step 289 | epoch: 0.6422222222222222
Step 290 | loss: 3.1105504035949707
Step 290 | grad_norm: 2.096118211746216
Step 290 | learning_rate: 7.85925925925926e-05
Step 290 | epoch: 0.6444444444444445
Step 291 | loss: 4.149672031402588
Step 291 | grad_norm: 2.6249914169311523
Step 291 | learning_rate: 7.851851851851852e-05
Step 291 | epoch: 0.6466666666666666
Step 292 | loss: 2.567350387573242
Step 292 | grad_norm: 1.646643042564392
Step 292 | learning_rate: 7.844444444444446e-05
Step 292 | epoch: 0.6488888888888888
Step 293 | loss: 2.831303596496582
Step 293 | grad_norm: 1.7024528980255127
Step 293 | learning_rate: 7.837037037037037e-05
Step 293 | epoch: 0.6511111111111111
Step 294 | loss: 3.447467803955078
Step 294 | grad_norm: 2.6592023372650146
Step 294 | learning_rate: 7.82962962962963e-05
Step 294 | epoch: 0.6533333333333333
Step 295 | loss: 3.3847904205322266
Step 295 | grad_norm: 1.8472552299499512
Step 295 | learning_rate: 7.822222222222223e-05
Step 295 | epoch: 0.6555555555555556
Step 296 | loss: 2.85261869430542
Step 296 | grad_norm: 2.6449413299560547
Step 296 | learning_rate: 7.814814814814815e-05
Step 296 | epoch: 0.6577777777777778
Step 297 | loss: 3.3022022247314453
Step 297 | grad_norm: 2.365410566329956
Step 297 | learning_rate: 7.807407407407408e-05
Step 297 | epoch: 0.66
Step 298 | loss: 3.957136631011963
Step 298 | grad_norm: 2.125969648361206
Step 298 | learning_rate: 7.800000000000001e-05
Step 298 | epoch: 0.6622222222222223
Step 299 | loss: 3.2623581886291504
Step 299 | grad_norm: 2.803952217102051
Step 299 | learning_rate: 7.792592592592592e-05
Step 299 | epoch: 0.6644444444444444
Step 300 | loss: 3.049325942993164
Step 300 | grad_norm: 1.9124053716659546
Step 300 | learning_rate: 7.785185185185186e-05
Step 300 | epoch: 0.6666666666666666
Step 301 | loss: 3.0856175422668457
Step 301 | grad_norm: 2.416440963745117
Step 301 | learning_rate: 7.777777777777778e-05
Step 301 | epoch: 0.6688888888888889
Step 302 | loss: 3.571986198425293
Step 302 | grad_norm: 2.801112651824951
Step 302 | learning_rate: 7.770370370370372e-05
Step 302 | epoch: 0.6711111111111111
Step 303 | loss: 4.130495548248291
Step 303 | grad_norm: 2.091519594192505
Step 303 | learning_rate: 7.762962962962963e-05
Step 303 | epoch: 0.6733333333333333
Step 304 | loss: 3.902207612991333
Step 304 | grad_norm: 2.1560299396514893
Step 304 | learning_rate: 7.755555555555556e-05
Step 304 | epoch: 0.6755555555555556
Step 305 | loss: 4.513683795928955
Step 305 | grad_norm: 2.6196248531341553
Step 305 | learning_rate: 7.748148148148149e-05
Step 305 | epoch: 0.6777777777777778
Step 306 | loss: 4.059693336486816
Step 306 | grad_norm: 1.9117860794067383
Step 306 | learning_rate: 7.740740740740741e-05
Step 306 | epoch: 0.68
Step 307 | loss: 4.0542778968811035
Step 307 | grad_norm: 2.136152505874634
Step 307 | learning_rate: 7.733333333333333e-05
Step 307 | epoch: 0.6822222222222222
Step 308 | loss: 2.5916860103607178
Step 308 | grad_norm: 1.7033348083496094
Step 308 | learning_rate: 7.725925925925927e-05
Step 308 | epoch: 0.6844444444444444
Step 309 | loss: 3.621466875076294
Step 309 | grad_norm: 2.3336710929870605
Step 309 | learning_rate: 7.71851851851852e-05
Step 309 | epoch: 0.6866666666666666
Step 310 | loss: 4.034036159515381
Step 310 | grad_norm: 2.6369645595550537
Step 310 | learning_rate: 7.711111111111112e-05
Step 310 | epoch: 0.6888888888888889
Step 311 | loss: 3.0634632110595703
Step 311 | grad_norm: 1.7273781299591064
Step 311 | learning_rate: 7.703703703703704e-05
Step 311 | epoch: 0.6911111111111111
Step 312 | loss: 4.646149158477783
Step 312 | grad_norm: 2.7221999168395996
Step 312 | learning_rate: 7.696296296296298e-05
Step 312 | epoch: 0.6933333333333334
Step 313 | loss: 3.7877368927001953
Step 313 | grad_norm: 2.033111333847046
Step 313 | learning_rate: 7.688888888888889e-05
Step 313 | epoch: 0.6955555555555556
Step 314 | loss: 3.99086856842041
Step 314 | grad_norm: 2.127573251724243
Step 314 | learning_rate: 7.681481481481482e-05
Step 314 | epoch: 0.6977777777777778
Step 315 | loss: 4.110041618347168
Step 315 | grad_norm: 2.9100632667541504
Step 315 | learning_rate: 7.674074074074075e-05
Step 315 | epoch: 0.7
Step 316 | loss: 3.545684814453125
Step 316 | grad_norm: 1.7420116662979126
Step 316 | learning_rate: 7.666666666666667e-05
Step 316 | epoch: 0.7022222222222222
Step 317 | loss: 2.961664915084839
Step 317 | grad_norm: 1.6648614406585693
Step 317 | learning_rate: 7.659259259259259e-05
Step 317 | epoch: 0.7044444444444444
Step 318 | loss: 3.2524046897888184
Step 318 | grad_norm: 2.049156427383423
Step 318 | learning_rate: 7.651851851851853e-05
Step 318 | epoch: 0.7066666666666667
Step 319 | loss: 3.153676748275757
Step 319 | grad_norm: 2.0863254070281982
Step 319 | learning_rate: 7.644444444444445e-05
Step 319 | epoch: 0.7088888888888889
Step 320 | loss: 4.312110424041748
Step 320 | grad_norm: 2.291325330734253
Step 320 | learning_rate: 7.637037037037038e-05
Step 320 | epoch: 0.7111111111111111
Step 321 | loss: 3.3784663677215576
Step 321 | grad_norm: 1.9648703336715698
Step 321 | learning_rate: 7.62962962962963e-05
Step 321 | epoch: 0.7133333333333334
Step 322 | loss: 3.8319270610809326
Step 322 | grad_norm: 2.217036724090576
Step 322 | learning_rate: 7.622222222222223e-05
Step 322 | epoch: 0.7155555555555555
Step 323 | loss: 3.6528611183166504
Step 323 | grad_norm: 2.2345359325408936
Step 323 | learning_rate: 7.614814814814816e-05
Step 323 | epoch: 0.7177777777777777
Step 324 | loss: 3.4925601482391357
Step 324 | grad_norm: 2.401012659072876
Step 324 | learning_rate: 7.607407407407408e-05
Step 324 | epoch: 0.72
Step 325 | loss: 3.6393752098083496
Step 325 | grad_norm: 1.9249845743179321
Step 325 | learning_rate: 7.6e-05
Step 325 | epoch: 0.7222222222222222
Step 326 | loss: 3.8581244945526123
Step 326 | grad_norm: 2.266052722930908
Step 326 | learning_rate: 7.592592592592593e-05
Step 326 | epoch: 0.7244444444444444
Step 327 | loss: 4.073860168457031
Step 327 | grad_norm: 2.0915558338165283
Step 327 | learning_rate: 7.585185185185185e-05
Step 327 | epoch: 0.7266666666666667
Step 328 | loss: 2.8661551475524902
Step 328 | grad_norm: 1.5091215372085571
Step 328 | learning_rate: 7.577777777777779e-05
Step 328 | epoch: 0.7288888888888889
Step 329 | loss: 4.764150619506836
Step 329 | grad_norm: 2.839486837387085
Step 329 | learning_rate: 7.570370370370371e-05
Step 329 | epoch: 0.7311111111111112
Step 330 | loss: 2.838887929916382
Step 330 | grad_norm: 1.95138418674469
Step 330 | learning_rate: 7.562962962962963e-05
Step 330 | epoch: 0.7333333333333333
Step 331 | loss: 3.870756149291992
Step 331 | grad_norm: 3.3254988193511963
Step 331 | learning_rate: 7.555555555555556e-05
Step 331 | epoch: 0.7355555555555555
Step 332 | loss: 3.502972364425659
Step 332 | grad_norm: 2.7619166374206543
Step 332 | learning_rate: 7.548148148148148e-05
Step 332 | epoch: 0.7377777777777778
Step 333 | loss: 4.008933067321777
Step 333 | grad_norm: 3.130652904510498
Step 333 | learning_rate: 7.540740740740742e-05
Step 333 | epoch: 0.74
Step 334 | loss: 2.5339231491088867
Step 334 | grad_norm: 1.8295148611068726
Step 334 | learning_rate: 7.533333333333334e-05
Step 334 | epoch: 0.7422222222222222
Step 335 | loss: 3.882082223892212
Step 335 | grad_norm: 1.8106250762939453
Step 335 | learning_rate: 7.525925925925926e-05
Step 335 | epoch: 0.7444444444444445
Step 336 | loss: 3.377375364303589
Step 336 | grad_norm: 2.3553366661071777
Step 336 | learning_rate: 7.518518518518519e-05
Step 336 | epoch: 0.7466666666666667
Step 337 | loss: 3.5676686763763428
Step 337 | grad_norm: 2.3490262031555176
Step 337 | learning_rate: 7.511111111111111e-05
Step 337 | epoch: 0.7488888888888889
Step 338 | loss: 3.4808669090270996
Step 338 | grad_norm: 2.0554065704345703
Step 338 | learning_rate: 7.503703703703705e-05
Step 338 | epoch: 0.7511111111111111
Step 339 | loss: 2.8905115127563477
Step 339 | grad_norm: 1.6461498737335205
Step 339 | learning_rate: 7.496296296296297e-05
Step 339 | epoch: 0.7533333333333333
Step 340 | loss: 2.9939401149749756
Step 340 | grad_norm: 2.12271785736084
Step 340 | learning_rate: 7.488888888888889e-05
Step 340 | epoch: 0.7555555555555555
Step 341 | loss: 3.7294766902923584
Step 341 | grad_norm: 2.3521885871887207
Step 341 | learning_rate: 7.481481481481481e-05
Step 341 | epoch: 0.7577777777777778
Step 342 | loss: 3.4764585494995117
Step 342 | grad_norm: 2.0631866455078125
Step 342 | learning_rate: 7.474074074074074e-05
Step 342 | epoch: 0.76
Step 343 | loss: 3.8688578605651855
Step 343 | grad_norm: 1.8667768239974976
Step 343 | learning_rate: 7.466666666666667e-05
Step 343 | epoch: 0.7622222222222222
Step 344 | loss: 4.649506092071533
Step 344 | grad_norm: 2.5160579681396484
Step 344 | learning_rate: 7.45925925925926e-05
Step 344 | epoch: 0.7644444444444445
Step 345 | loss: 3.4751622676849365
Step 345 | grad_norm: 2.1981327533721924
Step 345 | learning_rate: 7.451851851851852e-05
Step 345 | epoch: 0.7666666666666667
Step 346 | loss: 3.491489887237549
Step 346 | grad_norm: 3.3275115489959717
Step 346 | learning_rate: 7.444444444444444e-05
Step 346 | epoch: 0.7688888888888888
Step 347 | loss: 3.713526725769043
Step 347 | grad_norm: 1.94583261013031
Step 347 | learning_rate: 7.437037037037038e-05
Step 347 | epoch: 0.7711111111111111
Step 348 | loss: 3.0603601932525635
Step 348 | grad_norm: 1.7096492052078247
Step 348 | learning_rate: 7.42962962962963e-05
Step 348 | epoch: 0.7733333333333333
Step 349 | loss: 3.370105504989624
Step 349 | grad_norm: 2.5581843852996826
Step 349 | learning_rate: 7.422222222222223e-05
Step 349 | epoch: 0.7755555555555556
Step 350 | loss: 3.900738477706909
Step 350 | grad_norm: 2.5888261795043945
Step 350 | learning_rate: 7.414814814814815e-05
Step 350 | epoch: 0.7777777777777778
Step 351 | loss: 3.2822153568267822
Step 351 | grad_norm: 1.9996776580810547
Step 351 | learning_rate: 7.407407407407407e-05
Step 351 | epoch: 0.78
Step 352 | loss: 3.9529106616973877
Step 352 | grad_norm: 2.746358633041382
Step 352 | learning_rate: 7.4e-05
Step 352 | epoch: 0.7822222222222223
Step 353 | loss: 3.3208858966827393
Step 353 | grad_norm: 5.24462366104126
Step 353 | learning_rate: 7.392592592592593e-05
Step 353 | epoch: 0.7844444444444445
Step 354 | loss: 2.8531863689422607
Step 354 | grad_norm: 1.9171162843704224
Step 354 | learning_rate: 7.385185185185186e-05
Step 354 | epoch: 0.7866666666666666
Step 355 | loss: 3.139669418334961
Step 355 | grad_norm: 2.857006072998047
Step 355 | learning_rate: 7.377777777777778e-05
Step 355 | epoch: 0.7888888888888889
Step 356 | loss: 3.0129592418670654
Step 356 | grad_norm: 2.1155800819396973
Step 356 | learning_rate: 7.37037037037037e-05
Step 356 | epoch: 0.7911111111111111
Step 357 | loss: 3.475421667098999
Step 357 | grad_norm: 2.086609125137329
Step 357 | learning_rate: 7.362962962962964e-05
Step 357 | epoch: 0.7933333333333333
Step 358 | loss: 2.9951138496398926
Step 358 | grad_norm: 1.6744334697723389
Step 358 | learning_rate: 7.355555555555556e-05
Step 358 | epoch: 0.7955555555555556
Step 359 | loss: 3.0327908992767334
Step 359 | grad_norm: 1.8605451583862305
Step 359 | learning_rate: 7.348148148148149e-05
Step 359 | epoch: 0.7977777777777778
Step 360 | loss: 4.1725993156433105
Step 360 | grad_norm: 2.7394368648529053
Step 360 | learning_rate: 7.340740740740741e-05
Step 360 | epoch: 0.8
Step 361 | loss: 3.2334203720092773
Step 361 | grad_norm: 2.0766372680664062
Step 361 | learning_rate: 7.333333333333333e-05
Step 361 | epoch: 0.8022222222222222
Step 362 | loss: 2.6359236240386963
Step 362 | grad_norm: 1.7224185466766357
Step 362 | learning_rate: 7.325925925925925e-05
Step 362 | epoch: 0.8044444444444444
Step 363 | loss: 4.370731830596924
Step 363 | grad_norm: 2.484459638595581
Step 363 | learning_rate: 7.318518518518519e-05
Step 363 | epoch: 0.8066666666666666
Step 364 | loss: 3.477787971496582
Step 364 | grad_norm: 2.386228322982788
Step 364 | learning_rate: 7.311111111111111e-05
Step 364 | epoch: 0.8088888888888889
Step 365 | loss: 3.3162038326263428
Step 365 | grad_norm: 2.1279196739196777
Step 365 | learning_rate: 7.303703703703704e-05
Step 365 | epoch: 0.8111111111111111
Step 366 | loss: 3.622586727142334
Step 366 | grad_norm: 2.31978440284729
Step 366 | learning_rate: 7.296296296296296e-05
Step 366 | epoch: 0.8133333333333334
Step 367 | loss: 3.307448387145996
Step 367 | grad_norm: 1.9208941459655762
Step 367 | learning_rate: 7.28888888888889e-05
Step 367 | epoch: 0.8155555555555556
Step 368 | loss: 2.702781915664673
Step 368 | grad_norm: 2.213135004043579
Step 368 | learning_rate: 7.281481481481481e-05
Step 368 | epoch: 0.8177777777777778
Step 369 | loss: 4.343456745147705
Step 369 | grad_norm: 2.6199116706848145
Step 369 | learning_rate: 7.274074074074074e-05
Step 369 | epoch: 0.82
Step 370 | loss: 4.843178749084473
Step 370 | grad_norm: 2.822906494140625
Step 370 | learning_rate: 7.266666666666667e-05
Step 370 | epoch: 0.8222222222222222
Step 371 | loss: 4.20515251159668
Step 371 | grad_norm: 1.9912316799163818
Step 371 | learning_rate: 7.25925925925926e-05
Step 371 | epoch: 0.8244444444444444
Step 372 | loss: 3.4122869968414307
Step 372 | grad_norm: 2.151320457458496
Step 372 | learning_rate: 7.251851851851851e-05
Step 372 | epoch: 0.8266666666666667
Step 373 | loss: 4.032119274139404
Step 373 | grad_norm: 2.2889790534973145
Step 373 | learning_rate: 7.244444444444445e-05
Step 373 | epoch: 0.8288888888888889
Step 374 | loss: 3.8282182216644287
Step 374 | grad_norm: 2.3004956245422363
Step 374 | learning_rate: 7.237037037037037e-05
Step 374 | epoch: 0.8311111111111111
Step 375 | loss: 4.2754926681518555
Step 375 | grad_norm: 2.5621461868286133
Step 375 | learning_rate: 7.22962962962963e-05
Step 375 | epoch: 0.8333333333333334
Step 376 | loss: 3.5703330039978027
Step 376 | grad_norm: 2.0537009239196777
Step 376 | learning_rate: 7.222222222222222e-05
Step 376 | epoch: 0.8355555555555556
Step 377 | loss: 2.6273646354675293
Step 377 | grad_norm: 2.5740201473236084
Step 377 | learning_rate: 7.214814814814816e-05
Step 377 | epoch: 0.8377777777777777
Step 378 | loss: 2.8371968269348145
Step 378 | grad_norm: 1.7508063316345215
Step 378 | learning_rate: 7.207407407407408e-05
Step 378 | epoch: 0.84
Step 379 | loss: 3.4185681343078613
Step 379 | grad_norm: 2.0266900062561035
Step 379 | learning_rate: 7.2e-05
Step 379 | epoch: 0.8422222222222222
Step 380 | loss: 3.0383903980255127
Step 380 | grad_norm: 1.6600704193115234
Step 380 | learning_rate: 7.192592592592592e-05
Step 380 | epoch: 0.8444444444444444
Step 381 | loss: 2.613370418548584
Step 381 | grad_norm: 4.724678993225098
Step 381 | learning_rate: 7.185185185185186e-05
Step 381 | epoch: 0.8466666666666667
Step 382 | loss: 3.3615381717681885
Step 382 | grad_norm: 1.9236469268798828
Step 382 | learning_rate: 7.177777777777777e-05
Step 382 | epoch: 0.8488888888888889
Step 383 | loss: 3.6674132347106934
Step 383 | grad_norm: 1.850706696510315
Step 383 | learning_rate: 7.170370370370371e-05
Step 383 | epoch: 0.8511111111111112
Step 384 | loss: 3.693809747695923
Step 384 | grad_norm: 2.231985330581665
Step 384 | learning_rate: 7.162962962962963e-05
Step 384 | epoch: 0.8533333333333334
Step 385 | loss: 3.9728968143463135
Step 385 | grad_norm: 1.8534270524978638
Step 385 | learning_rate: 7.155555555555555e-05
Step 385 | epoch: 0.8555555555555555
Step 386 | loss: 3.5494914054870605
Step 386 | grad_norm: 2.4855234622955322
Step 386 | learning_rate: 7.148148148148148e-05
Step 386 | epoch: 0.8577777777777778
Step 387 | loss: 3.4848787784576416
Step 387 | grad_norm: 1.9587554931640625
Step 387 | learning_rate: 7.140740740740741e-05
Step 387 | epoch: 0.86
Step 388 | loss: 2.984251022338867
Step 388 | grad_norm: 2.304821491241455
Step 388 | learning_rate: 7.133333333333334e-05
Step 388 | epoch: 0.8622222222222222
Step 389 | loss: 3.3479063510894775
Step 389 | grad_norm: 2.025578498840332
Step 389 | learning_rate: 7.125925925925926e-05
Step 389 | epoch: 0.8644444444444445
Step 390 | loss: 3.30120849609375
Step 390 | grad_norm: 2.3089447021484375
Step 390 | learning_rate: 7.118518518518518e-05
Step 390 | epoch: 0.8666666666666667
Step 391 | loss: 3.1094391345977783
Step 391 | grad_norm: 2.3200080394744873
Step 391 | learning_rate: 7.111111111111112e-05
Step 391 | epoch: 0.8688888888888889
Step 392 | loss: 3.472702980041504
Step 392 | grad_norm: 1.9344573020935059
Step 392 | learning_rate: 7.103703703703703e-05
Step 392 | epoch: 0.8711111111111111
Step 393 | loss: 3.478402614593506
Step 393 | grad_norm: 2.361172676086426
Step 393 | learning_rate: 7.096296296296297e-05
Step 393 | epoch: 0.8733333333333333
Step 394 | loss: 4.195934295654297
Step 394 | grad_norm: 2.3791489601135254
Step 394 | learning_rate: 7.088888888888889e-05
Step 394 | epoch: 0.8755555555555555
Step 395 | loss: 4.101932525634766
Step 395 | grad_norm: 2.067932605743408
Step 395 | learning_rate: 7.081481481481483e-05
Step 395 | epoch: 0.8777777777777778
Step 396 | loss: 2.667532444000244
Step 396 | grad_norm: 2.624067783355713
Step 396 | learning_rate: 7.074074074074074e-05
Step 396 | epoch: 0.88
Step 397 | loss: 3.373086452484131
Step 397 | grad_norm: 1.9727851152420044
Step 397 | learning_rate: 7.066666666666667e-05
Step 397 | epoch: 0.8822222222222222
Step 398 | loss: 3.0226964950561523
Step 398 | grad_norm: 1.9516900777816772
Step 398 | learning_rate: 7.05925925925926e-05
Step 398 | epoch: 0.8844444444444445
Step 399 | loss: 3.1814053058624268
Step 399 | grad_norm: 2.588996171951294
Step 399 | learning_rate: 7.051851851851852e-05
Step 399 | epoch: 0.8866666666666667
Step 400 | loss: 3.950702667236328
Step 400 | grad_norm: 2.5048351287841797
Step 400 | learning_rate: 7.044444444444444e-05
Step 400 | epoch: 0.8888888888888888
Step 401 | loss: 3.250927686691284
Step 401 | grad_norm: 1.8489038944244385
Step 401 | learning_rate: 7.037037037037038e-05
Step 401 | epoch: 0.8911111111111111
Step 402 | loss: 3.676095962524414
Step 402 | grad_norm: 3.078622579574585
Step 402 | learning_rate: 7.02962962962963e-05
Step 402 | epoch: 0.8933333333333333
Step 403 | loss: 3.540830135345459
Step 403 | grad_norm: 1.9597865343093872
Step 403 | learning_rate: 7.022222222222222e-05
Step 403 | epoch: 0.8955555555555555
Step 404 | loss: 3.509521961212158
Step 404 | grad_norm: 2.011442184448242
Step 404 | learning_rate: 7.014814814814815e-05
Step 404 | epoch: 0.8977777777777778
Step 405 | loss: 3.345139741897583
Step 405 | grad_norm: 2.363900661468506
Step 405 | learning_rate: 7.007407407407408e-05
Step 405 | epoch: 0.9
Step 406 | loss: 4.489770889282227
Step 406 | grad_norm: 3.3938543796539307
Step 406 | learning_rate: 7e-05
Step 406 | epoch: 0.9022222222222223
Step 407 | loss: 2.9692745208740234
Step 407 | grad_norm: 2.3450286388397217
Step 407 | learning_rate: 6.992592592592593e-05
Step 407 | epoch: 0.9044444444444445
Step 408 | loss: 3.9971399307250977
Step 408 | grad_norm: 2.433481216430664
Step 408 | learning_rate: 6.985185185185185e-05
Step 408 | epoch: 0.9066666666666666
Step 409 | loss: 3.5467710494995117
Step 409 | grad_norm: 3.0539021492004395
Step 409 | learning_rate: 6.977777777777779e-05
Step 409 | epoch: 0.9088888888888889
Step 410 | loss: 3.0950000286102295
Step 410 | grad_norm: 2.8933520317077637
Step 410 | learning_rate: 6.97037037037037e-05
Step 410 | epoch: 0.9111111111111111
Step 411 | loss: 3.2437744140625
Step 411 | grad_norm: 1.894351840019226
Step 411 | learning_rate: 6.962962962962964e-05
Step 411 | epoch: 0.9133333333333333
Step 412 | loss: 3.590257167816162
Step 412 | grad_norm: 2.161452531814575
Step 412 | learning_rate: 6.955555555555556e-05
Step 412 | epoch: 0.9155555555555556
Step 413 | loss: 4.185503959655762
Step 413 | grad_norm: 3.0219662189483643
Step 413 | learning_rate: 6.948148148148148e-05
Step 413 | epoch: 0.9177777777777778
Step 414 | loss: 4.130402565002441
Step 414 | grad_norm: 2.710880756378174
Step 414 | learning_rate: 6.94074074074074e-05
Step 414 | epoch: 0.92
Step 415 | loss: 3.503533363342285
Step 415 | grad_norm: 2.149890661239624
Step 415 | learning_rate: 6.933333333333334e-05
Step 415 | epoch: 0.9222222222222223
Step 416 | loss: 2.5104682445526123
Step 416 | grad_norm: 1.624253749847412
Step 416 | learning_rate: 6.925925925925925e-05
Step 416 | epoch: 0.9244444444444444
Step 417 | loss: 3.071338653564453
Step 417 | grad_norm: 2.1812307834625244
Step 417 | learning_rate: 6.918518518518519e-05
Step 417 | epoch: 0.9266666666666666
Step 418 | loss: 3.335632562637329
Step 418 | grad_norm: 1.9401631355285645
Step 418 | learning_rate: 6.911111111111111e-05
Step 418 | epoch: 0.9288888888888889
Step 419 | loss: 3.3546600341796875
Step 419 | grad_norm: 2.073359966278076
Step 419 | learning_rate: 6.903703703703705e-05
Step 419 | epoch: 0.9311111111111111
Step 420 | loss: 3.866394281387329
Step 420 | grad_norm: 2.353040933609009
Step 420 | learning_rate: 6.896296296296296e-05
Step 420 | epoch: 0.9333333333333333
Step 421 | loss: 3.4134066104888916
Step 421 | grad_norm: 2.3179662227630615
Step 421 | learning_rate: 6.88888888888889e-05
Step 421 | epoch: 0.9355555555555556
Step 422 | loss: 3.238814353942871
Step 422 | grad_norm: 2.6709773540496826
Step 422 | learning_rate: 6.881481481481482e-05
Step 422 | epoch: 0.9377777777777778
Step 423 | loss: 2.9225263595581055
Step 423 | grad_norm: 2.31498122215271
Step 423 | learning_rate: 6.874074074074074e-05
Step 423 | epoch: 0.94
Step 424 | loss: 3.6147687435150146
Step 424 | grad_norm: 2.3113980293273926
Step 424 | learning_rate: 6.866666666666666e-05
Step 424 | epoch: 0.9422222222222222
Step 425 | loss: 2.4018423557281494
Step 425 | grad_norm: 1.7409067153930664
Step 425 | learning_rate: 6.85925925925926e-05
Step 425 | epoch: 0.9444444444444444
Step 426 | loss: 3.8017385005950928
Step 426 | grad_norm: 2.2510082721710205
Step 426 | learning_rate: 6.851851851851852e-05
Step 426 | epoch: 0.9466666666666667
Step 427 | loss: 2.1773288249969482
Step 427 | grad_norm: 1.7064636945724487
Step 427 | learning_rate: 6.844444444444445e-05
Step 427 | epoch: 0.9488888888888889
Step 428 | loss: 3.2134616374969482
Step 428 | grad_norm: 2.8981995582580566
Step 428 | learning_rate: 6.837037037037037e-05
Step 428 | epoch: 0.9511111111111111
Step 429 | loss: 4.104245662689209
Step 429 | grad_norm: 3.2180449962615967
Step 429 | learning_rate: 6.829629629629631e-05
Step 429 | epoch: 0.9533333333333334
Step 430 | loss: 4.130283355712891
Step 430 | grad_norm: 2.3861443996429443
Step 430 | learning_rate: 6.822222222222222e-05
Step 430 | epoch: 0.9555555555555556
Step 431 | loss: 3.775568962097168
Step 431 | grad_norm: 2.0630033016204834
Step 431 | learning_rate: 6.814814814814815e-05
Step 431 | epoch: 0.9577777777777777
Step 432 | loss: 3.626782178878784
Step 432 | grad_norm: 2.3780131340026855
Step 432 | learning_rate: 6.807407407407408e-05
Step 432 | epoch: 0.96
Step 433 | loss: 3.320429563522339
Step 433 | grad_norm: 3.0696017742156982
Step 433 | learning_rate: 6.800000000000001e-05
Step 433 | epoch: 0.9622222222222222
Step 434 | loss: 3.4801371097564697
Step 434 | grad_norm: 2.0211873054504395
Step 434 | learning_rate: 6.792592592592592e-05
Step 434 | epoch: 0.9644444444444444
Step 435 | loss: 2.775937795639038
Step 435 | grad_norm: 2.0689327716827393
Step 435 | learning_rate: 6.785185185185186e-05
Step 435 | epoch: 0.9666666666666667
Step 436 | loss: 2.7649216651916504
Step 436 | grad_norm: 2.495598554611206
Step 436 | learning_rate: 6.777777777777778e-05
Step 436 | epoch: 0.9688888888888889
Step 437 | loss: 3.438844680786133
Step 437 | grad_norm: 2.140235424041748
Step 437 | learning_rate: 6.77037037037037e-05
Step 437 | epoch: 0.9711111111111111
Step 438 | loss: 3.3435394763946533
Step 438 | grad_norm: 1.8819518089294434
Step 438 | learning_rate: 6.762962962962963e-05
Step 438 | epoch: 0.9733333333333334
Step 439 | loss: 3.0307862758636475
Step 439 | grad_norm: 1.883254051208496
Step 439 | learning_rate: 6.755555555555557e-05
Step 439 | epoch: 0.9755555555555555
Step 440 | loss: 2.8396828174591064
Step 440 | grad_norm: 2.1557435989379883
Step 440 | learning_rate: 6.748148148148149e-05
Step 440 | epoch: 0.9777777777777777
Step 441 | loss: 3.078914165496826
Step 441 | grad_norm: 2.3000142574310303
Step 441 | learning_rate: 6.740740740740741e-05
Step 441 | epoch: 0.98
Step 442 | loss: 3.109919548034668
Step 442 | grad_norm: 2.252147912979126
Step 442 | learning_rate: 6.733333333333333e-05
Step 442 | epoch: 0.9822222222222222
Step 443 | loss: 3.841822862625122
Step 443 | grad_norm: 1.9806979894638062
Step 443 | learning_rate: 6.725925925925927e-05
Step 443 | epoch: 0.9844444444444445
Step 444 | loss: 3.663684129714966
Step 444 | grad_norm: 1.9955819845199585
Step 444 | learning_rate: 6.718518518518518e-05
Step 444 | epoch: 0.9866666666666667
Step 445 | loss: 3.113677978515625
Step 445 | grad_norm: 1.970445156097412
Step 445 | learning_rate: 6.711111111111112e-05
Step 445 | epoch: 0.9888888888888889
Step 446 | loss: 2.9683115482330322
Step 446 | grad_norm: 2.9121196269989014
Step 446 | learning_rate: 6.703703703703704e-05
Step 446 | epoch: 0.9911111111111112
Step 447 | loss: 3.550086498260498
Step 447 | grad_norm: 2.1305103302001953
Step 447 | learning_rate: 6.696296296296296e-05
Step 447 | epoch: 0.9933333333333333
Step 448 | loss: 3.857351541519165
Step 448 | grad_norm: 2.1960341930389404
Step 448 | learning_rate: 6.688888888888889e-05
Step 448 | epoch: 0.9955555555555555
Step 449 | loss: 2.913095474243164
Step 449 | grad_norm: 2.653308391571045
Step 449 | learning_rate: 6.681481481481482e-05
Step 449 | epoch: 0.9977777777777778
Step 450 | loss: 3.054981231689453
Step 450 | grad_norm: 3.3137505054473877
Step 450 | learning_rate: 6.674074074074075e-05
Step 450 | epoch: 1.0
Step 451 | loss: 4.0107316970825195
Step 451 | grad_norm: 2.5275659561157227
Step 451 | learning_rate: 6.666666666666667e-05
Step 451 | epoch: 1.0022222222222221
Step 452 | loss: 3.6570918560028076
Step 452 | grad_norm: 2.059119462966919
Step 452 | learning_rate: 6.659259259259259e-05
Step 452 | epoch: 1.0044444444444445
Step 453 | loss: 3.411243200302124
Step 453 | grad_norm: 2.5764334201812744
Step 453 | learning_rate: 6.651851851851853e-05
Step 453 | epoch: 1.0066666666666666
Step 454 | loss: 3.2110188007354736
Step 454 | grad_norm: 2.2325985431671143
Step 454 | learning_rate: 6.644444444444444e-05
Step 454 | epoch: 1.008888888888889
Step 455 | loss: 3.8659050464630127
Step 455 | grad_norm: 2.791172981262207
Step 455 | learning_rate: 6.637037037037038e-05
Step 455 | epoch: 1.011111111111111
Step 456 | loss: 3.956590175628662
Step 456 | grad_norm: 2.0985143184661865
Step 456 | learning_rate: 6.62962962962963e-05
Step 456 | epoch: 1.0133333333333334
Step 457 | loss: 3.1320104598999023
Step 457 | grad_norm: 2.105761766433716
Step 457 | learning_rate: 6.622222222222224e-05
Step 457 | epoch: 1.0155555555555555
Step 458 | loss: 3.6981711387634277
Step 458 | grad_norm: 2.4288511276245117
Step 458 | learning_rate: 6.614814814814815e-05
Step 458 | epoch: 1.0177777777777777
Step 459 | loss: 4.070937156677246
Step 459 | grad_norm: 2.498143434524536
Step 459 | learning_rate: 6.607407407407408e-05
Step 459 | epoch: 1.02
Step 460 | loss: 3.7892706394195557
Step 460 | grad_norm: 1.865700125694275
Step 460 | learning_rate: 6.6e-05
Step 460 | epoch: 1.0222222222222221
Step 461 | loss: 3.792569160461426
Step 461 | grad_norm: 2.841844081878662
Step 461 | learning_rate: 6.592592592592593e-05
Step 461 | epoch: 1.0244444444444445
Step 462 | loss: 3.333639621734619
Step 462 | grad_norm: 2.475620746612549
Step 462 | learning_rate: 6.585185185185185e-05
Step 462 | epoch: 1.0266666666666666
Step 463 | loss: 3.4885828495025635
Step 463 | grad_norm: 2.090125560760498
Step 463 | learning_rate: 6.577777777777779e-05
Step 463 | epoch: 1.028888888888889
Step 464 | loss: 3.0674009323120117
Step 464 | grad_norm: 1.8760015964508057
Step 464 | learning_rate: 6.570370370370371e-05
Step 464 | epoch: 1.031111111111111
Step 465 | loss: 4.016892433166504
Step 465 | grad_norm: 3.0416574478149414
Step 465 | learning_rate: 6.562962962962963e-05
Step 465 | epoch: 1.0333333333333334
Step 466 | loss: 3.085874080657959
Step 466 | grad_norm: 1.5809975862503052
Step 466 | learning_rate: 6.555555555555556e-05
Step 466 | epoch: 1.0355555555555556
Step 467 | loss: 4.087025165557861
Step 467 | grad_norm: 2.4338629245758057
Step 467 | learning_rate: 6.54814814814815e-05
Step 467 | epoch: 1.0377777777777777
Step 468 | loss: 3.236450433731079
Step 468 | grad_norm: 2.361729860305786
Step 468 | learning_rate: 6.54074074074074e-05
Step 468 | epoch: 1.04
Step 469 | loss: 3.419097900390625
Step 469 | grad_norm: 1.775571346282959
Step 469 | learning_rate: 6.533333333333334e-05
Step 469 | epoch: 1.0422222222222222
Step 470 | loss: 3.5980005264282227
Step 470 | grad_norm: 2.1020569801330566
Step 470 | learning_rate: 6.525925925925926e-05
Step 470 | epoch: 1.0444444444444445
Step 471 | loss: 3.1265695095062256
Step 471 | grad_norm: 1.8891297578811646
Step 471 | learning_rate: 6.51851851851852e-05
Step 471 | epoch: 1.0466666666666666
Step 472 | loss: 2.6481711864471436
Step 472 | grad_norm: 2.4230802059173584
Step 472 | learning_rate: 6.511111111111111e-05
Step 472 | epoch: 1.048888888888889
Step 473 | loss: 3.743593692779541
Step 473 | grad_norm: 2.1970274448394775
Step 473 | learning_rate: 6.503703703703705e-05
Step 473 | epoch: 1.051111111111111
Step 474 | loss: 3.6853480339050293
Step 474 | grad_norm: 2.0461411476135254
Step 474 | learning_rate: 6.496296296296297e-05
Step 474 | epoch: 1.0533333333333332
Step 475 | loss: 3.180649518966675
Step 475 | grad_norm: 2.0117626190185547
Step 475 | learning_rate: 6.488888888888889e-05
Step 475 | epoch: 1.0555555555555556
Step 476 | loss: 3.0586788654327393
Step 476 | grad_norm: 2.700974702835083
Step 476 | learning_rate: 6.481481481481482e-05
Step 476 | epoch: 1.0577777777777777
Step 477 | loss: 2.9469828605651855
Step 477 | grad_norm: 1.9956754446029663
Step 477 | learning_rate: 6.474074074074075e-05
Step 477 | epoch: 1.06
Step 478 | loss: 3.0649752616882324
Step 478 | grad_norm: 2.7745883464813232
Step 478 | learning_rate: 6.466666666666666e-05
Step 478 | epoch: 1.0622222222222222
Step 479 | loss: 3.2750561237335205
Step 479 | grad_norm: 2.2611424922943115
Step 479 | learning_rate: 6.45925925925926e-05
Step 479 | epoch: 1.0644444444444445
Step 480 | loss: 3.101642370223999
Step 480 | grad_norm: 2.4077181816101074
Step 480 | learning_rate: 6.451851851851852e-05
Step 480 | epoch: 1.0666666666666667
Step 481 | loss: 3.2525980472564697
Step 481 | grad_norm: 2.1045193672180176
Step 481 | learning_rate: 6.444444444444446e-05
Step 481 | epoch: 1.068888888888889
Step 482 | loss: 2.648531675338745
Step 482 | grad_norm: 3.2944912910461426
Step 482 | learning_rate: 6.437037037037037e-05
Step 482 | epoch: 1.0711111111111111
Step 483 | loss: 2.6800951957702637
Step 483 | grad_norm: 2.0576725006103516
Step 483 | learning_rate: 6.42962962962963e-05
Step 483 | epoch: 1.0733333333333333
Step 484 | loss: 2.9087350368499756
Step 484 | grad_norm: 2.0126376152038574
Step 484 | learning_rate: 6.422222222222223e-05
Step 484 | epoch: 1.0755555555555556
Step 485 | loss: 3.443575143814087
Step 485 | grad_norm: 2.155141830444336
Step 485 | learning_rate: 6.414814814814815e-05
Step 485 | epoch: 1.0777777777777777
Step 486 | loss: 2.5722427368164062
Step 486 | grad_norm: 2.4618048667907715
Step 486 | learning_rate: 6.407407407407407e-05
Step 486 | epoch: 1.08
Step 487 | loss: 2.759856939315796
Step 487 | grad_norm: 2.5503785610198975
Step 487 | learning_rate: 6.400000000000001e-05
Step 487 | epoch: 1.0822222222222222
Step 488 | loss: 4.09926700592041
Step 488 | grad_norm: 2.630434513092041
Step 488 | learning_rate: 6.392592592592593e-05
Step 488 | epoch: 1.0844444444444445
Step 489 | loss: 3.4386446475982666
Step 489 | grad_norm: 3.1133010387420654
Step 489 | learning_rate: 6.385185185185186e-05
Step 489 | epoch: 1.0866666666666667
Step 490 | loss: 3.268439292907715
Step 490 | grad_norm: 2.388669967651367
Step 490 | learning_rate: 6.377777777777778e-05
Step 490 | epoch: 1.0888888888888888
Step 491 | loss: 3.0996766090393066
Step 491 | grad_norm: 2.1810710430145264
Step 491 | learning_rate: 6.37037037037037e-05
Step 491 | epoch: 1.0911111111111111
Step 492 | loss: 3.5739872455596924
Step 492 | grad_norm: 2.0859568119049072
Step 492 | learning_rate: 6.362962962962963e-05
Step 492 | epoch: 1.0933333333333333
Step 493 | loss: 3.5490074157714844
Step 493 | grad_norm: 2.1719985008239746
Step 493 | learning_rate: 6.355555555555556e-05
Step 493 | epoch: 1.0955555555555556
Step 494 | loss: 4.30058479309082
Step 494 | grad_norm: 2.612252950668335
Step 494 | learning_rate: 6.348148148148149e-05
Step 494 | epoch: 1.0977777777777777
Step 495 | loss: 3.9931833744049072
Step 495 | grad_norm: 2.4276368618011475
Step 495 | learning_rate: 6.340740740740741e-05
Step 495 | epoch: 1.1
Step 496 | loss: 2.3912105560302734
Step 496 | grad_norm: 1.9607521295547485
Step 496 | learning_rate: 6.333333333333333e-05
Step 496 | epoch: 1.1022222222222222
Step 497 | loss: 3.1510019302368164
Step 497 | grad_norm: 2.4085052013397217
Step 497 | learning_rate: 6.325925925925927e-05
Step 497 | epoch: 1.1044444444444443
Step 498 | loss: 3.2555253505706787
Step 498 | grad_norm: 1.7504603862762451
Step 498 | learning_rate: 6.318518518518519e-05
Step 498 | epoch: 1.1066666666666667
Step 499 | loss: 3.0479817390441895
Step 499 | grad_norm: 2.2169392108917236
Step 499 | learning_rate: 6.311111111111112e-05
Step 499 | epoch: 1.1088888888888888
Step 500 | loss: 3.780233144760132
Step 500 | grad_norm: 2.5691375732421875
Step 500 | learning_rate: 6.303703703703704e-05
Step 500 | epoch: 1.1111111111111112
Step 501 | loss: 3.630101442337036
Step 501 | grad_norm: 2.9220268726348877
Step 501 | learning_rate: 6.296296296296296e-05
Step 501 | epoch: 1.1133333333333333
Step 502 | loss: 2.99424409866333
Step 502 | grad_norm: 2.0082991123199463
Step 502 | learning_rate: 6.28888888888889e-05
Step 502 | epoch: 1.1155555555555556
Step 503 | loss: 2.9435696601867676
Step 503 | grad_norm: 1.9448916912078857
Step 503 | learning_rate: 6.281481481481482e-05
Step 503 | epoch: 1.1177777777777778
Step 504 | loss: 3.6817104816436768
Step 504 | grad_norm: 2.7769384384155273
Step 504 | learning_rate: 6.274074074074074e-05
Step 504 | epoch: 1.12
Step 505 | loss: 3.507983922958374
Step 505 | grad_norm: 2.4615559577941895
Step 505 | learning_rate: 6.266666666666667e-05
Step 505 | epoch: 1.1222222222222222
Step 506 | loss: 3.6507649421691895
Step 506 | grad_norm: 3.179392099380493
Step 506 | learning_rate: 6.259259259259259e-05
Step 506 | epoch: 1.1244444444444444
Step 507 | loss: 3.7126941680908203
Step 507 | grad_norm: 3.143566131591797
Step 507 | learning_rate: 6.251851851851853e-05
Step 507 | epoch: 1.1266666666666667
Step 508 | loss: 3.2579290866851807
Step 508 | grad_norm: 2.3653757572174072
Step 508 | learning_rate: 6.244444444444445e-05
Step 508 | epoch: 1.1288888888888888
Step 509 | loss: 3.6664628982543945
Step 509 | grad_norm: 2.8774356842041016
Step 509 | learning_rate: 6.237037037037037e-05
Step 509 | epoch: 1.1311111111111112
Step 510 | loss: 3.8551900386810303
Step 510 | grad_norm: 4.306842803955078
Step 510 | learning_rate: 6.22962962962963e-05
Step 510 | epoch: 1.1333333333333333
Step 511 | loss: 2.64628267288208
Step 511 | grad_norm: 2.2532947063446045
Step 511 | learning_rate: 6.222222222222222e-05
Step 511 | epoch: 1.1355555555555557
Step 512 | loss: 4.597231388092041
Step 512 | grad_norm: 3.636782169342041
Step 512 | learning_rate: 6.214814814814816e-05
Step 512 | epoch: 1.1377777777777778
Step 513 | loss: 3.7902681827545166
Step 513 | grad_norm: 2.1677932739257812
Step 513 | learning_rate: 6.207407407407408e-05
Step 513 | epoch: 1.1400000000000001
Step 514 | loss: 2.9107229709625244
Step 514 | grad_norm: 2.047549247741699
Step 514 | learning_rate: 6.2e-05
Step 514 | epoch: 1.1422222222222222
Step 515 | loss: 3.658879518508911
Step 515 | grad_norm: 2.440647840499878
Step 515 | learning_rate: 6.192592592592593e-05
Step 515 | epoch: 1.1444444444444444
Step 516 | loss: 3.703097343444824
Step 516 | grad_norm: 2.999035358428955
Step 516 | learning_rate: 6.185185185185185e-05
Step 516 | epoch: 1.1466666666666667
Step 517 | loss: 3.368577241897583
Step 517 | grad_norm: 1.792091965675354
Step 517 | learning_rate: 6.177777777777779e-05
Step 517 | epoch: 1.1488888888888888
Step 518 | loss: 3.255213499069214
Step 518 | grad_norm: 2.2486660480499268
Step 518 | learning_rate: 6.170370370370371e-05
Step 518 | epoch: 1.1511111111111112
Step 519 | loss: 3.9075815677642822
Step 519 | grad_norm: 2.633392572402954
Step 519 | learning_rate: 6.162962962962963e-05
Step 519 | epoch: 1.1533333333333333
Step 520 | loss: 3.0099470615386963
Step 520 | grad_norm: 2.6319148540496826
Step 520 | learning_rate: 6.155555555555555e-05
Step 520 | epoch: 1.1555555555555554
Step 521 | loss: 3.6647322177886963
Step 521 | grad_norm: 2.068446397781372
Step 521 | learning_rate: 6.148148148148148e-05
Step 521 | epoch: 1.1577777777777778
Step 522 | loss: 3.038658618927002
Step 522 | grad_norm: 1.9787169694900513
Step 522 | learning_rate: 6.140740740740741e-05
Step 522 | epoch: 1.16
Step 523 | loss: 3.4113852977752686
Step 523 | grad_norm: 2.497887134552002
Step 523 | learning_rate: 6.133333333333334e-05
Step 523 | epoch: 1.1622222222222223
Step 524 | loss: 3.358579397201538
Step 524 | grad_norm: 1.9159938097000122
Step 524 | learning_rate: 6.125925925925926e-05
Step 524 | epoch: 1.1644444444444444
Step 525 | loss: 3.413177490234375
Step 525 | grad_norm: 2.6377909183502197
Step 525 | learning_rate: 6.118518518518518e-05
Step 525 | epoch: 1.1666666666666667
Step 526 | loss: 3.602238655090332
Step 526 | grad_norm: 2.78651762008667
Step 526 | learning_rate: 6.111111111111112e-05
Step 526 | epoch: 1.1688888888888889
Step 527 | loss: 2.7897324562072754
Step 527 | grad_norm: 2.4275434017181396
Step 527 | learning_rate: 6.103703703703703e-05
Step 527 | epoch: 1.1711111111111112
Step 528 | loss: 2.835984706878662
Step 528 | grad_norm: 2.686108112335205
Step 528 | learning_rate: 6.096296296296297e-05
Step 528 | epoch: 1.1733333333333333
Step 529 | loss: 2.9976248741149902
Step 529 | grad_norm: 2.2749288082122803
Step 529 | learning_rate: 6.08888888888889e-05
Step 529 | epoch: 1.1755555555555555
Step 530 | loss: 2.8706061840057373
Step 530 | grad_norm: 1.8459359407424927
Step 530 | learning_rate: 6.081481481481481e-05
Step 530 | epoch: 1.1777777777777778
Step 531 | loss: 3.373908758163452
Step 531 | grad_norm: 2.2893996238708496
Step 531 | learning_rate: 6.074074074074074e-05
Step 531 | epoch: 1.18
Step 532 | loss: 2.757098436355591
Step 532 | grad_norm: 1.8219131231307983
Step 532 | learning_rate: 6.066666666666667e-05
Step 532 | epoch: 1.1822222222222223
Step 533 | loss: 3.781308650970459
Step 533 | grad_norm: 2.1399147510528564
Step 533 | learning_rate: 6.05925925925926e-05
Step 533 | epoch: 1.1844444444444444
Step 534 | loss: 3.5551865100860596
Step 534 | grad_norm: 2.343801498413086
Step 534 | learning_rate: 6.051851851851852e-05
Step 534 | epoch: 1.1866666666666668
Step 535 | loss: 3.861067295074463
Step 535 | grad_norm: 2.5887317657470703
Step 535 | learning_rate: 6.044444444444445e-05
Step 535 | epoch: 1.1888888888888889
Step 536 | loss: 3.251588821411133
Step 536 | grad_norm: 2.2620363235473633
Step 536 | learning_rate: 6.037037037037038e-05
Step 536 | epoch: 1.1911111111111112
Step 537 | loss: 3.8503987789154053
Step 537 | grad_norm: 2.2308998107910156
Step 537 | learning_rate: 6.0296296296296295e-05
Step 537 | epoch: 1.1933333333333334
Step 538 | loss: 3.8862950801849365
Step 538 | grad_norm: 2.8276093006134033
Step 538 | learning_rate: 6.0222222222222225e-05
Step 538 | epoch: 1.1955555555555555
Step 539 | loss: 3.0425448417663574
Step 539 | grad_norm: 2.6979727745056152
Step 539 | learning_rate: 6.0148148148148155e-05
Step 539 | epoch: 1.1977777777777778
Step 540 | loss: 3.0865190029144287
Step 540 | grad_norm: 2.3553974628448486
Step 540 | learning_rate: 6.007407407407407e-05
Step 540 | epoch: 1.2
Step 541 | loss: 4.138092517852783
Step 541 | grad_norm: 2.34613299369812
Step 541 | learning_rate: 6e-05
Step 541 | epoch: 1.2022222222222223
Step 542 | loss: 3.0345335006713867
Step 542 | grad_norm: 1.6559197902679443
Step 542 | learning_rate: 5.992592592592593e-05
Step 542 | epoch: 1.2044444444444444
Step 543 | loss: 3.076447010040283
Step 543 | grad_norm: 2.67113995552063
Step 543 | learning_rate: 5.985185185185186e-05
Step 543 | epoch: 1.2066666666666666
Step 544 | loss: 3.1834795475006104
Step 544 | grad_norm: 2.3759708404541016
Step 544 | learning_rate: 5.977777777777778e-05
Step 544 | epoch: 1.208888888888889
Step 545 | loss: 3.807847261428833
Step 545 | grad_norm: 2.004539966583252
Step 545 | learning_rate: 5.970370370370371e-05
Step 545 | epoch: 1.211111111111111
Step 546 | loss: 3.364586114883423
Step 546 | grad_norm: 2.263667345046997
Step 546 | learning_rate: 5.962962962962964e-05
Step 546 | epoch: 1.2133333333333334
Step 547 | loss: 3.821028470993042
Step 547 | grad_norm: 3.2470438480377197
Step 547 | learning_rate: 5.9555555555555554e-05
Step 547 | epoch: 1.2155555555555555
Step 548 | loss: 2.7627511024475098
Step 548 | grad_norm: 1.8058228492736816
Step 548 | learning_rate: 5.9481481481481484e-05
Step 548 | epoch: 1.2177777777777778
Step 549 | loss: 3.483229875564575
Step 549 | grad_norm: 1.984397530555725
Step 549 | learning_rate: 5.9407407407407414e-05
Step 549 | epoch: 1.22
Step 550 | loss: 3.862734794616699
Step 550 | grad_norm: 2.247438430786133
Step 550 | learning_rate: 5.9333333333333343e-05
Step 550 | epoch: 1.2222222222222223
Step 551 | loss: 3.7978971004486084
Step 551 | grad_norm: 2.068528413772583
Step 551 | learning_rate: 5.925925925925926e-05
Step 551 | epoch: 1.2244444444444444
Step 552 | loss: 2.936042547225952
Step 552 | grad_norm: 3.1754722595214844
Step 552 | learning_rate: 5.918518518518519e-05
Step 552 | epoch: 1.2266666666666666
Step 553 | loss: 3.4495739936828613
Step 553 | grad_norm: 2.1876018047332764
Step 553 | learning_rate: 5.911111111111112e-05
Step 553 | epoch: 1.228888888888889
Step 554 | loss: 3.8249900341033936
Step 554 | grad_norm: 2.1401174068450928
Step 554 | learning_rate: 5.9037037037037036e-05
Step 554 | epoch: 1.231111111111111
Step 555 | loss: 4.0388689041137695
Step 555 | grad_norm: 3.3137876987457275
Step 555 | learning_rate: 5.8962962962962966e-05
Step 555 | epoch: 1.2333333333333334
Step 556 | loss: 4.640064239501953
Step 556 | grad_norm: 3.05735445022583
Step 556 | learning_rate: 5.8888888888888896e-05
Step 556 | epoch: 1.2355555555555555
Step 557 | loss: 3.7822654247283936
Step 557 | grad_norm: 2.2304258346557617
Step 557 | learning_rate: 5.8814814814814826e-05
Step 557 | epoch: 1.2377777777777779
Step 558 | loss: 3.5819270610809326
Step 558 | grad_norm: 3.2968931198120117
Step 558 | learning_rate: 5.874074074074074e-05
Step 558 | epoch: 1.24
Step 559 | loss: 3.709644317626953
Step 559 | grad_norm: 2.8113491535186768
Step 559 | learning_rate: 5.866666666666667e-05
Step 559 | epoch: 1.2422222222222223
Step 560 | loss: 3.2848870754241943
Step 560 | grad_norm: 2.8448970317840576
Step 560 | learning_rate: 5.85925925925926e-05
Step 560 | epoch: 1.2444444444444445
Step 561 | loss: 3.8163139820098877
Step 561 | grad_norm: 2.3097281455993652
Step 561 | learning_rate: 5.851851851851852e-05
Step 561 | epoch: 1.2466666666666666
Step 562 | loss: 3.7026281356811523
Step 562 | grad_norm: 2.0842490196228027
Step 562 | learning_rate: 5.844444444444445e-05
Step 562 | epoch: 1.248888888888889
Step 563 | loss: 3.706110715866089
Step 563 | grad_norm: 2.388108253479004
Step 563 | learning_rate: 5.837037037037038e-05
Step 563 | epoch: 1.251111111111111
Step 564 | loss: 3.826763868331909
Step 564 | grad_norm: 2.7208948135375977
Step 564 | learning_rate: 5.82962962962963e-05
Step 564 | epoch: 1.2533333333333334
Step 565 | loss: 4.185246467590332
Step 565 | grad_norm: 2.195927381515503
Step 565 | learning_rate: 5.8222222222222224e-05
Step 565 | epoch: 1.2555555555555555
Step 566 | loss: 2.8872740268707275
Step 566 | grad_norm: 1.9365770816802979
Step 566 | learning_rate: 5.8148148148148154e-05
Step 566 | epoch: 1.2577777777777777
Step 567 | loss: 3.414806365966797
Step 567 | grad_norm: 3.328512191772461
Step 567 | learning_rate: 5.8074074074074084e-05
Step 567 | epoch: 1.26
Step 568 | loss: 4.239681243896484
Step 568 | grad_norm: 2.3889620304107666
Step 568 | learning_rate: 5.8e-05
Step 568 | epoch: 1.2622222222222224
Step 569 | loss: 3.3629062175750732
Step 569 | grad_norm: 2.3630053997039795
Step 569 | learning_rate: 5.792592592592593e-05
Step 569 | epoch: 1.2644444444444445
Step 570 | loss: 3.7557990550994873
Step 570 | grad_norm: 2.6582560539245605
Step 570 | learning_rate: 5.785185185185186e-05
Step 570 | epoch: 1.2666666666666666
Step 571 | loss: 5.687858581542969
Step 571 | grad_norm: 2.908825635910034
Step 571 | learning_rate: 5.7777777777777776e-05
Step 571 | epoch: 1.268888888888889
Step 572 | loss: 4.266557216644287
Step 572 | grad_norm: 2.9808452129364014
Step 572 | learning_rate: 5.7703703703703706e-05
Step 572 | epoch: 1.271111111111111
Step 573 | loss: 3.0599684715270996
Step 573 | grad_norm: 1.9744623899459839
Step 573 | learning_rate: 5.7629629629629636e-05
Step 573 | epoch: 1.2733333333333334
Step 574 | loss: 3.921663284301758
Step 574 | grad_norm: 2.4227025508880615
Step 574 | learning_rate: 5.755555555555556e-05
Step 574 | epoch: 1.2755555555555556
Step 575 | loss: 2.897278308868408
Step 575 | grad_norm: 1.7709639072418213
Step 575 | learning_rate: 5.748148148148148e-05
Step 575 | epoch: 1.2777777777777777
Step 576 | loss: 3.3551902770996094
Step 576 | grad_norm: 1.8275338411331177
Step 576 | learning_rate: 5.740740740740741e-05
Step 576 | epoch: 1.28
Step 577 | loss: 3.554664373397827
Step 577 | grad_norm: 2.983213424682617
Step 577 | learning_rate: 5.7333333333333336e-05
Step 577 | epoch: 1.2822222222222222
Step 578 | loss: 3.1762115955352783
Step 578 | grad_norm: 2.002798557281494
Step 578 | learning_rate: 5.725925925925926e-05
Step 578 | epoch: 1.2844444444444445
Step 579 | loss: 3.94364070892334
Step 579 | grad_norm: 2.0481839179992676
Step 579 | learning_rate: 5.718518518518519e-05
Step 579 | epoch: 1.2866666666666666
Step 580 | loss: 3.061850070953369
Step 580 | grad_norm: 2.0610015392303467
Step 580 | learning_rate: 5.711111111111112e-05
Step 580 | epoch: 1.2888888888888888
Step 581 | loss: 3.7209126949310303
Step 581 | grad_norm: 1.9903528690338135
Step 581 | learning_rate: 5.703703703703704e-05
Step 581 | epoch: 1.291111111111111
Step 582 | loss: 4.061621189117432
Step 582 | grad_norm: 2.3046445846557617
Step 582 | learning_rate: 5.6962962962962965e-05
Step 582 | epoch: 1.2933333333333334
Step 583 | loss: 2.7601447105407715
Step 583 | grad_norm: 2.077749013900757
Step 583 | learning_rate: 5.6888888888888895e-05
Step 583 | epoch: 1.2955555555555556
Step 584 | loss: 3.389514923095703
Step 584 | grad_norm: 2.4212608337402344
Step 584 | learning_rate: 5.681481481481482e-05
Step 584 | epoch: 1.2977777777777777
Step 585 | loss: 2.5868313312530518
Step 585 | grad_norm: 3.09031343460083
Step 585 | learning_rate: 5.674074074074074e-05
Step 585 | epoch: 1.3
Step 586 | loss: 3.5054128170013428
Step 586 | grad_norm: 1.9685519933700562
Step 586 | learning_rate: 5.666666666666667e-05
Step 586 | epoch: 1.3022222222222222
Step 587 | loss: 2.7253339290618896
Step 587 | grad_norm: 1.6096365451812744
Step 587 | learning_rate: 5.6592592592592594e-05
Step 587 | epoch: 1.3044444444444445
Step 588 | loss: 3.30375599861145
Step 588 | grad_norm: 2.3083579540252686
Step 588 | learning_rate: 5.6518518518518524e-05
Step 588 | epoch: 1.3066666666666666
Step 589 | loss: 2.6892459392547607
Step 589 | grad_norm: 2.512532949447632
Step 589 | learning_rate: 5.644444444444445e-05
Step 589 | epoch: 1.3088888888888888
Step 590 | loss: 3.0538218021392822
Step 590 | grad_norm: 2.3627917766571045
Step 590 | learning_rate: 5.637037037037037e-05
Step 590 | epoch: 1.3111111111111111
Step 591 | loss: 3.9003207683563232
Step 591 | grad_norm: 2.413395881652832
Step 591 | learning_rate: 5.62962962962963e-05
Step 591 | epoch: 1.3133333333333335
Step 592 | loss: 3.598745346069336
Step 592 | grad_norm: 2.318809986114502
Step 592 | learning_rate: 5.622222222222222e-05
Step 592 | epoch: 1.3155555555555556
Step 593 | loss: 2.863921642303467
Step 593 | grad_norm: 2.4233813285827637
Step 593 | learning_rate: 5.614814814814815e-05
Step 593 | epoch: 1.3177777777777777
Step 594 | loss: 4.378608703613281
Step 594 | grad_norm: 2.3731205463409424
Step 594 | learning_rate: 5.6074074074074076e-05
Step 594 | epoch: 1.32
Step 595 | loss: 3.5757031440734863
Step 595 | grad_norm: 1.7227864265441895
Step 595 | learning_rate: 5.6000000000000006e-05
Step 595 | epoch: 1.3222222222222222
Step 596 | loss: 4.384768962860107
Step 596 | grad_norm: 2.9654853343963623
Step 596 | learning_rate: 5.592592592592593e-05
Step 596 | epoch: 1.3244444444444445
Step 597 | loss: 2.9150519371032715
Step 597 | grad_norm: 3.343383550643921
Step 597 | learning_rate: 5.585185185185185e-05
Step 597 | epoch: 1.3266666666666667
Step 598 | loss: 2.760559320449829
Step 598 | grad_norm: 1.826134443283081
Step 598 | learning_rate: 5.577777777777778e-05
Step 598 | epoch: 1.3288888888888888
Step 599 | loss: 3.3567774295806885
Step 599 | grad_norm: 2.3634719848632812
Step 599 | learning_rate: 5.5703703703703705e-05
Step 599 | epoch: 1.3311111111111111
Step 600 | loss: 2.9843244552612305
Step 600 | grad_norm: 2.1206576824188232
Step 600 | learning_rate: 5.562962962962963e-05
Step 600 | epoch: 1.3333333333333333
Step 601 | loss: 3.4881014823913574
Step 601 | grad_norm: 2.7805421352386475
Step 601 | learning_rate: 5.555555555555556e-05
Step 601 | epoch: 1.3355555555555556
Step 602 | loss: 2.883023738861084
Step 602 | grad_norm: 2.0163350105285645
Step 602 | learning_rate: 5.548148148148148e-05
Step 602 | epoch: 1.3377777777777777
Step 603 | loss: 3.0364186763763428
Step 603 | grad_norm: 2.0919435024261475
Step 603 | learning_rate: 5.540740740740741e-05
Step 603 | epoch: 1.34
Step 604 | loss: 3.231233596801758
Step 604 | grad_norm: 2.5022201538085938
Step 604 | learning_rate: 5.5333333333333334e-05
Step 604 | epoch: 1.3422222222222222
Step 605 | loss: 2.940269947052002
Step 605 | grad_norm: 2.159329414367676
Step 605 | learning_rate: 5.5259259259259264e-05
Step 605 | epoch: 1.3444444444444446
Step 606 | loss: 3.732637405395508
Step 606 | grad_norm: 2.194566488265991
Step 606 | learning_rate: 5.518518518518519e-05
Step 606 | epoch: 1.3466666666666667
Step 607 | loss: 4.243546485900879
Step 607 | grad_norm: 2.7980220317840576
Step 607 | learning_rate: 5.511111111111111e-05
Step 607 | epoch: 1.3488888888888888
Step 608 | loss: 4.029306888580322
Step 608 | grad_norm: 2.3168489933013916
Step 608 | learning_rate: 5.503703703703704e-05
Step 608 | epoch: 1.3511111111111112
Step 609 | loss: 2.6768171787261963
Step 609 | grad_norm: 2.1774017810821533
Step 609 | learning_rate: 5.4962962962962964e-05
Step 609 | epoch: 1.3533333333333333
Step 610 | loss: 3.2649576663970947
Step 610 | grad_norm: 2.360485553741455
Step 610 | learning_rate: 5.488888888888889e-05
Step 610 | epoch: 1.3555555555555556
Step 611 | loss: 3.8845624923706055
Step 611 | grad_norm: 2.5184900760650635
Step 611 | learning_rate: 5.4814814814814817e-05
Step 611 | epoch: 1.3577777777777778
Step 612 | loss: 3.0630784034729004
Step 612 | grad_norm: 2.030277967453003
Step 612 | learning_rate: 5.4740740740740746e-05
Step 612 | epoch: 1.3599999999999999
Step 613 | loss: 3.5498454570770264
Step 613 | grad_norm: 2.06327486038208
Step 613 | learning_rate: 5.466666666666666e-05
Step 613 | epoch: 1.3622222222222222
Step 614 | loss: 3.6349408626556396
Step 614 | grad_norm: 2.272432804107666
Step 614 | learning_rate: 5.459259259259259e-05
Step 614 | epoch: 1.3644444444444446
Step 615 | loss: 3.61354398727417
Step 615 | grad_norm: 2.6081507205963135
Step 615 | learning_rate: 5.451851851851852e-05
Step 615 | epoch: 1.3666666666666667
Step 616 | loss: 3.4103100299835205
Step 616 | grad_norm: 2.2046127319335938
Step 616 | learning_rate: 5.4444444444444446e-05
Step 616 | epoch: 1.3688888888888888
Step 617 | loss: 4.041485786437988
Step 617 | grad_norm: 2.4840264320373535
Step 617 | learning_rate: 5.437037037037037e-05
Step 617 | epoch: 1.3711111111111112
Step 618 | loss: 3.3363289833068848
Step 618 | grad_norm: 1.8095515966415405
Step 618 | learning_rate: 5.42962962962963e-05
Step 618 | epoch: 1.3733333333333333
Step 619 | loss: 4.348216533660889
Step 619 | grad_norm: 2.637078285217285
Step 619 | learning_rate: 5.422222222222223e-05
Step 619 | epoch: 1.3755555555555556
Step 620 | loss: 3.1658999919891357
Step 620 | grad_norm: 1.9815622568130493
Step 620 | learning_rate: 5.4148148148148145e-05
Step 620 | epoch: 1.3777777777777778
Step 621 | loss: 3.7552120685577393
Step 621 | grad_norm: 2.4763643741607666
Step 621 | learning_rate: 5.4074074074074075e-05
Step 621 | epoch: 1.38
Step 622 | loss: 3.127448081970215
Step 622 | grad_norm: 2.5593342781066895
Step 622 | learning_rate: 5.4000000000000005e-05
Step 622 | epoch: 1.3822222222222222
Step 623 | loss: 2.957799196243286
Step 623 | grad_norm: 1.9217543601989746
Step 623 | learning_rate: 5.392592592592592e-05
Step 623 | epoch: 1.3844444444444444
Step 624 | loss: 3.496295690536499
Step 624 | grad_norm: 3.9266903400421143
Step 624 | learning_rate: 5.385185185185185e-05
Step 624 | epoch: 1.3866666666666667
Step 625 | loss: 3.9253735542297363
Step 625 | grad_norm: 1.9625762701034546
Step 625 | learning_rate: 5.377777777777778e-05
Step 625 | epoch: 1.3888888888888888
Step 626 | loss: 3.4303689002990723
Step 626 | grad_norm: 2.7531797885894775
Step 626 | learning_rate: 5.370370370370371e-05
Step 626 | epoch: 1.3911111111111112
Step 627 | loss: 3.9877984523773193
Step 627 | grad_norm: 2.12495493888855
Step 627 | learning_rate: 5.362962962962963e-05
Step 627 | epoch: 1.3933333333333333
Step 628 | loss: 4.055935382843018
Step 628 | grad_norm: 2.5447239875793457
Step 628 | learning_rate: 5.355555555555556e-05
Step 628 | epoch: 1.3955555555555557
Step 629 | loss: 3.3339710235595703
Step 629 | grad_norm: 2.403289556503296
Step 629 | learning_rate: 5.348148148148149e-05
Step 629 | epoch: 1.3977777777777778
Step 630 | loss: 2.7898645401000977
Step 630 | grad_norm: 1.935184121131897
Step 630 | learning_rate: 5.34074074074074e-05
Step 630 | epoch: 1.4
Step 631 | loss: 3.744948625564575
Step 631 | grad_norm: 2.1828744411468506
Step 631 | learning_rate: 5.333333333333333e-05
Step 631 | epoch: 1.4022222222222223
Step 632 | loss: 3.791440486907959
Step 632 | grad_norm: 2.8119986057281494
Step 632 | learning_rate: 5.325925925925926e-05
Step 632 | epoch: 1.4044444444444444
Step 633 | loss: 3.167572021484375
Step 633 | grad_norm: 2.3454902172088623
Step 633 | learning_rate: 5.318518518518518e-05
Step 633 | epoch: 1.4066666666666667
Step 634 | loss: 3.329369068145752
Step 634 | grad_norm: 1.9791302680969238
Step 634 | learning_rate: 5.311111111111111e-05
Step 634 | epoch: 1.4088888888888889
Step 635 | loss: 3.7119266986846924
Step 635 | grad_norm: 2.4982829093933105
Step 635 | learning_rate: 5.303703703703704e-05
Step 635 | epoch: 1.411111111111111
Step 636 | loss: 3.143692970275879
Step 636 | grad_norm: 3.953606128692627
Step 636 | learning_rate: 5.296296296296297e-05
Step 636 | epoch: 1.4133333333333333
Step 637 | loss: 3.8820536136627197
Step 637 | grad_norm: 2.8754706382751465
Step 637 | learning_rate: 5.2888888888888885e-05
Step 637 | epoch: 1.4155555555555557
Step 638 | loss: 2.6111791133880615
Step 638 | grad_norm: 2.603543996810913
Step 638 | learning_rate: 5.2814814814814815e-05
Step 638 | epoch: 1.4177777777777778
Step 639 | loss: 2.9885144233703613
Step 639 | grad_norm: 1.9308569431304932
Step 639 | learning_rate: 5.2740740740740745e-05
Step 639 | epoch: 1.42
Step 640 | loss: 3.0905184745788574
Step 640 | grad_norm: 2.848747968673706
Step 640 | learning_rate: 5.266666666666666e-05
Step 640 | epoch: 1.4222222222222223
Step 641 | loss: 3.5025553703308105
Step 641 | grad_norm: 2.352382183074951
Step 641 | learning_rate: 5.259259259259259e-05
Step 641 | epoch: 1.4244444444444444
Step 642 | loss: 3.6778452396392822
Step 642 | grad_norm: 2.8896045684814453
Step 642 | learning_rate: 5.251851851851852e-05
Step 642 | epoch: 1.4266666666666667
Step 643 | loss: 3.8283660411834717
Step 643 | grad_norm: 3.6788153648376465
Step 643 | learning_rate: 5.244444444444445e-05
Step 643 | epoch: 1.4288888888888889
Step 644 | loss: 2.885005474090576
Step 644 | grad_norm: 1.6579017639160156
Step 644 | learning_rate: 5.237037037037037e-05
Step 644 | epoch: 1.431111111111111
Step 645 | loss: 3.0333199501037598
Step 645 | grad_norm: 1.9112496376037598
Step 645 | learning_rate: 5.22962962962963e-05
Step 645 | epoch: 1.4333333333333333
Step 646 | loss: 3.8385159969329834
Step 646 | grad_norm: 2.171938896179199
Step 646 | learning_rate: 5.222222222222223e-05
Step 646 | epoch: 1.4355555555555555
Step 647 | loss: 3.6302080154418945
Step 647 | grad_norm: 2.5343453884124756
Step 647 | learning_rate: 5.2148148148148144e-05
Step 647 | epoch: 1.4377777777777778
Step 648 | loss: 3.512864351272583
Step 648 | grad_norm: 2.240692615509033
Step 648 | learning_rate: 5.2074074074074074e-05
Step 648 | epoch: 1.44
Step 649 | loss: 3.7039358615875244
Step 649 | grad_norm: 2.5770671367645264
Step 649 | learning_rate: 5.2000000000000004e-05
Step 649 | epoch: 1.4422222222222223
Step 650 | loss: 3.339261531829834
Step 650 | grad_norm: 2.3373279571533203
Step 650 | learning_rate: 5.1925925925925933e-05
Step 650 | epoch: 1.4444444444444444
Step 651 | loss: 3.062169075012207
Step 651 | grad_norm: 2.3941338062286377
Step 651 | learning_rate: 5.185185185185185e-05
Step 651 | epoch: 1.4466666666666668
Step 652 | loss: 2.901801824569702
Step 652 | grad_norm: 2.3557369709014893
Step 652 | learning_rate: 5.177777777777778e-05
Step 652 | epoch: 1.448888888888889
Step 653 | loss: 3.7958896160125732
Step 653 | grad_norm: 3.202690839767456
Step 653 | learning_rate: 5.170370370370371e-05
Step 653 | epoch: 1.451111111111111
Step 654 | loss: 3.466478109359741
Step 654 | grad_norm: 2.3338146209716797
Step 654 | learning_rate: 5.1629629629629626e-05
Step 654 | epoch: 1.4533333333333334
Step 655 | loss: 3.2651069164276123
Step 655 | grad_norm: 2.860818862915039
Step 655 | learning_rate: 5.1555555555555556e-05
Step 655 | epoch: 1.4555555555555555
Step 656 | loss: 3.328096866607666
Step 656 | grad_norm: 2.21364426612854
Step 656 | learning_rate: 5.1481481481481486e-05
Step 656 | epoch: 1.4577777777777778
Step 657 | loss: 3.6034252643585205
Step 657 | grad_norm: 2.196993112564087
Step 657 | learning_rate: 5.1407407407407416e-05
Step 657 | epoch: 1.46
Step 658 | loss: 3.969557762145996
Step 658 | grad_norm: 2.3954293727874756
Step 658 | learning_rate: 5.133333333333333e-05
Step 658 | epoch: 1.462222222222222
Step 659 | loss: 3.264624834060669
Step 659 | grad_norm: 2.0910286903381348
Step 659 | learning_rate: 5.125925925925926e-05
Step 659 | epoch: 1.4644444444444444
Step 660 | loss: 3.09629487991333
Step 660 | grad_norm: 2.2483153343200684
Step 660 | learning_rate: 5.118518518518519e-05
Step 660 | epoch: 1.4666666666666668
Step 661 | loss: 2.7044477462768555
Step 661 | grad_norm: 1.8333932161331177
Step 661 | learning_rate: 5.111111111111111e-05
Step 661 | epoch: 1.468888888888889
Step 662 | loss: 3.2505147457122803
Step 662 | grad_norm: 3.967200994491577
Step 662 | learning_rate: 5.103703703703704e-05
Step 662 | epoch: 1.471111111111111
Step 663 | loss: 3.422078847885132
Step 663 | grad_norm: 2.2735488414764404
Step 663 | learning_rate: 5.096296296296297e-05
Step 663 | epoch: 1.4733333333333334
Step 664 | loss: 3.5179643630981445
Step 664 | grad_norm: 3.5506441593170166
Step 664 | learning_rate: 5.0888888888888884e-05
Step 664 | epoch: 1.4755555555555555
Step 665 | loss: 2.9772231578826904
Step 665 | grad_norm: 3.7959320545196533
Step 665 | learning_rate: 5.0814814814814814e-05
Step 665 | epoch: 1.4777777777777779
Step 666 | loss: 2.591555118560791
Step 666 | grad_norm: 2.1752476692199707
Step 666 | learning_rate: 5.0740740740740744e-05
Step 666 | epoch: 1.48
Step 667 | loss: 4.100686073303223
Step 667 | grad_norm: 2.8186347484588623
Step 667 | learning_rate: 5.0666666666666674e-05
Step 667 | epoch: 1.482222222222222
Step 668 | loss: 3.16996693611145
Step 668 | grad_norm: 2.5134079456329346
Step 668 | learning_rate: 5.059259259259259e-05
Step 668 | epoch: 1.4844444444444445
Step 669 | loss: 3.568263530731201
Step 669 | grad_norm: 2.3936688899993896
Step 669 | learning_rate: 5.051851851851852e-05
Step 669 | epoch: 1.4866666666666668
Step 670 | loss: 3.043126344680786
Step 670 | grad_norm: 2.334157705307007
Step 670 | learning_rate: 5.044444444444445e-05
Step 670 | epoch: 1.488888888888889
Step 671 | loss: 3.785709857940674
Step 671 | grad_norm: 3.1451187133789062
Step 671 | learning_rate: 5.0370370370370366e-05
Step 671 | epoch: 1.491111111111111
Step 672 | loss: 3.590881586074829
Step 672 | grad_norm: 2.2344064712524414
Step 672 | learning_rate: 5.0296296296296296e-05
Step 672 | epoch: 1.4933333333333334
Step 673 | loss: 3.555342435836792
Step 673 | grad_norm: 2.500147819519043
Step 673 | learning_rate: 5.0222222222222226e-05
Step 673 | epoch: 1.4955555555555555
Step 674 | loss: 3.4400551319122314
Step 674 | grad_norm: 2.2438714504241943
Step 674 | learning_rate: 5.0148148148148156e-05
Step 674 | epoch: 1.4977777777777779
Step 675 | loss: 3.929408311843872
Step 675 | grad_norm: 2.305243492126465
Step 675 | learning_rate: 5.007407407407407e-05
Step 675 | epoch: 1.5
Step 676 | loss: 2.8771586418151855
Step 676 | grad_norm: 2.0032527446746826
Step 676 | learning_rate: 5e-05
Step 676 | epoch: 1.5022222222222221
Step 677 | loss: 4.2286577224731445
Step 677 | grad_norm: 2.803605556488037
Step 677 | learning_rate: 4.9925925925925926e-05
Step 677 | epoch: 1.5044444444444445
Step 678 | loss: 2.839614152908325
Step 678 | grad_norm: 1.8812613487243652
Step 678 | learning_rate: 4.9851851851851855e-05
Step 678 | epoch: 1.5066666666666668
Step 679 | loss: 3.624783754348755
Step 679 | grad_norm: 2.527097702026367
Step 679 | learning_rate: 4.977777777777778e-05
Step 679 | epoch: 1.508888888888889
Step 680 | loss: 2.7987356185913086
Step 680 | grad_norm: 2.091935157775879
Step 680 | learning_rate: 4.970370370370371e-05
Step 680 | epoch: 1.511111111111111
Step 681 | loss: 3.4030559062957764
Step 681 | grad_norm: 1.9107857942581177
Step 681 | learning_rate: 4.962962962962963e-05
Step 681 | epoch: 1.5133333333333332
Step 682 | loss: 3.0144236087799072
Step 682 | grad_norm: 2.000926971435547
Step 682 | learning_rate: 4.955555555555556e-05
Step 682 | epoch: 1.5155555555555555
Step 683 | loss: 3.501107692718506
Step 683 | grad_norm: 2.210942268371582
Step 683 | learning_rate: 4.9481481481481485e-05
Step 683 | epoch: 1.517777777777778
Step 684 | loss: 2.976536512374878
Step 684 | grad_norm: 2.942850112915039
Step 684 | learning_rate: 4.940740740740741e-05
Step 684 | epoch: 1.52
Step 685 | loss: 3.3461103439331055
Step 685 | grad_norm: 2.1264092922210693
Step 685 | learning_rate: 4.933333333333334e-05
Step 685 | epoch: 1.5222222222222221
Step 686 | loss: 3.04693865776062
Step 686 | grad_norm: 2.0373687744140625
Step 686 | learning_rate: 4.925925925925926e-05
Step 686 | epoch: 1.5244444444444445
Step 687 | loss: 3.5727710723876953
Step 687 | grad_norm: 3.022339344024658
Step 687 | learning_rate: 4.918518518518519e-05
Step 687 | epoch: 1.5266666666666666
Step 688 | loss: 2.62565541267395
Step 688 | grad_norm: 2.1903669834136963
Step 688 | learning_rate: 4.9111111111111114e-05
Step 688 | epoch: 1.528888888888889
Step 689 | loss: 2.8699142932891846
Step 689 | grad_norm: 2.629481077194214
Step 689 | learning_rate: 4.903703703703704e-05
Step 689 | epoch: 1.531111111111111
Step 690 | loss: 3.140554904937744
Step 690 | grad_norm: 2.242781162261963
Step 690 | learning_rate: 4.896296296296297e-05
Step 690 | epoch: 1.5333333333333332
Step 691 | loss: 3.2871172428131104
Step 691 | grad_norm: 2.0774271488189697
Step 691 | learning_rate: 4.888888888888889e-05
Step 691 | epoch: 1.5355555555555556
Step 692 | loss: 3.807438850402832
Step 692 | grad_norm: 3.5010600090026855
Step 692 | learning_rate: 4.881481481481482e-05
Step 692 | epoch: 1.537777777777778
Step 693 | loss: 3.4658806324005127
Step 693 | grad_norm: 3.2186434268951416
Step 693 | learning_rate: 4.874074074074074e-05
Step 693 | epoch: 1.54
Step 694 | loss: 3.53125
Step 694 | grad_norm: 2.277721643447876
Step 694 | learning_rate: 4.866666666666667e-05
Step 694 | epoch: 1.5422222222222222
Step 695 | loss: 2.80489182472229
Step 695 | grad_norm: 2.3513286113739014
Step 695 | learning_rate: 4.8592592592592596e-05
Step 695 | epoch: 1.5444444444444443
Step 696 | loss: 2.02842378616333
Step 696 | grad_norm: 2.191897392272949
Step 696 | learning_rate: 4.851851851851852e-05
Step 696 | epoch: 1.5466666666666666
Step 697 | loss: 3.2194976806640625
Step 697 | grad_norm: 2.5423648357391357
Step 697 | learning_rate: 4.844444444444445e-05
Step 697 | epoch: 1.548888888888889
Step 698 | loss: 3.2897279262542725
Step 698 | grad_norm: 2.8639137744903564
Step 698 | learning_rate: 4.837037037037037e-05
Step 698 | epoch: 1.551111111111111
Step 699 | loss: 3.8747143745422363
Step 699 | grad_norm: 2.98848819732666
Step 699 | learning_rate: 4.82962962962963e-05
Step 699 | epoch: 1.5533333333333332
Step 700 | loss: 4.095672607421875
Step 700 | grad_norm: 2.3189287185668945
Step 700 | learning_rate: 4.8222222222222225e-05
Step 700 | epoch: 1.5555555555555556
Step 701 | loss: 3.6535794734954834
Step 701 | grad_norm: 6.451162815093994
Step 701 | learning_rate: 4.814814814814815e-05
Step 701 | epoch: 1.557777777777778
Step 702 | loss: 3.1064202785491943
Step 702 | grad_norm: 2.525663375854492
Step 702 | learning_rate: 4.807407407407408e-05
Step 702 | epoch: 1.56
Step 703 | loss: 4.241554260253906
Step 703 | grad_norm: 3.9018056392669678
Step 703 | learning_rate: 4.8e-05
Step 703 | epoch: 1.5622222222222222
Step 704 | loss: 2.4817936420440674
Step 704 | grad_norm: 2.6060028076171875
Step 704 | learning_rate: 4.792592592592593e-05
Step 704 | epoch: 1.5644444444444443
Step 705 | loss: 3.7346832752227783
Step 705 | grad_norm: 3.142354726791382
Step 705 | learning_rate: 4.7851851851851854e-05
Step 705 | epoch: 1.5666666666666667
Step 706 | loss: 3.0833852291107178
Step 706 | grad_norm: 3.336914300918579
Step 706 | learning_rate: 4.7777777777777784e-05
Step 706 | epoch: 1.568888888888889
Step 707 | loss: 2.56954288482666
Step 707 | grad_norm: 2.7613790035247803
Step 707 | learning_rate: 4.770370370370371e-05
Step 707 | epoch: 1.5711111111111111
Step 708 | loss: 4.377303123474121
Step 708 | grad_norm: 2.9023656845092773
Step 708 | learning_rate: 4.762962962962963e-05
Step 708 | epoch: 1.5733333333333333
Step 709 | loss: 3.630152702331543
Step 709 | grad_norm: 2.4054996967315674
Step 709 | learning_rate: 4.755555555555556e-05
Step 709 | epoch: 1.5755555555555556
Step 710 | loss: 3.6770195960998535
Step 710 | grad_norm: 3.1096303462982178
Step 710 | learning_rate: 4.7481481481481483e-05
Step 710 | epoch: 1.5777777777777777
Step 711 | loss: 3.492042064666748
Step 711 | grad_norm: 2.4765195846557617
Step 711 | learning_rate: 4.740740740740741e-05
Step 711 | epoch: 1.58
Step 712 | loss: 2.3879542350769043
Step 712 | grad_norm: 2.378063917160034
Step 712 | learning_rate: 4.7333333333333336e-05
Step 712 | epoch: 1.5822222222222222
Step 713 | loss: 4.031048774719238
Step 713 | grad_norm: 2.4198381900787354
Step 713 | learning_rate: 4.7259259259259266e-05
Step 713 | epoch: 1.5844444444444443
Step 714 | loss: 3.157203197479248
Step 714 | grad_norm: 2.065718173980713
Step 714 | learning_rate: 4.718518518518519e-05
Step 714 | epoch: 1.5866666666666667
Step 715 | loss: 3.4646005630493164
Step 715 | grad_norm: 2.488206386566162
Step 715 | learning_rate: 4.711111111111111e-05
Step 715 | epoch: 1.588888888888889
Step 716 | loss: 3.284459352493286
Step 716 | grad_norm: 2.2251336574554443
Step 716 | learning_rate: 4.703703703703704e-05
Step 716 | epoch: 1.5911111111111111
Step 717 | loss: 3.4944875240325928
Step 717 | grad_norm: 2.363887310028076
Step 717 | learning_rate: 4.6962962962962966e-05
Step 717 | epoch: 1.5933333333333333
Step 718 | loss: 3.0946385860443115
Step 718 | grad_norm: 2.100858449935913
Step 718 | learning_rate: 4.6888888888888895e-05
Step 718 | epoch: 1.5955555555555554
Step 719 | loss: 3.5689926147460938
Step 719 | grad_norm: 2.5066614151000977
Step 719 | learning_rate: 4.681481481481482e-05
Step 719 | epoch: 1.5977777777777777
Step 720 | loss: 4.427603721618652
Step 720 | grad_norm: 2.734487295150757
Step 720 | learning_rate: 4.674074074074074e-05
Step 720 | epoch: 1.6
Step 721 | loss: 3.082968235015869
Step 721 | grad_norm: 2.3601508140563965
Step 721 | learning_rate: 4.666666666666667e-05
Step 721 | epoch: 1.6022222222222222
Step 722 | loss: 3.779104709625244
Step 722 | grad_norm: 2.767470359802246
Step 722 | learning_rate: 4.6592592592592595e-05
Step 722 | epoch: 1.6044444444444443
Step 723 | loss: 2.845895290374756
Step 723 | grad_norm: 2.0452933311462402
Step 723 | learning_rate: 4.6518518518518525e-05
Step 723 | epoch: 1.6066666666666667
Step 724 | loss: 3.2058191299438477
Step 724 | grad_norm: 2.0859060287475586
Step 724 | learning_rate: 4.644444444444445e-05
Step 724 | epoch: 1.608888888888889
Step 725 | loss: 3.5204548835754395
Step 725 | grad_norm: 2.532447576522827
Step 725 | learning_rate: 4.637037037037038e-05
Step 725 | epoch: 1.6111111111111112
Step 726 | loss: 3.8545379638671875
Step 726 | grad_norm: 2.900283098220825
Step 726 | learning_rate: 4.62962962962963e-05
Step 726 | epoch: 1.6133333333333333
Step 727 | loss: 3.6744697093963623
Step 727 | grad_norm: 4.0375142097473145
Step 727 | learning_rate: 4.6222222222222224e-05
Step 727 | epoch: 1.6155555555555554
Step 728 | loss: 3.2232906818389893
Step 728 | grad_norm: 2.377631664276123
Step 728 | learning_rate: 4.6148148148148154e-05
Step 728 | epoch: 1.6177777777777778
Step 729 | loss: 3.378464937210083
Step 729 | grad_norm: 2.9075329303741455
Step 729 | learning_rate: 4.607407407407408e-05
Step 729 | epoch: 1.62
Step 730 | loss: 3.537362813949585
Step 730 | grad_norm: 2.4679059982299805
Step 730 | learning_rate: 4.600000000000001e-05
Step 730 | epoch: 1.6222222222222222
Step 731 | loss: 2.9527900218963623
Step 731 | grad_norm: 2.265894889831543
Step 731 | learning_rate: 4.592592592592593e-05
Step 731 | epoch: 1.6244444444444444
Step 732 | loss: 3.688870668411255
Step 732 | grad_norm: 2.3291594982147217
Step 732 | learning_rate: 4.585185185185185e-05
Step 732 | epoch: 1.6266666666666667
Step 733 | loss: 3.3770995140075684
Step 733 | grad_norm: 2.202756643295288
Step 733 | learning_rate: 4.577777777777778e-05
Step 733 | epoch: 1.628888888888889
Step 734 | loss: 3.782804012298584
Step 734 | grad_norm: 2.991787910461426
Step 734 | learning_rate: 4.5703703703703706e-05
Step 734 | epoch: 1.6311111111111112
Step 735 | loss: 3.6653096675872803
Step 735 | grad_norm: 2.512434244155884
Step 735 | learning_rate: 4.5629629629629636e-05
Step 735 | epoch: 1.6333333333333333
Step 736 | loss: 3.3898189067840576
Step 736 | grad_norm: 2.3027682304382324
Step 736 | learning_rate: 4.555555555555556e-05
Step 736 | epoch: 1.6355555555555554
Step 737 | loss: 3.6944003105163574
Step 737 | grad_norm: 2.764765739440918
Step 737 | learning_rate: 4.548148148148149e-05
Step 737 | epoch: 1.6377777777777778
Step 738 | loss: 3.2870941162109375
Step 738 | grad_norm: 2.3610925674438477
Step 738 | learning_rate: 4.540740740740741e-05
Step 738 | epoch: 1.6400000000000001
Step 739 | loss: 3.4343643188476562
Step 739 | grad_norm: 2.095487356185913
Step 739 | learning_rate: 4.5333333333333335e-05
Step 739 | epoch: 1.6422222222222222
Step 740 | loss: 3.7856526374816895
Step 740 | grad_norm: 2.2587730884552
Step 740 | learning_rate: 4.5259259259259265e-05
Step 740 | epoch: 1.6444444444444444
Step 741 | loss: 4.059206962585449
Step 741 | grad_norm: 2.6787660121917725
Step 741 | learning_rate: 4.518518518518519e-05
Step 741 | epoch: 1.6466666666666665
Step 742 | loss: 3.9364895820617676
Step 742 | grad_norm: 2.398693561553955
Step 742 | learning_rate: 4.511111111111112e-05
Step 742 | epoch: 1.6488888888888888
Step 743 | loss: 3.2803359031677246
Step 743 | grad_norm: 2.593562364578247
Step 743 | learning_rate: 4.503703703703704e-05
Step 743 | epoch: 1.6511111111111112
Step 744 | loss: 3.356701135635376
Step 744 | grad_norm: 2.09187650680542
Step 744 | learning_rate: 4.496296296296297e-05
Step 744 | epoch: 1.6533333333333333
Step 745 | loss: 3.778716564178467
Step 745 | grad_norm: 2.9884510040283203
Step 745 | learning_rate: 4.4888888888888894e-05
Step 745 | epoch: 1.6555555555555554
Step 746 | loss: 3.2465016841888428
Step 746 | grad_norm: 2.355645179748535
Step 746 | learning_rate: 4.481481481481482e-05
Step 746 | epoch: 1.6577777777777778
Step 747 | loss: 2.7437901496887207
Step 747 | grad_norm: 2.534203052520752
Step 747 | learning_rate: 4.474074074074075e-05
Step 747 | epoch: 1.6600000000000001
Step 748 | loss: 3.5322165489196777
Step 748 | grad_norm: 3.051813840866089
Step 748 | learning_rate: 4.466666666666667e-05
Step 748 | epoch: 1.6622222222222223
Step 749 | loss: 2.763725519180298
Step 749 | grad_norm: 1.9288582801818848
Step 749 | learning_rate: 4.4592592592592594e-05
Step 749 | epoch: 1.6644444444444444
Step 750 | loss: 3.022459030151367
Step 750 | grad_norm: 2.767451763153076
Step 750 | learning_rate: 4.4518518518518523e-05
Step 750 | epoch: 1.6666666666666665
Step 751 | loss: 4.076461315155029
Step 751 | grad_norm: 2.6763901710510254
Step 751 | learning_rate: 4.4444444444444447e-05
Step 751 | epoch: 1.6688888888888889
Step 752 | loss: 2.6185765266418457
Step 752 | grad_norm: 1.802260160446167
Step 752 | learning_rate: 4.4370370370370376e-05
Step 752 | epoch: 1.6711111111111112
Step 753 | loss: 3.7530181407928467
Step 753 | grad_norm: 2.617492198944092
Step 753 | learning_rate: 4.42962962962963e-05
Step 753 | epoch: 1.6733333333333333
Step 754 | loss: 3.8517158031463623
Step 754 | grad_norm: 2.8243231773376465
Step 754 | learning_rate: 4.422222222222222e-05
Step 754 | epoch: 1.6755555555555555
Step 755 | loss: 3.6613619327545166
Step 755 | grad_norm: 2.0559160709381104
Step 755 | learning_rate: 4.414814814814815e-05
Step 755 | epoch: 1.6777777777777778
Step 756 | loss: 3.37689208984375
Step 756 | grad_norm: 2.3518621921539307
Step 756 | learning_rate: 4.4074074074074076e-05
Step 756 | epoch: 1.6800000000000002
Step 757 | loss: 3.117016077041626
Step 757 | grad_norm: 2.206508159637451
Step 757 | learning_rate: 4.4000000000000006e-05
Step 757 | epoch: 1.6822222222222223
Step 758 | loss: 3.5483200550079346
Step 758 | grad_norm: 2.9914603233337402
Step 758 | learning_rate: 4.392592592592593e-05
Step 758 | epoch: 1.6844444444444444
Step 759 | loss: 3.82470440864563
Step 759 | grad_norm: 2.2944929599761963
Step 759 | learning_rate: 4.385185185185185e-05
Step 759 | epoch: 1.6866666666666665
Step 760 | loss: 3.303008794784546
Step 760 | grad_norm: 1.9939831495285034
Step 760 | learning_rate: 4.377777777777778e-05
Step 760 | epoch: 1.6888888888888889
Step 761 | loss: 3.4619216918945312
Step 761 | grad_norm: 2.3381946086883545
Step 761 | learning_rate: 4.3703703703703705e-05
Step 761 | epoch: 1.6911111111111112
Step 762 | loss: 3.4109625816345215
Step 762 | grad_norm: 2.400365114212036
Step 762 | learning_rate: 4.3629629629629635e-05
Step 762 | epoch: 1.6933333333333334
Step 763 | loss: 3.2056803703308105
Step 763 | grad_norm: 2.700087308883667
Step 763 | learning_rate: 4.355555555555556e-05
Step 763 | epoch: 1.6955555555555555
Step 764 | loss: 4.3404154777526855
Step 764 | grad_norm: 2.7182273864746094
Step 764 | learning_rate: 4.348148148148148e-05
Step 764 | epoch: 1.6977777777777778
Step 765 | loss: 2.928784132003784
Step 765 | grad_norm: 2.277155876159668
Step 765 | learning_rate: 4.340740740740741e-05
Step 765 | epoch: 1.7
Step 766 | loss: 3.80177903175354
Step 766 | grad_norm: 2.804283380508423
Step 766 | learning_rate: 4.3333333333333334e-05
Step 766 | epoch: 1.7022222222222223
Step 767 | loss: 3.9328699111938477
Step 767 | grad_norm: 2.5993824005126953
Step 767 | learning_rate: 4.325925925925926e-05
Step 767 | epoch: 1.7044444444444444
Step 768 | loss: 2.930020570755005
Step 768 | grad_norm: 2.8676114082336426
Step 768 | learning_rate: 4.318518518518519e-05
Step 768 | epoch: 1.7066666666666666
Step 769 | loss: 3.5807087421417236
Step 769 | grad_norm: 2.695831537246704
Step 769 | learning_rate: 4.311111111111111e-05
Step 769 | epoch: 1.708888888888889
Step 770 | loss: 3.2571756839752197
Step 770 | grad_norm: 2.771275281906128
Step 770 | learning_rate: 4.303703703703704e-05
Step 770 | epoch: 1.7111111111111112
Step 771 | loss: 2.86537766456604
Step 771 | grad_norm: 2.357600688934326
Step 771 | learning_rate: 4.296296296296296e-05
Step 771 | epoch: 1.7133333333333334
Step 772 | loss: 3.0449979305267334
Step 772 | grad_norm: 2.2326760292053223
Step 772 | learning_rate: 4.2888888888888886e-05
Step 772 | epoch: 1.7155555555555555
Step 773 | loss: 3.4494712352752686
Step 773 | grad_norm: 2.333422899246216
Step 773 | learning_rate: 4.2814814814814816e-05
Step 773 | epoch: 1.7177777777777776
Step 774 | loss: 3.4448843002319336
Step 774 | grad_norm: 2.8413660526275635
Step 774 | learning_rate: 4.274074074074074e-05
Step 774 | epoch: 1.72
Step 775 | loss: 2.565796375274658
Step 775 | grad_norm: 2.5596940517425537
Step 775 | learning_rate: 4.266666666666667e-05
Step 775 | epoch: 1.7222222222222223
Step 776 | loss: 4.036561012268066
Step 776 | grad_norm: 3.1335339546203613
Step 776 | learning_rate: 4.259259259259259e-05
Step 776 | epoch: 1.7244444444444444
Step 777 | loss: 2.88053035736084
Step 777 | grad_norm: 2.311038017272949
Step 777 | learning_rate: 4.2518518518518515e-05
Step 777 | epoch: 1.7266666666666666
Step 778 | loss: 2.671182632446289
Step 778 | grad_norm: 2.147554636001587
Step 778 | learning_rate: 4.2444444444444445e-05
Step 778 | epoch: 1.728888888888889
Step 779 | loss: 3.8630032539367676
Step 779 | grad_norm: 2.6063504219055176
Step 779 | learning_rate: 4.237037037037037e-05
Step 779 | epoch: 1.7311111111111113
Step 780 | loss: 2.8629276752471924
Step 780 | grad_norm: 2.1297128200531006
Step 780 | learning_rate: 4.22962962962963e-05
Step 780 | epoch: 1.7333333333333334
Step 781 | loss: 3.273526906967163
Step 781 | grad_norm: 2.8389830589294434
Step 781 | learning_rate: 4.222222222222222e-05
Step 781 | epoch: 1.7355555555555555
Step 782 | loss: 3.7344560623168945
Step 782 | grad_norm: 2.1097285747528076
Step 782 | learning_rate: 4.2148148148148145e-05
Step 782 | epoch: 1.7377777777777776
Step 783 | loss: 4.159731864929199
Step 783 | grad_norm: 3.2172560691833496
Step 783 | learning_rate: 4.2074074074074075e-05
Step 783 | epoch: 1.74
Step 784 | loss: 2.9513068199157715
Step 784 | grad_norm: 2.1483776569366455
Step 784 | learning_rate: 4.2e-05
Step 784 | epoch: 1.7422222222222223
Step 785 | loss: 4.302516460418701
Step 785 | grad_norm: 3.3535423278808594
Step 785 | learning_rate: 4.192592592592593e-05
Step 785 | epoch: 1.7444444444444445
Step 786 | loss: 3.6771950721740723
Step 786 | grad_norm: 2.2859559059143066
Step 786 | learning_rate: 4.185185185185185e-05
Step 786 | epoch: 1.7466666666666666
Step 787 | loss: 3.245298147201538
Step 787 | grad_norm: 1.9943463802337646
Step 787 | learning_rate: 4.177777777777778e-05
Step 787 | epoch: 1.748888888888889
Step 788 | loss: 3.6907496452331543
Step 788 | grad_norm: 3.0508835315704346
Step 788 | learning_rate: 4.1703703703703704e-05
Step 788 | epoch: 1.751111111111111
Step 789 | loss: 2.9688665866851807
Step 789 | grad_norm: 2.293839931488037
Step 789 | learning_rate: 4.162962962962963e-05
Step 789 | epoch: 1.7533333333333334
Step 790 | loss: 3.9624125957489014
Step 790 | grad_norm: 2.7994959354400635
Step 790 | learning_rate: 4.155555555555556e-05
Step 790 | epoch: 1.7555555555555555
Step 791 | loss: 3.406156539916992
Step 791 | grad_norm: 2.3986878395080566
Step 791 | learning_rate: 4.148148148148148e-05
Step 791 | epoch: 1.7577777777777777
Step 792 | loss: 3.943458080291748
Step 792 | grad_norm: 2.573232412338257
Step 792 | learning_rate: 4.140740740740741e-05
Step 792 | epoch: 1.76
Step 793 | loss: 3.8223962783813477
Step 793 | grad_norm: 2.0478267669677734
Step 793 | learning_rate: 4.133333333333333e-05
Step 793 | epoch: 1.7622222222222224
Step 794 | loss: 3.1538572311401367
Step 794 | grad_norm: 2.029207229614258
Step 794 | learning_rate: 4.1259259259259256e-05
Step 794 | epoch: 1.7644444444444445
Step 795 | loss: 3.310189962387085
Step 795 | grad_norm: 2.453213930130005
Step 795 | learning_rate: 4.1185185185185186e-05
Step 795 | epoch: 1.7666666666666666
Step 796 | loss: 2.998720407485962
Step 796 | grad_norm: 3.0774929523468018
Step 796 | learning_rate: 4.111111111111111e-05
Step 796 | epoch: 1.7688888888888887
Step 797 | loss: 3.443617820739746
Step 797 | grad_norm: 2.636323928833008
Step 797 | learning_rate: 4.103703703703704e-05
Step 797 | epoch: 1.771111111111111
Step 798 | loss: 4.212064743041992
Step 798 | grad_norm: 2.4426753520965576
Step 798 | learning_rate: 4.096296296296296e-05
Step 798 | epoch: 1.7733333333333334
Step 799 | loss: 3.364285945892334
Step 799 | grad_norm: 2.960287570953369
Step 799 | learning_rate: 4.088888888888889e-05
Step 799 | epoch: 1.7755555555555556
Step 800 | loss: 3.5240235328674316
Step 800 | grad_norm: 2.5458805561065674
Step 800 | learning_rate: 4.0814814814814815e-05
Step 800 | epoch: 1.7777777777777777
Step 801 | loss: 4.298247814178467
Step 801 | grad_norm: 2.7901551723480225
Step 801 | learning_rate: 4.074074074074074e-05
Step 801 | epoch: 1.78
Step 802 | loss: 3.502431869506836
Step 802 | grad_norm: 2.501054048538208
Step 802 | learning_rate: 4.066666666666667e-05
Step 802 | epoch: 1.7822222222222224
Step 803 | loss: 3.0312535762786865
Step 803 | grad_norm: 2.3193695545196533
Step 803 | learning_rate: 4.059259259259259e-05
Step 803 | epoch: 1.7844444444444445
Step 804 | loss: 3.845208168029785
Step 804 | grad_norm: 3.7683639526367188
Step 804 | learning_rate: 4.051851851851852e-05
Step 804 | epoch: 1.7866666666666666
Step 805 | loss: 3.1587607860565186
Step 805 | grad_norm: 1.9807125329971313
Step 805 | learning_rate: 4.0444444444444444e-05
Step 805 | epoch: 1.7888888888888888
Step 806 | loss: 3.207096576690674
Step 806 | grad_norm: 2.6956329345703125
Step 806 | learning_rate: 4.0370370370370374e-05
Step 806 | epoch: 1.791111111111111
Step 807 | loss: 3.1348319053649902
Step 807 | grad_norm: 3.309946298599243
Step 807 | learning_rate: 4.02962962962963e-05
Step 807 | epoch: 1.7933333333333334
Step 808 | loss: 3.3629767894744873
Step 808 | grad_norm: 2.805598258972168
Step 808 | learning_rate: 4.022222222222222e-05
Step 808 | epoch: 1.7955555555555556
Step 809 | loss: 4.035331726074219
Step 809 | grad_norm: 2.8099825382232666
Step 809 | learning_rate: 4.014814814814815e-05
Step 809 | epoch: 1.7977777777777777
Step 810 | loss: 4.081336498260498
Step 810 | grad_norm: 3.053654909133911
Step 810 | learning_rate: 4.007407407407407e-05
Step 810 | epoch: 1.8
Step 811 | loss: 2.5689704418182373
Step 811 | grad_norm: 2.084717035293579
Step 811 | learning_rate: 4e-05
Step 811 | epoch: 1.8022222222222222
Step 812 | loss: 3.1602025032043457
Step 812 | grad_norm: 2.282607316970825
Step 812 | learning_rate: 3.9925925925925926e-05
Step 812 | epoch: 1.8044444444444445
Step 813 | loss: 3.4345316886901855
Step 813 | grad_norm: 2.2204129695892334
Step 813 | learning_rate: 3.985185185185185e-05
Step 813 | epoch: 1.8066666666666666
Step 814 | loss: 3.475090265274048
Step 814 | grad_norm: 2.089033842086792
Step 814 | learning_rate: 3.977777777777778e-05
Step 814 | epoch: 1.8088888888888888
Step 815 | loss: 4.089921474456787
Step 815 | grad_norm: 4.331231594085693
Step 815 | learning_rate: 3.97037037037037e-05
Step 815 | epoch: 1.8111111111111111
Step 816 | loss: 3.6872365474700928
Step 816 | grad_norm: 2.3684699535369873
Step 816 | learning_rate: 3.962962962962963e-05
Step 816 | epoch: 1.8133333333333335
Step 817 | loss: 2.9948019981384277
Step 817 | grad_norm: 2.153590679168701
Step 817 | learning_rate: 3.9555555555555556e-05
Step 817 | epoch: 1.8155555555555556
Step 818 | loss: 3.323812246322632
Step 818 | grad_norm: 3.517794132232666
Step 818 | learning_rate: 3.9481481481481485e-05
Step 818 | epoch: 1.8177777777777777
Step 819 | loss: 3.3841214179992676
Step 819 | grad_norm: 2.295604705810547
Step 819 | learning_rate: 3.940740740740741e-05
Step 819 | epoch: 1.8199999999999998
Step 820 | loss: 3.424346923828125
Step 820 | grad_norm: 2.350118637084961
Step 820 | learning_rate: 3.933333333333333e-05
Step 820 | epoch: 1.8222222222222222
Step 821 | loss: 3.281249761581421
Step 821 | grad_norm: 2.6227176189422607
Step 821 | learning_rate: 3.925925925925926e-05
Step 821 | epoch: 1.8244444444444445
Step 822 | loss: 3.4162282943725586
Step 822 | grad_norm: 2.6510159969329834
Step 822 | learning_rate: 3.9185185185185185e-05
Step 822 | epoch: 1.8266666666666667
Step 823 | loss: 3.2662551403045654
Step 823 | grad_norm: 2.278588056564331
Step 823 | learning_rate: 3.9111111111111115e-05
Step 823 | epoch: 1.8288888888888888
Step 824 | loss: 3.2751874923706055
Step 824 | grad_norm: 2.364590883255005
Step 824 | learning_rate: 3.903703703703704e-05
Step 824 | epoch: 1.8311111111111111
Step 825 | loss: 3.422250270843506
Step 825 | grad_norm: 2.3191452026367188
Step 825 | learning_rate: 3.896296296296296e-05
Step 825 | epoch: 1.8333333333333335
Step 826 | loss: 3.3528006076812744
Step 826 | grad_norm: 2.531198024749756
Step 826 | learning_rate: 3.888888888888889e-05
Step 826 | epoch: 1.8355555555555556
Step 827 | loss: 3.5519607067108154
Step 827 | grad_norm: 2.2953481674194336
Step 827 | learning_rate: 3.8814814814814814e-05
Step 827 | epoch: 1.8377777777777777
Step 828 | loss: 3.0551676750183105
Step 828 | grad_norm: 2.061021566390991
Step 828 | learning_rate: 3.8740740740740744e-05
Step 828 | epoch: 1.8399999999999999
Step 829 | loss: 2.9473676681518555
Step 829 | grad_norm: 2.2233200073242188
Step 829 | learning_rate: 3.866666666666667e-05
Step 829 | epoch: 1.8422222222222222
Step 830 | loss: 2.791628360748291
Step 830 | grad_norm: 2.5300116539001465
Step 830 | learning_rate: 3.85925925925926e-05
Step 830 | epoch: 1.8444444444444446
Step 831 | loss: 3.8047614097595215
Step 831 | grad_norm: 2.5686328411102295
Step 831 | learning_rate: 3.851851851851852e-05
Step 831 | epoch: 1.8466666666666667
Step 832 | loss: 4.223902702331543
Step 832 | grad_norm: 3.347252607345581
Step 832 | learning_rate: 3.844444444444444e-05
Step 832 | epoch: 1.8488888888888888
Step 833 | loss: 3.657663345336914
Step 833 | grad_norm: 1.9904340505599976
Step 833 | learning_rate: 3.837037037037037e-05
Step 833 | epoch: 1.8511111111111112
Step 834 | loss: 2.8617475032806396
Step 834 | grad_norm: 2.155917167663574
Step 834 | learning_rate: 3.8296296296296296e-05
Step 834 | epoch: 1.8533333333333335
Step 835 | loss: 2.5845391750335693
Step 835 | grad_norm: 2.1777560710906982
Step 835 | learning_rate: 3.8222222222222226e-05
Step 835 | epoch: 1.8555555555555556
Step 836 | loss: 3.7025928497314453
Step 836 | grad_norm: 2.589698314666748
Step 836 | learning_rate: 3.814814814814815e-05
Step 836 | epoch: 1.8577777777777778
Step 837 | loss: 3.1637349128723145
Step 837 | grad_norm: 2.163740634918213
Step 837 | learning_rate: 3.807407407407408e-05
Step 837 | epoch: 1.8599999999999999
Step 838 | loss: 3.5361194610595703
Step 838 | grad_norm: 2.6676852703094482
Step 838 | learning_rate: 3.8e-05
Step 838 | epoch: 1.8622222222222222
Step 839 | loss: 3.4729692935943604
Step 839 | grad_norm: 2.5861153602600098
Step 839 | learning_rate: 3.7925925925925925e-05
Step 839 | epoch: 1.8644444444444446
Step 840 | loss: 2.979928493499756
Step 840 | grad_norm: 2.421987295150757
Step 840 | learning_rate: 3.7851851851851855e-05
Step 840 | epoch: 1.8666666666666667
Step 841 | loss: 3.540376901626587
Step 841 | grad_norm: 2.009884834289551
Step 841 | learning_rate: 3.777777777777778e-05
Step 841 | epoch: 1.8688888888888888
Step 842 | loss: 3.349430799484253
Step 842 | grad_norm: 2.0914947986602783
Step 842 | learning_rate: 3.770370370370371e-05
Step 842 | epoch: 1.871111111111111
Step 843 | loss: 3.6264336109161377
Step 843 | grad_norm: 2.547067403793335
Step 843 | learning_rate: 3.762962962962963e-05
Step 843 | epoch: 1.8733333333333333
Step 844 | loss: 3.4141223430633545
Step 844 | grad_norm: 2.5972094535827637
Step 844 | learning_rate: 3.7555555555555554e-05
Step 844 | epoch: 1.8755555555555556
Step 845 | loss: 3.111098289489746
Step 845 | grad_norm: 2.016225576400757
Step 845 | learning_rate: 3.7481481481481484e-05
Step 845 | epoch: 1.8777777777777778
Step 846 | loss: 2.944312810897827
Step 846 | grad_norm: 2.1623499393463135
Step 846 | learning_rate: 3.740740740740741e-05
Step 846 | epoch: 1.88
Step 847 | loss: 3.1595914363861084
Step 847 | grad_norm: 2.329679012298584
Step 847 | learning_rate: 3.733333333333334e-05
Step 847 | epoch: 1.8822222222222222
Step 848 | loss: 4.157157897949219
Step 848 | grad_norm: 2.609544038772583
Step 848 | learning_rate: 3.725925925925926e-05
Step 848 | epoch: 1.8844444444444446
Step 849 | loss: 3.338212490081787
Step 849 | grad_norm: 1.9700887203216553
Step 849 | learning_rate: 3.718518518518519e-05
Step 849 | epoch: 1.8866666666666667
Step 850 | loss: 3.45736026763916
Step 850 | grad_norm: 2.0654916763305664
Step 850 | learning_rate: 3.7111111111111113e-05
Step 850 | epoch: 1.8888888888888888
Step 851 | loss: 2.958864450454712
Step 851 | grad_norm: 2.631148099899292
Step 851 | learning_rate: 3.7037037037037037e-05
Step 851 | epoch: 1.891111111111111
Step 852 | loss: 3.6937131881713867
Step 852 | grad_norm: 2.3020436763763428
Step 852 | learning_rate: 3.6962962962962966e-05
Step 852 | epoch: 1.8933333333333333
Step 853 | loss: 3.161079168319702
Step 853 | grad_norm: 2.5223774909973145
Step 853 | learning_rate: 3.688888888888889e-05
Step 853 | epoch: 1.8955555555555557
Step 854 | loss: 3.6219069957733154
Step 854 | grad_norm: 3.288764476776123
Step 854 | learning_rate: 3.681481481481482e-05
Step 854 | epoch: 1.8977777777777778
Step 855 | loss: 3.51369571685791
Step 855 | grad_norm: 2.9003496170043945
Step 855 | learning_rate: 3.674074074074074e-05
Step 855 | epoch: 1.9
Step 856 | loss: 3.4424140453338623
Step 856 | grad_norm: 2.4952213764190674
Step 856 | learning_rate: 3.6666666666666666e-05
Step 856 | epoch: 1.9022222222222223
Step 857 | loss: 3.024714469909668
Step 857 | grad_norm: 2.729532480239868
Step 857 | learning_rate: 3.6592592592592596e-05
Step 857 | epoch: 1.9044444444444446
Step 858 | loss: 4.253907203674316
Step 858 | grad_norm: 2.5731852054595947
Step 858 | learning_rate: 3.651851851851852e-05
Step 858 | epoch: 1.9066666666666667
Step 859 | loss: 3.6303749084472656
Step 859 | grad_norm: 2.5047547817230225
Step 859 | learning_rate: 3.644444444444445e-05
Step 859 | epoch: 1.9088888888888889
Step 860 | loss: 3.6103570461273193
Step 860 | grad_norm: 2.6673851013183594
Step 860 | learning_rate: 3.637037037037037e-05
Step 860 | epoch: 1.911111111111111
Step 861 | loss: 3.174487829208374
Step 861 | grad_norm: 2.669509172439575
Step 861 | learning_rate: 3.62962962962963e-05
Step 861 | epoch: 1.9133333333333333
Step 862 | loss: 3.6079213619232178
Step 862 | grad_norm: 2.4475555419921875
Step 862 | learning_rate: 3.6222222222222225e-05
Step 862 | epoch: 1.9155555555555557
Step 863 | loss: 3.480860471725464
Step 863 | grad_norm: 2.9864866733551025
Step 863 | learning_rate: 3.614814814814815e-05
Step 863 | epoch: 1.9177777777777778
Step 864 | loss: 4.303365707397461
Step 864 | grad_norm: 3.048053503036499
Step 864 | learning_rate: 3.607407407407408e-05
Step 864 | epoch: 1.92
Step 865 | loss: 3.2848587036132812
Step 865 | grad_norm: 2.3912360668182373
Step 865 | learning_rate: 3.6e-05
Step 865 | epoch: 1.9222222222222223
Step 866 | loss: 2.7712535858154297
Step 866 | grad_norm: 2.3540210723876953
Step 866 | learning_rate: 3.592592592592593e-05
Step 866 | epoch: 1.9244444444444444
Step 867 | loss: 3.450432777404785
Step 867 | grad_norm: 2.3936619758605957
Step 867 | learning_rate: 3.5851851851851854e-05
Step 867 | epoch: 1.9266666666666667
Step 868 | loss: 2.8418970108032227
Step 868 | grad_norm: 2.8311376571655273
Step 868 | learning_rate: 3.577777777777778e-05
Step 868 | epoch: 1.9288888888888889
Step 869 | loss: 2.8369200229644775
Step 869 | grad_norm: 2.3080103397369385
Step 869 | learning_rate: 3.570370370370371e-05
Step 869 | epoch: 1.931111111111111
Step 870 | loss: 2.7888317108154297
Step 870 | grad_norm: 2.846949338912964
Step 870 | learning_rate: 3.562962962962963e-05
Step 870 | epoch: 1.9333333333333333
Step 871 | loss: 4.095093727111816
Step 871 | grad_norm: 3.392498254776001
Step 871 | learning_rate: 3.555555555555556e-05
Step 871 | epoch: 1.9355555555555557
Step 872 | loss: 3.865330457687378
Step 872 | grad_norm: 2.6416754722595215
Step 872 | learning_rate: 3.548148148148148e-05
Step 872 | epoch: 1.9377777777777778
Step 873 | loss: 3.9729676246643066
Step 873 | grad_norm: 3.3254716396331787
Step 873 | learning_rate: 3.540740740740741e-05
Step 873 | epoch: 1.94
Step 874 | loss: 3.3635671138763428
Step 874 | grad_norm: 2.35050630569458
Step 874 | learning_rate: 3.5333333333333336e-05
Step 874 | epoch: 1.942222222222222
Step 875 | loss: 2.4675686359405518
Step 875 | grad_norm: 2.6501026153564453
Step 875 | learning_rate: 3.525925925925926e-05
Step 875 | epoch: 1.9444444444444444
Step 876 | loss: 3.2358436584472656
Step 876 | grad_norm: 2.6704812049865723
Step 876 | learning_rate: 3.518518518518519e-05
Step 876 | epoch: 1.9466666666666668
Step 877 | loss: 3.3660731315612793
Step 877 | grad_norm: 3.0358333587646484
Step 877 | learning_rate: 3.511111111111111e-05
Step 877 | epoch: 1.948888888888889
Step 878 | loss: 2.528769016265869
Step 878 | grad_norm: 3.102263927459717
Step 878 | learning_rate: 3.503703703703704e-05
Step 878 | epoch: 1.951111111111111
Step 879 | loss: 3.0767979621887207
Step 879 | grad_norm: 2.346065044403076
Step 879 | learning_rate: 3.4962962962962965e-05
Step 879 | epoch: 1.9533333333333334
Step 880 | loss: 2.8744218349456787
Step 880 | grad_norm: 2.0455732345581055
Step 880 | learning_rate: 3.4888888888888895e-05
Step 880 | epoch: 1.9555555555555557
Step 881 | loss: 3.542628049850464
Step 881 | grad_norm: 2.0334527492523193
Step 881 | learning_rate: 3.481481481481482e-05
Step 881 | epoch: 1.9577777777777778
Step 882 | loss: 3.773380994796753
Step 882 | grad_norm: 2.6197803020477295
Step 882 | learning_rate: 3.474074074074074e-05
Step 882 | epoch: 1.96
Step 883 | loss: 3.0574281215667725
Step 883 | grad_norm: 2.351724147796631
Step 883 | learning_rate: 3.466666666666667e-05
Step 883 | epoch: 1.962222222222222
Step 884 | loss: 3.0141475200653076
Step 884 | grad_norm: 1.8703420162200928
Step 884 | learning_rate: 3.4592592592592594e-05
Step 884 | epoch: 1.9644444444444444
Step 885 | loss: 3.491825819015503
Step 885 | grad_norm: 2.0601868629455566
Step 885 | learning_rate: 3.4518518518518524e-05
Step 885 | epoch: 1.9666666666666668
Step 886 | loss: 3.2889647483825684
Step 886 | grad_norm: 2.404827833175659
Step 886 | learning_rate: 3.444444444444445e-05
Step 886 | epoch: 1.968888888888889
Step 887 | loss: 4.294521808624268
Step 887 | grad_norm: 3.1230242252349854
Step 887 | learning_rate: 3.437037037037037e-05
Step 887 | epoch: 1.971111111111111
Step 888 | loss: 3.3756043910980225
Step 888 | grad_norm: 3.177145481109619
Step 888 | learning_rate: 3.42962962962963e-05
Step 888 | epoch: 1.9733333333333334
Step 889 | loss: 2.5887582302093506
Step 889 | grad_norm: 2.228748083114624
Step 889 | learning_rate: 3.4222222222222224e-05
Step 889 | epoch: 1.9755555555555555
Step 890 | loss: 3.1723780632019043
Step 890 | grad_norm: 2.3715169429779053
Step 890 | learning_rate: 3.4148148148148153e-05
Step 890 | epoch: 1.9777777777777779
Step 891 | loss: 3.191272020339966
Step 891 | grad_norm: 3.1190545558929443
Step 891 | learning_rate: 3.4074074074074077e-05
Step 891 | epoch: 1.98
Step 892 | loss: 3.586919069290161
Step 892 | grad_norm: 3.2876555919647217
Step 892 | learning_rate: 3.4000000000000007e-05
Step 892 | epoch: 1.982222222222222
Step 893 | loss: 2.882699489593506
Step 893 | grad_norm: 2.072126626968384
Step 893 | learning_rate: 3.392592592592593e-05
Step 893 | epoch: 1.9844444444444445
Step 894 | loss: 3.233672857284546
Step 894 | grad_norm: 2.3876547813415527
Step 894 | learning_rate: 3.385185185185185e-05
Step 894 | epoch: 1.9866666666666668
Step 895 | loss: 3.0271613597869873
Step 895 | grad_norm: 2.4573748111724854
Step 895 | learning_rate: 3.377777777777778e-05
Step 895 | epoch: 1.988888888888889
Step 896 | loss: 3.502802610397339
Step 896 | grad_norm: 2.659804105758667
Step 896 | learning_rate: 3.3703703703703706e-05
Step 896 | epoch: 1.991111111111111
Step 897 | loss: 3.790816068649292
Step 897 | grad_norm: 2.7896814346313477
Step 897 | learning_rate: 3.3629629629629636e-05
Step 897 | epoch: 1.9933333333333332
Step 898 | loss: 2.9053914546966553
Step 898 | grad_norm: 2.7546226978302
Step 898 | learning_rate: 3.355555555555556e-05
Step 898 | epoch: 1.9955555555555555
Step 899 | loss: 3.2670063972473145
Step 899 | grad_norm: 2.540356159210205
Step 899 | learning_rate: 3.348148148148148e-05
Step 899 | epoch: 1.9977777777777779
Step 900 | loss: 3.119378089904785
Step 900 | grad_norm: 2.4102091789245605
Step 900 | learning_rate: 3.340740740740741e-05
Step 900 | epoch: 2.0
Step 901 | loss: 3.4474198818206787
Step 901 | grad_norm: 2.748080015182495
Step 901 | learning_rate: 3.3333333333333335e-05
Step 901 | epoch: 2.002222222222222
Step 902 | loss: 3.591249704360962
Step 902 | grad_norm: 3.1421866416931152
Step 902 | learning_rate: 3.3259259259259265e-05
Step 902 | epoch: 2.0044444444444443
Step 903 | loss: 3.372926712036133
Step 903 | grad_norm: 2.4994163513183594
Step 903 | learning_rate: 3.318518518518519e-05
Step 903 | epoch: 2.006666666666667
Step 904 | loss: 3.603388786315918
Step 904 | grad_norm: 2.5515103340148926
Step 904 | learning_rate: 3.311111111111112e-05
Step 904 | epoch: 2.008888888888889
Step 905 | loss: 5.094610214233398
Step 905 | grad_norm: 3.7764220237731934
Step 905 | learning_rate: 3.303703703703704e-05
Step 905 | epoch: 2.011111111111111
Step 906 | loss: 3.147196054458618
Step 906 | grad_norm: 2.93873929977417
Step 906 | learning_rate: 3.2962962962962964e-05
Step 906 | epoch: 2.013333333333333
Step 907 | loss: 2.965137481689453
Step 907 | grad_norm: 2.186971426010132
Step 907 | learning_rate: 3.2888888888888894e-05
Step 907 | epoch: 2.0155555555555558
Step 908 | loss: 3.0333049297332764
Step 908 | grad_norm: 2.5087366104125977
Step 908 | learning_rate: 3.281481481481482e-05
Step 908 | epoch: 2.017777777777778
Step 909 | loss: 2.9334895610809326
Step 909 | grad_norm: 2.4095733165740967
Step 909 | learning_rate: 3.274074074074075e-05
Step 909 | epoch: 2.02
Step 910 | loss: 3.6253747940063477
Step 910 | grad_norm: 3.428297281265259
Step 910 | learning_rate: 3.266666666666667e-05
Step 910 | epoch: 2.022222222222222
Step 911 | loss: 3.4873275756835938
Step 911 | grad_norm: 2.3019957542419434
Step 911 | learning_rate: 3.25925925925926e-05
Step 911 | epoch: 2.0244444444444443
Step 912 | loss: 3.085376262664795
Step 912 | grad_norm: 2.698611259460449
Step 912 | learning_rate: 3.251851851851852e-05
Step 912 | epoch: 2.026666666666667
Step 913 | loss: 3.587790012359619
Step 913 | grad_norm: 2.32021427154541
Step 913 | learning_rate: 3.2444444444444446e-05
Step 913 | epoch: 2.028888888888889
Step 914 | loss: 3.30230975151062
Step 914 | grad_norm: 2.432913064956665
Step 914 | learning_rate: 3.2370370370370376e-05
Step 914 | epoch: 2.031111111111111
Step 915 | loss: 4.270177364349365
Step 915 | grad_norm: 2.8572747707366943
Step 915 | learning_rate: 3.22962962962963e-05
Step 915 | epoch: 2.033333333333333
Step 916 | loss: 3.772449254989624
Step 916 | grad_norm: 2.8148114681243896
Step 916 | learning_rate: 3.222222222222223e-05
Step 916 | epoch: 2.0355555555555553
Step 917 | loss: 3.4487805366516113
Step 917 | grad_norm: 2.5633625984191895
Step 917 | learning_rate: 3.214814814814815e-05
Step 917 | epoch: 2.037777777777778
Step 918 | loss: 3.37764835357666
Step 918 | grad_norm: 2.6003875732421875
Step 918 | learning_rate: 3.2074074074074075e-05
Step 918 | epoch: 2.04
Step 919 | loss: 3.558232307434082
Step 919 | grad_norm: 2.6272997856140137
Step 919 | learning_rate: 3.2000000000000005e-05
Step 919 | epoch: 2.042222222222222
Step 920 | loss: 2.8933022022247314
Step 920 | grad_norm: 2.2606728076934814
Step 920 | learning_rate: 3.192592592592593e-05
Step 920 | epoch: 2.0444444444444443
Step 921 | loss: 3.413119316101074
Step 921 | grad_norm: 2.49220871925354
Step 921 | learning_rate: 3.185185185185185e-05
Step 921 | epoch: 2.046666666666667
Step 922 | loss: 3.832087278366089
Step 922 | grad_norm: 1.9511358737945557
Step 922 | learning_rate: 3.177777777777778e-05
Step 922 | epoch: 2.048888888888889
Step 923 | loss: 2.660881280899048
Step 923 | grad_norm: 2.307331085205078
Step 923 | learning_rate: 3.1703703703703705e-05
Step 923 | epoch: 2.051111111111111
Step 924 | loss: 3.722256898880005
Step 924 | grad_norm: 2.5188825130462646
Step 924 | learning_rate: 3.1629629629629634e-05
Step 924 | epoch: 2.0533333333333332
Step 925 | loss: 3.1226394176483154
Step 925 | grad_norm: 2.6292262077331543
Step 925 | learning_rate: 3.155555555555556e-05
Step 925 | epoch: 2.0555555555555554
Step 926 | loss: 4.222146034240723
Step 926 | grad_norm: 3.0069425106048584
Step 926 | learning_rate: 3.148148148148148e-05
Step 926 | epoch: 2.057777777777778
Step 927 | loss: 3.0391180515289307
Step 927 | grad_norm: 1.9343442916870117
Step 927 | learning_rate: 3.140740740740741e-05
Step 927 | epoch: 2.06
Step 928 | loss: 3.01435923576355
Step 928 | grad_norm: 2.455322027206421
Step 928 | learning_rate: 3.1333333333333334e-05
Step 928 | epoch: 2.062222222222222
Step 929 | loss: 3.7446954250335693
Step 929 | grad_norm: 2.7779977321624756
Step 929 | learning_rate: 3.1259259259259264e-05
Step 929 | epoch: 2.0644444444444443
Step 930 | loss: 3.157496213912964
Step 930 | grad_norm: 2.269909620285034
Step 930 | learning_rate: 3.118518518518519e-05
Step 930 | epoch: 2.066666666666667
Step 931 | loss: 4.220101356506348
Step 931 | grad_norm: 2.6362686157226562
Step 931 | learning_rate: 3.111111111111111e-05
Step 931 | epoch: 2.068888888888889
Step 932 | loss: 3.43603515625
Step 932 | grad_norm: 2.471881866455078
Step 932 | learning_rate: 3.103703703703704e-05
Step 932 | epoch: 2.071111111111111
Step 933 | loss: 3.7101192474365234
Step 933 | grad_norm: 2.9499943256378174
Step 933 | learning_rate: 3.096296296296296e-05
Step 933 | epoch: 2.0733333333333333
Step 934 | loss: 3.0868396759033203
Step 934 | grad_norm: 2.3556556701660156
Step 934 | learning_rate: 3.088888888888889e-05
Step 934 | epoch: 2.0755555555555554
Step 935 | loss: 3.3998401165008545
Step 935 | grad_norm: 2.7317380905151367
Step 935 | learning_rate: 3.0814814814814816e-05
Step 935 | epoch: 2.077777777777778
Step 936 | loss: 3.5753731727600098
Step 936 | grad_norm: 2.5068140029907227
Step 936 | learning_rate: 3.074074074074074e-05
Step 936 | epoch: 2.08
Step 937 | loss: 2.762209892272949
Step 937 | grad_norm: 2.054872751235962
Step 937 | learning_rate: 3.066666666666667e-05
Step 937 | epoch: 2.082222222222222
Step 938 | loss: 3.9310648441314697
Step 938 | grad_norm: 2.515958309173584
Step 938 | learning_rate: 3.059259259259259e-05
Step 938 | epoch: 2.0844444444444443
Step 939 | loss: 3.517362594604492
Step 939 | grad_norm: 2.576014757156372
Step 939 | learning_rate: 3.0518518518518515e-05
Step 939 | epoch: 2.086666666666667
Step 940 | loss: 3.006476879119873
Step 940 | grad_norm: 1.869023084640503
Step 940 | learning_rate: 3.044444444444445e-05
Step 940 | epoch: 2.088888888888889
Step 941 | loss: 3.556107997894287
Step 941 | grad_norm: 2.4845311641693115
Step 941 | learning_rate: 3.037037037037037e-05
Step 941 | epoch: 2.091111111111111
Step 942 | loss: 2.835040807723999
Step 942 | grad_norm: 2.2078754901885986
Step 942 | learning_rate: 3.02962962962963e-05
Step 942 | epoch: 2.0933333333333333
Step 943 | loss: 3.5569891929626465
Step 943 | grad_norm: 2.718820333480835
Step 943 | learning_rate: 3.0222222222222225e-05
Step 943 | epoch: 2.0955555555555554
Step 944 | loss: 2.833977699279785
Step 944 | grad_norm: 2.336129903793335
Step 944 | learning_rate: 3.0148148148148148e-05
Step 944 | epoch: 2.097777777777778
Step 945 | loss: 3.5850415229797363
Step 945 | grad_norm: 2.8789286613464355
Step 945 | learning_rate: 3.0074074074074078e-05
Step 945 | epoch: 2.1
Step 946 | loss: 3.2645585536956787
Step 946 | grad_norm: 2.560455799102783
Step 946 | learning_rate: 3e-05
Step 946 | epoch: 2.102222222222222
Step 947 | loss: 2.5114450454711914
Step 947 | grad_norm: 1.5246458053588867
Step 947 | learning_rate: 2.992592592592593e-05
Step 947 | epoch: 2.1044444444444443
Step 948 | loss: 3.302046298980713
Step 948 | grad_norm: 2.3741767406463623
Step 948 | learning_rate: 2.9851851851851854e-05
Step 948 | epoch: 2.1066666666666665
Step 949 | loss: 3.735283613204956
Step 949 | grad_norm: 2.633087396621704
Step 949 | learning_rate: 2.9777777777777777e-05
Step 949 | epoch: 2.108888888888889
Step 950 | loss: 3.171160936355591
Step 950 | grad_norm: 2.235811471939087
Step 950 | learning_rate: 2.9703703703703707e-05
Step 950 | epoch: 2.111111111111111
Step 951 | loss: 2.506687879562378
Step 951 | grad_norm: 2.230401039123535
Step 951 | learning_rate: 2.962962962962963e-05
Step 951 | epoch: 2.1133333333333333
Step 952 | loss: 2.297868013381958
Step 952 | grad_norm: 2.591909885406494
Step 952 | learning_rate: 2.955555555555556e-05
Step 952 | epoch: 2.1155555555555554
Step 953 | loss: 2.712418794631958
Step 953 | grad_norm: 3.2502355575561523
Step 953 | learning_rate: 2.9481481481481483e-05
Step 953 | epoch: 2.117777777777778
Step 954 | loss: 3.3732025623321533
Step 954 | grad_norm: 2.438528299331665
Step 954 | learning_rate: 2.9407407407407413e-05
Step 954 | epoch: 2.12
Step 955 | loss: 3.4590156078338623
Step 955 | grad_norm: 3.5143024921417236
Step 955 | learning_rate: 2.9333333333333336e-05
Step 955 | epoch: 2.1222222222222222
Step 956 | loss: 3.3265540599823
Step 956 | grad_norm: 2.346940279006958
Step 956 | learning_rate: 2.925925925925926e-05
Step 956 | epoch: 2.1244444444444444
Step 957 | loss: 2.911370277404785
Step 957 | grad_norm: 2.1794369220733643
Step 957 | learning_rate: 2.918518518518519e-05
Step 957 | epoch: 2.1266666666666665
Step 958 | loss: 3.414828300476074
Step 958 | grad_norm: 2.9408884048461914
Step 958 | learning_rate: 2.9111111111111112e-05
Step 958 | epoch: 2.128888888888889
Step 959 | loss: 3.483919382095337
Step 959 | grad_norm: 1.8490159511566162
Step 959 | learning_rate: 2.9037037037037042e-05
Step 959 | epoch: 2.131111111111111
Step 960 | loss: 3.354004144668579
Step 960 | grad_norm: 2.2089383602142334
Step 960 | learning_rate: 2.8962962962962965e-05
Step 960 | epoch: 2.1333333333333333
Step 961 | loss: 2.5437374114990234
Step 961 | grad_norm: 2.498952865600586
Step 961 | learning_rate: 2.8888888888888888e-05
Step 961 | epoch: 2.1355555555555554
Step 962 | loss: 2.6125569343566895
Step 962 | grad_norm: 2.4800822734832764
Step 962 | learning_rate: 2.8814814814814818e-05
Step 962 | epoch: 2.137777777777778
Step 963 | loss: 2.952728271484375
Step 963 | grad_norm: 2.889082670211792
Step 963 | learning_rate: 2.874074074074074e-05
Step 963 | epoch: 2.14
Step 964 | loss: 3.703050136566162
Step 964 | grad_norm: 2.4706790447235107
Step 964 | learning_rate: 2.8666666666666668e-05
Step 964 | epoch: 2.1422222222222222
Step 965 | loss: 4.254663944244385
Step 965 | grad_norm: 2.6715478897094727
Step 965 | learning_rate: 2.8592592592592594e-05
Step 965 | epoch: 2.1444444444444444
Step 966 | loss: 2.8948981761932373
Step 966 | grad_norm: 2.0469613075256348
Step 966 | learning_rate: 2.851851851851852e-05
Step 966 | epoch: 2.1466666666666665
Step 967 | loss: 3.133868455886841
Step 967 | grad_norm: 2.2027361392974854
Step 967 | learning_rate: 2.8444444444444447e-05
Step 967 | epoch: 2.148888888888889
Step 968 | loss: 3.0286059379577637
Step 968 | grad_norm: 2.4254300594329834
Step 968 | learning_rate: 2.837037037037037e-05
Step 968 | epoch: 2.151111111111111
Step 969 | loss: 3.344029426574707
Step 969 | grad_norm: 2.1265485286712646
Step 969 | learning_rate: 2.8296296296296297e-05
Step 969 | epoch: 2.1533333333333333
Step 970 | loss: 3.322646379470825
Step 970 | grad_norm: 2.343287944793701
Step 970 | learning_rate: 2.8222222222222223e-05
Step 970 | epoch: 2.1555555555555554
Step 971 | loss: 4.028956890106201
Step 971 | grad_norm: 3.0484988689422607
Step 971 | learning_rate: 2.814814814814815e-05
Step 971 | epoch: 2.1577777777777776
Step 972 | loss: 3.7047863006591797
Step 972 | grad_norm: 2.924452304840088
Step 972 | learning_rate: 2.8074074074074076e-05
Step 972 | epoch: 2.16
Step 973 | loss: 2.5928733348846436
Step 973 | grad_norm: 1.9205834865570068
Step 973 | learning_rate: 2.8000000000000003e-05
Step 973 | epoch: 2.1622222222222223
Step 974 | loss: 3.396061897277832
Step 974 | grad_norm: 2.5165324211120605
Step 974 | learning_rate: 2.7925925925925926e-05
Step 974 | epoch: 2.1644444444444444
Step 975 | loss: 3.338484048843384
Step 975 | grad_norm: 2.922748327255249
Step 975 | learning_rate: 2.7851851851851853e-05
Step 975 | epoch: 2.1666666666666665
Step 976 | loss: 3.6047403812408447
Step 976 | grad_norm: 2.1451451778411865
Step 976 | learning_rate: 2.777777777777778e-05
Step 976 | epoch: 2.168888888888889
Step 977 | loss: 3.1647279262542725
Step 977 | grad_norm: 2.222536325454712
Step 977 | learning_rate: 2.7703703703703706e-05
Step 977 | epoch: 2.171111111111111
Step 978 | loss: 3.616941213607788
Step 978 | grad_norm: 2.9455482959747314
Step 978 | learning_rate: 2.7629629629629632e-05
Step 978 | epoch: 2.1733333333333333
Step 979 | loss: 2.3931589126586914
Step 979 | grad_norm: 2.1970467567443848
Step 979 | learning_rate: 2.7555555555555555e-05
Step 979 | epoch: 2.1755555555555555
Step 980 | loss: 3.298261880874634
Step 980 | grad_norm: 2.536116361618042
Step 980 | learning_rate: 2.7481481481481482e-05
Step 980 | epoch: 2.1777777777777776
Step 981 | loss: 3.6621150970458984
Step 981 | grad_norm: 2.601372003555298
Step 981 | learning_rate: 2.7407407407407408e-05
Step 981 | epoch: 2.18
Step 982 | loss: 2.946300745010376
Step 982 | grad_norm: 2.3598270416259766
Step 982 | learning_rate: 2.733333333333333e-05
Step 982 | epoch: 2.1822222222222223
Step 983 | loss: 3.6458775997161865
Step 983 | grad_norm: 2.959014654159546
Step 983 | learning_rate: 2.725925925925926e-05
Step 983 | epoch: 2.1844444444444444
Step 984 | loss: 3.997398853302002
Step 984 | grad_norm: 2.4977972507476807
Step 984 | learning_rate: 2.7185185185185184e-05
Step 984 | epoch: 2.1866666666666665
Step 985 | loss: 3.3105716705322266
Step 985 | grad_norm: 4.095158576965332
Step 985 | learning_rate: 2.7111111111111114e-05
Step 985 | epoch: 2.188888888888889
Step 986 | loss: 3.292879104614258
Step 986 | grad_norm: 2.715749740600586
Step 986 | learning_rate: 2.7037037037037037e-05
Step 986 | epoch: 2.1911111111111112
Step 987 | loss: 3.4500105381011963
Step 987 | grad_norm: 2.458458423614502
Step 987 | learning_rate: 2.696296296296296e-05
Step 987 | epoch: 2.1933333333333334
Step 988 | loss: 3.4800479412078857
Step 988 | grad_norm: 2.2325353622436523
Step 988 | learning_rate: 2.688888888888889e-05
Step 988 | epoch: 2.1955555555555555
Step 989 | loss: 3.4657557010650635
Step 989 | grad_norm: 2.611814498901367
Step 989 | learning_rate: 2.6814814814814814e-05
Step 989 | epoch: 2.1977777777777776
Step 990 | loss: 3.517031192779541
Step 990 | grad_norm: 3.0894105434417725
Step 990 | learning_rate: 2.6740740740740743e-05
Step 990 | epoch: 2.2
Step 991 | loss: 3.4156017303466797
Step 991 | grad_norm: 2.841538190841675
Step 991 | learning_rate: 2.6666666666666667e-05
Step 991 | epoch: 2.2022222222222223
Step 992 | loss: 3.5582387447357178
Step 992 | grad_norm: 3.089175224304199
Step 992 | learning_rate: 2.659259259259259e-05
Step 992 | epoch: 2.2044444444444444
Step 993 | loss: 3.390627145767212
Step 993 | grad_norm: 2.3989920616149902
Step 993 | learning_rate: 2.651851851851852e-05
Step 993 | epoch: 2.2066666666666666
Step 994 | loss: 2.9207658767700195
Step 994 | grad_norm: 1.8872015476226807
Step 994 | learning_rate: 2.6444444444444443e-05
Step 994 | epoch: 2.2088888888888887
Step 995 | loss: 2.7352676391601562
Step 995 | grad_norm: 2.1630496978759766
Step 995 | learning_rate: 2.6370370370370373e-05
Step 995 | epoch: 2.2111111111111112
Step 996 | loss: 3.2815499305725098
Step 996 | grad_norm: 2.6589300632476807
Step 996 | learning_rate: 2.6296296296296296e-05
Step 996 | epoch: 2.2133333333333334
Step 997 | loss: 3.634228467941284
Step 997 | grad_norm: 2.878697156906128
Step 997 | learning_rate: 2.6222222222222226e-05
Step 997 | epoch: 2.2155555555555555
Step 998 | loss: 4.201599597930908
Step 998 | grad_norm: 3.5702953338623047
Step 998 | learning_rate: 2.614814814814815e-05
Step 998 | epoch: 2.2177777777777776
Step 999 | loss: 3.676616668701172
Step 999 | grad_norm: 2.2908942699432373
Step 999 | learning_rate: 2.6074074074074072e-05
Step 999 | epoch: 2.22
Step 1000 | loss: 3.239442825317383
Step 1000 | grad_norm: 2.6219587326049805
Step 1000 | learning_rate: 2.6000000000000002e-05
Step 1000 | epoch: 2.2222222222222223
Step 1001 | loss: 3.2230381965637207
Step 1001 | grad_norm: 2.555610418319702
Step 1001 | learning_rate: 2.5925925925925925e-05
Step 1001 | epoch: 2.2244444444444444
Step 1002 | loss: 3.0414669513702393
Step 1002 | grad_norm: 2.2207412719726562
Step 1002 | learning_rate: 2.5851851851851855e-05
Step 1002 | epoch: 2.2266666666666666
Step 1003 | loss: 4.011416435241699
Step 1003 | grad_norm: 2.554140090942383
Step 1003 | learning_rate: 2.5777777777777778e-05
Step 1003 | epoch: 2.2288888888888887
Step 1004 | loss: 3.699328660964966
Step 1004 | grad_norm: 2.5403425693511963
Step 1004 | learning_rate: 2.5703703703703708e-05
Step 1004 | epoch: 2.2311111111111113
Step 1005 | loss: 3.0348052978515625
Step 1005 | grad_norm: 2.5897057056427
Step 1005 | learning_rate: 2.562962962962963e-05
Step 1005 | epoch: 2.2333333333333334
Step 1006 | loss: 2.763521909713745
Step 1006 | grad_norm: 2.784501314163208
Step 1006 | learning_rate: 2.5555555555555554e-05
Step 1006 | epoch: 2.2355555555555555
Step 1007 | loss: 2.7956011295318604
Step 1007 | grad_norm: 2.2858102321624756
Step 1007 | learning_rate: 2.5481481481481484e-05
Step 1007 | epoch: 2.2377777777777776
Step 1008 | loss: 3.4577770233154297
Step 1008 | grad_norm: 2.4475772380828857
Step 1008 | learning_rate: 2.5407407407407407e-05
Step 1008 | epoch: 2.24
Step 1009 | loss: 2.965747117996216
Step 1009 | grad_norm: 2.1279473304748535
Step 1009 | learning_rate: 2.5333333333333337e-05
Step 1009 | epoch: 2.2422222222222223
Step 1010 | loss: 3.0373852252960205
Step 1010 | grad_norm: 2.5184125900268555
Step 1010 | learning_rate: 2.525925925925926e-05
Step 1010 | epoch: 2.2444444444444445
Step 1011 | loss: 3.394477128982544
Step 1011 | grad_norm: 3.6035711765289307
Step 1011 | learning_rate: 2.5185185185185183e-05
Step 1011 | epoch: 2.2466666666666666
Step 1012 | loss: 3.748173475265503
Step 1012 | grad_norm: 3.1230955123901367
Step 1012 | learning_rate: 2.5111111111111113e-05
Step 1012 | epoch: 2.2488888888888887
Step 1013 | loss: 3.488468647003174
Step 1013 | grad_norm: 3.1566877365112305
Step 1013 | learning_rate: 2.5037037037037036e-05
Step 1013 | epoch: 2.2511111111111113
Step 1014 | loss: 3.110090970993042
Step 1014 | grad_norm: 2.781989336013794
Step 1014 | learning_rate: 2.4962962962962963e-05
Step 1014 | epoch: 2.2533333333333334
Step 1015 | loss: 3.8134517669677734
Step 1015 | grad_norm: 2.1695568561553955
Step 1015 | learning_rate: 2.488888888888889e-05
Step 1015 | epoch: 2.2555555555555555
Step 1016 | loss: 3.276613712310791
Step 1016 | grad_norm: 3.076097249984741
Step 1016 | learning_rate: 2.4814814814814816e-05
Step 1016 | epoch: 2.2577777777777777
Step 1017 | loss: 2.8183252811431885
Step 1017 | grad_norm: 1.8253957033157349
Step 1017 | learning_rate: 2.4740740740740742e-05
Step 1017 | epoch: 2.26
Step 1018 | loss: 3.086993932723999
Step 1018 | grad_norm: 2.4991495609283447
Step 1018 | learning_rate: 2.466666666666667e-05
Step 1018 | epoch: 2.2622222222222224
Step 1019 | loss: 3.3385651111602783
Step 1019 | grad_norm: 2.5372049808502197
Step 1019 | learning_rate: 2.4592592592592595e-05
Step 1019 | epoch: 2.2644444444444445
Step 1020 | loss: 3.54613995552063
Step 1020 | grad_norm: 3.1314985752105713
Step 1020 | learning_rate: 2.451851851851852e-05
Step 1020 | epoch: 2.2666666666666666
Step 1021 | loss: 3.4489827156066895
Step 1021 | grad_norm: 2.3985660076141357
Step 1021 | learning_rate: 2.4444444444444445e-05
Step 1021 | epoch: 2.2688888888888887
Step 1022 | loss: 2.987407684326172
Step 1022 | grad_norm: 2.675969362258911
Step 1022 | learning_rate: 2.437037037037037e-05
Step 1022 | epoch: 2.2711111111111113
Step 1023 | loss: 3.653646230697632
Step 1023 | grad_norm: 2.5450775623321533
Step 1023 | learning_rate: 2.4296296296296298e-05
Step 1023 | epoch: 2.2733333333333334
Step 1024 | loss: 3.6493735313415527
Step 1024 | grad_norm: 3.5922396183013916
Step 1024 | learning_rate: 2.4222222222222224e-05
Step 1024 | epoch: 2.2755555555555556
Step 1025 | loss: 3.4365673065185547
Step 1025 | grad_norm: 2.668405532836914
Step 1025 | learning_rate: 2.414814814814815e-05
Step 1025 | epoch: 2.2777777777777777
Step 1026 | loss: 2.7043755054473877
Step 1026 | grad_norm: 2.249448299407959
Step 1026 | learning_rate: 2.4074074074074074e-05
Step 1026 | epoch: 2.2800000000000002
Step 1027 | loss: 3.7054080963134766
Step 1027 | grad_norm: 2.9081451892852783
Step 1027 | learning_rate: 2.4e-05
Step 1027 | epoch: 2.2822222222222224
Step 1028 | loss: 3.5499649047851562
Step 1028 | grad_norm: 3.1151742935180664
Step 1028 | learning_rate: 2.3925925925925927e-05
Step 1028 | epoch: 2.2844444444444445
Step 1029 | loss: 3.095362901687622
Step 1029 | grad_norm: 2.2196972370147705
Step 1029 | learning_rate: 2.3851851851851854e-05
Step 1029 | epoch: 2.2866666666666666
Step 1030 | loss: 3.479895830154419
Step 1030 | grad_norm: 2.1900346279144287
Step 1030 | learning_rate: 2.377777777777778e-05
Step 1030 | epoch: 2.2888888888888888
Step 1031 | loss: 3.8462600708007812
Step 1031 | grad_norm: 2.717841625213623
Step 1031 | learning_rate: 2.3703703703703707e-05
Step 1031 | epoch: 2.2911111111111113
Step 1032 | loss: 2.652449131011963
Step 1032 | grad_norm: 2.367349147796631
Step 1032 | learning_rate: 2.3629629629629633e-05
Step 1032 | epoch: 2.2933333333333334
Step 1033 | loss: 2.920926570892334
Step 1033 | grad_norm: 2.153775930404663
Step 1033 | learning_rate: 2.3555555555555556e-05
Step 1033 | epoch: 2.2955555555555556
Step 1034 | loss: 3.34748911857605
Step 1034 | grad_norm: 2.8280532360076904
Step 1034 | learning_rate: 2.3481481481481483e-05
Step 1034 | epoch: 2.2977777777777777
Step 1035 | loss: 4.208230495452881
Step 1035 | grad_norm: 3.1574811935424805
Step 1035 | learning_rate: 2.340740740740741e-05
Step 1035 | epoch: 2.3
Step 1036 | loss: 4.525964736938477
Step 1036 | grad_norm: 2.912856340408325
Step 1036 | learning_rate: 2.3333333333333336e-05
Step 1036 | epoch: 2.3022222222222224
Step 1037 | loss: 3.190627098083496
Step 1037 | grad_norm: 2.131131172180176
Step 1037 | learning_rate: 2.3259259259259262e-05
Step 1037 | epoch: 2.3044444444444445
Step 1038 | loss: 3.1790976524353027
Step 1038 | grad_norm: 2.6204304695129395
Step 1038 | learning_rate: 2.318518518518519e-05
Step 1038 | epoch: 2.3066666666666666
Step 1039 | loss: 3.0546207427978516
Step 1039 | grad_norm: 2.7026565074920654
Step 1039 | learning_rate: 2.3111111111111112e-05
Step 1039 | epoch: 2.3088888888888888
Step 1040 | loss: 2.7294516563415527
Step 1040 | grad_norm: 2.224299669265747
Step 1040 | learning_rate: 2.303703703703704e-05
Step 1040 | epoch: 2.311111111111111
Step 1041 | loss: 3.932487964630127
Step 1041 | grad_norm: 3.192415714263916
Step 1041 | learning_rate: 2.2962962962962965e-05
Step 1041 | epoch: 2.3133333333333335
Step 1042 | loss: 3.6842148303985596
Step 1042 | grad_norm: 2.3622429370880127
Step 1042 | learning_rate: 2.288888888888889e-05
Step 1042 | epoch: 2.3155555555555556
Step 1043 | loss: 3.0806891918182373
Step 1043 | grad_norm: 3.1488094329833984
Step 1043 | learning_rate: 2.2814814814814818e-05
Step 1043 | epoch: 2.3177777777777777
Step 1044 | loss: 3.406728744506836
Step 1044 | grad_norm: 2.263134241104126
Step 1044 | learning_rate: 2.2740740740740744e-05
Step 1044 | epoch: 2.32
Step 1045 | loss: 3.4668407440185547
Step 1045 | grad_norm: 2.32108736038208
Step 1045 | learning_rate: 2.2666666666666668e-05
Step 1045 | epoch: 2.3222222222222224
Step 1046 | loss: 3.9059011936187744
Step 1046 | grad_norm: 3.081719160079956
Step 1046 | learning_rate: 2.2592592592592594e-05
Step 1046 | epoch: 2.3244444444444445
Step 1047 | loss: 2.9377923011779785
Step 1047 | grad_norm: 3.0110456943511963
Step 1047 | learning_rate: 2.251851851851852e-05
Step 1047 | epoch: 2.3266666666666667
Step 1048 | loss: 3.06685471534729
Step 1048 | grad_norm: 2.767655849456787
Step 1048 | learning_rate: 2.2444444444444447e-05
Step 1048 | epoch: 2.328888888888889
Step 1049 | loss: 2.8292317390441895
Step 1049 | grad_norm: 2.621835231781006
Step 1049 | learning_rate: 2.2370370370370374e-05
Step 1049 | epoch: 2.3311111111111114
Step 1050 | loss: 3.6168320178985596
Step 1050 | grad_norm: 2.6008880138397217
Step 1050 | learning_rate: 2.2296296296296297e-05
Step 1050 | epoch: 2.3333333333333335
Step 1051 | loss: 4.240199089050293
Step 1051 | grad_norm: 2.7162811756134033
Step 1051 | learning_rate: 2.2222222222222223e-05
Step 1051 | epoch: 2.3355555555555556
Step 1052 | loss: 3.32507586479187
Step 1052 | grad_norm: 2.9280917644500732
Step 1052 | learning_rate: 2.214814814814815e-05
Step 1052 | epoch: 2.3377777777777777
Step 1053 | loss: 3.5989720821380615
Step 1053 | grad_norm: 2.4672019481658936
Step 1053 | learning_rate: 2.2074074074074076e-05
Step 1053 | epoch: 2.34
Step 1054 | loss: 2.9730923175811768
Step 1054 | grad_norm: 2.360940456390381
Step 1054 | learning_rate: 2.2000000000000003e-05
Step 1054 | epoch: 2.3422222222222224
Step 1055 | loss: 2.894608497619629
Step 1055 | grad_norm: 2.326765775680542
Step 1055 | learning_rate: 2.1925925925925926e-05
Step 1055 | epoch: 2.3444444444444446
Step 1056 | loss: 3.4636316299438477
Step 1056 | grad_norm: 2.3815717697143555
Step 1056 | learning_rate: 2.1851851851851852e-05
Step 1056 | epoch: 2.3466666666666667
Step 1057 | loss: 3.8737974166870117
Step 1057 | grad_norm: 2.661053419113159
Step 1057 | learning_rate: 2.177777777777778e-05
Step 1057 | epoch: 2.348888888888889
Step 1058 | loss: 3.653881311416626
Step 1058 | grad_norm: 2.970041036605835
Step 1058 | learning_rate: 2.1703703703703705e-05
Step 1058 | epoch: 2.351111111111111
Step 1059 | loss: 3.7938177585601807
Step 1059 | grad_norm: 2.426309823989868
Step 1059 | learning_rate: 2.162962962962963e-05
Step 1059 | epoch: 2.3533333333333335
Step 1060 | loss: 3.665306329727173
Step 1060 | grad_norm: 2.815605878829956
Step 1060 | learning_rate: 2.1555555555555555e-05
Step 1060 | epoch: 2.3555555555555556
Step 1061 | loss: 3.4511911869049072
Step 1061 | grad_norm: 2.9748892784118652
Step 1061 | learning_rate: 2.148148148148148e-05
Step 1061 | epoch: 2.3577777777777778
Step 1062 | loss: 3.2717738151550293
Step 1062 | grad_norm: 2.382016181945801
Step 1062 | learning_rate: 2.1407407407407408e-05
Step 1062 | epoch: 2.36
Step 1063 | loss: 3.8272266387939453
Step 1063 | grad_norm: 2.845643997192383
Step 1063 | learning_rate: 2.1333333333333335e-05
Step 1063 | epoch: 2.362222222222222
Step 1064 | loss: 3.879547119140625
Step 1064 | grad_norm: 2.8129587173461914
Step 1064 | learning_rate: 2.1259259259259258e-05
Step 1064 | epoch: 2.3644444444444446
Step 1065 | loss: 2.8967485427856445
Step 1065 | grad_norm: 2.154236078262329
Step 1065 | learning_rate: 2.1185185185185184e-05
Step 1065 | epoch: 2.3666666666666667
Step 1066 | loss: 3.581655740737915
Step 1066 | grad_norm: 2.811020851135254
Step 1066 | learning_rate: 2.111111111111111e-05
Step 1066 | epoch: 2.368888888888889
Step 1067 | loss: 3.515807867050171
Step 1067 | grad_norm: 2.8390579223632812
Step 1067 | learning_rate: 2.1037037037037037e-05
Step 1067 | epoch: 2.371111111111111
Step 1068 | loss: 3.183164358139038
Step 1068 | grad_norm: 2.553044080734253
Step 1068 | learning_rate: 2.0962962962962964e-05
Step 1068 | epoch: 2.3733333333333335
Step 1069 | loss: 2.8985769748687744
Step 1069 | grad_norm: 2.4959847927093506
Step 1069 | learning_rate: 2.088888888888889e-05
Step 1069 | epoch: 2.3755555555555556
Step 1070 | loss: 3.173696279525757
Step 1070 | grad_norm: 2.638921022415161
Step 1070 | learning_rate: 2.0814814814814813e-05
Step 1070 | epoch: 2.3777777777777778
Step 1071 | loss: 2.8516933917999268
Step 1071 | grad_norm: 2.3231728076934814
Step 1071 | learning_rate: 2.074074074074074e-05
Step 1071 | epoch: 2.38
Step 1072 | loss: 3.9386589527130127
Step 1072 | grad_norm: 3.057642698287964
Step 1072 | learning_rate: 2.0666666666666666e-05
Step 1072 | epoch: 2.3822222222222225
Step 1073 | loss: 4.113243103027344
Step 1073 | grad_norm: 2.546151876449585
Step 1073 | learning_rate: 2.0592592592592593e-05
Step 1073 | epoch: 2.3844444444444446
Step 1074 | loss: 3.2464001178741455
Step 1074 | grad_norm: 2.5215353965759277
Step 1074 | learning_rate: 2.051851851851852e-05
Step 1074 | epoch: 2.3866666666666667
Step 1075 | loss: 2.8629093170166016
Step 1075 | grad_norm: 1.9298040866851807
Step 1075 | learning_rate: 2.0444444444444446e-05
Step 1075 | epoch: 2.388888888888889
Step 1076 | loss: 3.1803882122039795
Step 1076 | grad_norm: 2.903895854949951
Step 1076 | learning_rate: 2.037037037037037e-05
Step 1076 | epoch: 2.391111111111111
Step 1077 | loss: 3.538396120071411
Step 1077 | grad_norm: 2.223447799682617
Step 1077 | learning_rate: 2.0296296296296296e-05
Step 1077 | epoch: 2.3933333333333335
Step 1078 | loss: 3.559983015060425
Step 1078 | grad_norm: 2.358930826187134
Step 1078 | learning_rate: 2.0222222222222222e-05
Step 1078 | epoch: 2.3955555555555557
Step 1079 | loss: 3.096467971801758
Step 1079 | grad_norm: 2.273146867752075
Step 1079 | learning_rate: 2.014814814814815e-05
Step 1079 | epoch: 2.397777777777778
Step 1080 | loss: 3.490684747695923
Step 1080 | grad_norm: 3.6960225105285645
Step 1080 | learning_rate: 2.0074074074074075e-05
Step 1080 | epoch: 2.4
Step 1081 | loss: 3.8583221435546875
Step 1081 | grad_norm: 2.1237337589263916
Step 1081 | learning_rate: 2e-05
Step 1081 | epoch: 2.402222222222222
Step 1082 | loss: 3.3933002948760986
Step 1082 | grad_norm: 2.8576278686523438
Step 1082 | learning_rate: 1.9925925925925925e-05
Step 1082 | epoch: 2.4044444444444446
Step 1083 | loss: 3.5282020568847656
Step 1083 | grad_norm: 2.413719654083252
Step 1083 | learning_rate: 1.985185185185185e-05
Step 1083 | epoch: 2.4066666666666667
Step 1084 | loss: 3.2108266353607178
Step 1084 | grad_norm: 2.223849058151245
Step 1084 | learning_rate: 1.9777777777777778e-05
Step 1084 | epoch: 2.408888888888889
Step 1085 | loss: 3.5647776126861572
Step 1085 | grad_norm: 2.5241761207580566
Step 1085 | learning_rate: 1.9703703703703704e-05
Step 1085 | epoch: 2.411111111111111
Step 1086 | loss: 3.4046502113342285
Step 1086 | grad_norm: 2.393714427947998
Step 1086 | learning_rate: 1.962962962962963e-05
Step 1086 | epoch: 2.413333333333333
Step 1087 | loss: 2.8002445697784424
Step 1087 | grad_norm: 2.141453981399536
Step 1087 | learning_rate: 1.9555555555555557e-05
Step 1087 | epoch: 2.4155555555555557
Step 1088 | loss: 3.578610420227051
Step 1088 | grad_norm: 2.6933679580688477
Step 1088 | learning_rate: 1.948148148148148e-05
Step 1088 | epoch: 2.417777777777778
Step 1089 | loss: 3.5405309200286865
Step 1089 | grad_norm: 2.2003982067108154
Step 1089 | learning_rate: 1.9407407407407407e-05
Step 1089 | epoch: 2.42
Step 1090 | loss: 2.9136784076690674
Step 1090 | grad_norm: 2.327249765396118
Step 1090 | learning_rate: 1.9333333333333333e-05
Step 1090 | epoch: 2.422222222222222
Step 1091 | loss: 3.4010844230651855
Step 1091 | grad_norm: 2.3583385944366455
Step 1091 | learning_rate: 1.925925925925926e-05
Step 1091 | epoch: 2.4244444444444446
Step 1092 | loss: 4.445844650268555
Step 1092 | grad_norm: 2.8754937648773193
Step 1092 | learning_rate: 1.9185185185185186e-05
Step 1092 | epoch: 2.4266666666666667
Step 1093 | loss: 3.5159409046173096
Step 1093 | grad_norm: 2.8728177547454834
Step 1093 | learning_rate: 1.9111111111111113e-05
Step 1093 | epoch: 2.428888888888889
Step 1094 | loss: 2.546903133392334
Step 1094 | grad_norm: 2.114203691482544
Step 1094 | learning_rate: 1.903703703703704e-05
Step 1094 | epoch: 2.431111111111111
Step 1095 | loss: 2.7259063720703125
Step 1095 | grad_norm: 2.0724382400512695
Step 1095 | learning_rate: 1.8962962962962963e-05
Step 1095 | epoch: 2.4333333333333336
Step 1096 | loss: 3.3976635932922363
Step 1096 | grad_norm: 2.670091152191162
Step 1096 | learning_rate: 1.888888888888889e-05
Step 1096 | epoch: 2.4355555555555557
Step 1097 | loss: 4.038876533508301
Step 1097 | grad_norm: 4.813744068145752
Step 1097 | learning_rate: 1.8814814814814816e-05
Step 1097 | epoch: 2.437777777777778
Step 1098 | loss: 3.3629398345947266
Step 1098 | grad_norm: 2.2354650497436523
Step 1098 | learning_rate: 1.8740740740740742e-05
Step 1098 | epoch: 2.44
Step 1099 | loss: 3.243870496749878
Step 1099 | grad_norm: 2.2413036823272705
Step 1099 | learning_rate: 1.866666666666667e-05
Step 1099 | epoch: 2.442222222222222
Step 1100 | loss: 3.8677971363067627
Step 1100 | grad_norm: 2.511063814163208
Step 1100 | learning_rate: 1.8592592592592595e-05
Step 1100 | epoch: 2.4444444444444446
Step 1101 | loss: 4.014357566833496
Step 1101 | grad_norm: 3.1239097118377686
Step 1101 | learning_rate: 1.8518518518518518e-05
Step 1101 | epoch: 2.4466666666666668
Step 1102 | loss: 3.125162363052368
Step 1102 | grad_norm: 3.0087897777557373
Step 1102 | learning_rate: 1.8444444444444445e-05
Step 1102 | epoch: 2.448888888888889
Step 1103 | loss: 4.120966911315918
Step 1103 | grad_norm: 2.4556329250335693
Step 1103 | learning_rate: 1.837037037037037e-05
Step 1103 | epoch: 2.451111111111111
Step 1104 | loss: 3.0036160945892334
Step 1104 | grad_norm: 2.598449230194092
Step 1104 | learning_rate: 1.8296296296296298e-05
Step 1104 | epoch: 2.453333333333333
Step 1105 | loss: 2.967097043991089
Step 1105 | grad_norm: 2.1933670043945312
Step 1105 | learning_rate: 1.8222222222222224e-05
Step 1105 | epoch: 2.4555555555555557
Step 1106 | loss: 2.874009132385254
Step 1106 | grad_norm: 2.365671157836914
Step 1106 | learning_rate: 1.814814814814815e-05
Step 1106 | epoch: 2.457777777777778
Step 1107 | loss: 3.8923466205596924
Step 1107 | grad_norm: 2.6062071323394775
Step 1107 | learning_rate: 1.8074074074074074e-05
Step 1107 | epoch: 2.46
Step 1108 | loss: 3.0311660766601562
Step 1108 | grad_norm: 3.0763065814971924
Step 1108 | learning_rate: 1.8e-05
Step 1108 | epoch: 2.462222222222222
Step 1109 | loss: 2.7912933826446533
Step 1109 | grad_norm: 2.6425108909606934
Step 1109 | learning_rate: 1.7925925925925927e-05
Step 1109 | epoch: 2.464444444444444
Step 1110 | loss: 3.159080982208252
Step 1110 | grad_norm: 2.3218019008636475
Step 1110 | learning_rate: 1.7851851851851853e-05
Step 1110 | epoch: 2.466666666666667
Step 1111 | loss: 2.639901638031006
Step 1111 | grad_norm: 2.388009548187256
Step 1111 | learning_rate: 1.777777777777778e-05
Step 1111 | epoch: 2.468888888888889
Step 1112 | loss: 3.5172691345214844
Step 1112 | grad_norm: 2.5363833904266357
Step 1112 | learning_rate: 1.7703703703703706e-05
Step 1112 | epoch: 2.471111111111111
Step 1113 | loss: 3.13504958152771
Step 1113 | grad_norm: 2.007603168487549
Step 1113 | learning_rate: 1.762962962962963e-05
Step 1113 | epoch: 2.473333333333333
Step 1114 | loss: 3.4453632831573486
Step 1114 | grad_norm: 2.943758249282837
Step 1114 | learning_rate: 1.7555555555555556e-05
Step 1114 | epoch: 2.4755555555555557
Step 1115 | loss: 3.1148934364318848
Step 1115 | grad_norm: 2.9717519283294678
Step 1115 | learning_rate: 1.7481481481481483e-05
Step 1115 | epoch: 2.477777777777778
Step 1116 | loss: 3.857377529144287
Step 1116 | grad_norm: 3.269897699356079
Step 1116 | learning_rate: 1.740740740740741e-05
Step 1116 | epoch: 2.48
Step 1117 | loss: 3.4074578285217285
Step 1117 | grad_norm: 2.1925578117370605
Step 1117 | learning_rate: 1.7333333333333336e-05
Step 1117 | epoch: 2.482222222222222
Step 1118 | loss: 3.4598426818847656
Step 1118 | grad_norm: 2.6602046489715576
Step 1118 | learning_rate: 1.7259259259259262e-05
Step 1118 | epoch: 2.4844444444444447
Step 1119 | loss: 2.8873252868652344
Step 1119 | grad_norm: 3.9293699264526367
Step 1119 | learning_rate: 1.7185185185185185e-05
Step 1119 | epoch: 2.486666666666667
Step 1120 | loss: 3.6907100677490234
Step 1120 | grad_norm: 2.319096088409424
Step 1120 | learning_rate: 1.7111111111111112e-05
Step 1120 | epoch: 2.488888888888889
Step 1121 | loss: 3.2633395195007324
Step 1121 | grad_norm: 2.96738338470459
Step 1121 | learning_rate: 1.7037037037037038e-05
Step 1121 | epoch: 2.491111111111111
Step 1122 | loss: 3.3561043739318848
Step 1122 | grad_norm: 2.8388736248016357
Step 1122 | learning_rate: 1.6962962962962965e-05
Step 1122 | epoch: 2.493333333333333
Step 1123 | loss: 3.038766384124756
Step 1123 | grad_norm: 2.2681005001068115
Step 1123 | learning_rate: 1.688888888888889e-05
Step 1123 | epoch: 2.4955555555555557
Step 1124 | loss: 2.6806552410125732
Step 1124 | grad_norm: 2.0377843379974365
Step 1124 | learning_rate: 1.6814814814814818e-05
Step 1124 | epoch: 2.497777777777778
Step 1125 | loss: 2.2465219497680664
Step 1125 | grad_norm: 2.1596641540527344
Step 1125 | learning_rate: 1.674074074074074e-05
Step 1125 | epoch: 2.5
Step 1126 | loss: 3.005998134613037
Step 1126 | grad_norm: 1.9624857902526855
Step 1126 | learning_rate: 1.6666666666666667e-05
Step 1126 | epoch: 2.502222222222222
Step 1127 | loss: 3.627164840698242
Step 1127 | grad_norm: 2.0602169036865234
Step 1127 | learning_rate: 1.6592592592592594e-05
Step 1127 | epoch: 2.5044444444444443
Step 1128 | loss: 2.7107460498809814
Step 1128 | grad_norm: 2.5339598655700684
Step 1128 | learning_rate: 1.651851851851852e-05
Step 1128 | epoch: 2.506666666666667
Step 1129 | loss: 3.6800343990325928
Step 1129 | grad_norm: 2.663780450820923
Step 1129 | learning_rate: 1.6444444444444447e-05
Step 1129 | epoch: 2.508888888888889
Step 1130 | loss: 4.581859111785889
Step 1130 | grad_norm: 4.21275520324707
Step 1130 | learning_rate: 1.6370370370370374e-05
Step 1130 | epoch: 2.511111111111111
Step 1131 | loss: 3.6485612392425537
Step 1131 | grad_norm: 2.737759590148926
Step 1131 | learning_rate: 1.62962962962963e-05
Step 1131 | epoch: 2.513333333333333
Step 1132 | loss: 3.401149034500122
Step 1132 | grad_norm: 2.520559787750244
Step 1132 | learning_rate: 1.6222222222222223e-05
Step 1132 | epoch: 2.5155555555555553
Step 1133 | loss: 4.080501556396484
Step 1133 | grad_norm: 2.213902473449707
Step 1133 | learning_rate: 1.614814814814815e-05
Step 1133 | epoch: 2.517777777777778
Step 1134 | loss: 3.6765894889831543
Step 1134 | grad_norm: 3.081289529800415
Step 1134 | learning_rate: 1.6074074074074076e-05
Step 1134 | epoch: 2.52
Step 1135 | loss: 3.3276119232177734
Step 1135 | grad_norm: 2.5826661586761475
Step 1135 | learning_rate: 1.6000000000000003e-05
Step 1135 | epoch: 2.522222222222222
Step 1136 | loss: 3.7092702388763428
Step 1136 | grad_norm: 3.014030933380127
Step 1136 | learning_rate: 1.5925925925925926e-05
Step 1136 | epoch: 2.5244444444444447
Step 1137 | loss: 3.6926395893096924
Step 1137 | grad_norm: 2.7109150886535645
Step 1137 | learning_rate: 1.5851851851851852e-05
Step 1137 | epoch: 2.5266666666666664
Step 1138 | loss: 2.626146078109741
Step 1138 | grad_norm: 2.8096628189086914
Step 1138 | learning_rate: 1.577777777777778e-05
Step 1138 | epoch: 2.528888888888889
Step 1139 | loss: 3.422396659851074
Step 1139 | grad_norm: 2.6412365436553955
Step 1139 | learning_rate: 1.5703703703703705e-05
Step 1139 | epoch: 2.531111111111111
Step 1140 | loss: 4.247067928314209
Step 1140 | grad_norm: 2.7959837913513184
Step 1140 | learning_rate: 1.5629629629629632e-05
Step 1140 | epoch: 2.533333333333333
Step 1141 | loss: 3.1570639610290527
Step 1141 | grad_norm: 2.3891184329986572
Step 1141 | learning_rate: 1.5555555555555555e-05
Step 1141 | epoch: 2.535555555555556
Step 1142 | loss: 3.2308754920959473
Step 1142 | grad_norm: 2.34013032913208
Step 1142 | learning_rate: 1.548148148148148e-05
Step 1142 | epoch: 2.537777777777778
Step 1143 | loss: 3.1301729679107666
Step 1143 | grad_norm: 2.706835985183716
Step 1143 | learning_rate: 1.5407407407407408e-05
Step 1143 | epoch: 2.54
Step 1144 | loss: 2.7999916076660156
Step 1144 | grad_norm: 2.982649803161621
Step 1144 | learning_rate: 1.5333333333333334e-05
Step 1144 | epoch: 2.542222222222222
Step 1145 | loss: 2.4613285064697266
Step 1145 | grad_norm: 2.337935447692871
Step 1145 | learning_rate: 1.5259259259259258e-05
Step 1145 | epoch: 2.5444444444444443
Step 1146 | loss: 3.285985231399536
Step 1146 | grad_norm: 2.758255958557129
Step 1146 | learning_rate: 1.5185185185185186e-05
Step 1146 | epoch: 2.546666666666667
Step 1147 | loss: 3.5380046367645264
Step 1147 | grad_norm: 2.547149181365967
Step 1147 | learning_rate: 1.5111111111111112e-05
Step 1147 | epoch: 2.548888888888889
Step 1148 | loss: 3.1158664226531982
Step 1148 | grad_norm: 2.4824180603027344
Step 1148 | learning_rate: 1.5037037037037039e-05
Step 1148 | epoch: 2.551111111111111
Step 1149 | loss: 4.011398792266846
Step 1149 | grad_norm: 2.582505464553833
Step 1149 | learning_rate: 1.4962962962962965e-05
Step 1149 | epoch: 2.5533333333333332
Step 1150 | loss: 3.81546688079834
Step 1150 | grad_norm: 2.8536269664764404
Step 1150 | learning_rate: 1.4888888888888888e-05
Step 1150 | epoch: 2.5555555555555554
Step 1151 | loss: 2.987706422805786
Step 1151 | grad_norm: 2.2338571548461914
Step 1151 | learning_rate: 1.4814814814814815e-05
Step 1151 | epoch: 2.557777777777778
Step 1152 | loss: 3.6343472003936768
Step 1152 | grad_norm: 2.0553395748138428
Step 1152 | learning_rate: 1.4740740740740741e-05
Step 1152 | epoch: 2.56
Step 1153 | loss: 3.1538331508636475
Step 1153 | grad_norm: 2.242868185043335
Step 1153 | learning_rate: 1.4666666666666668e-05
Step 1153 | epoch: 2.562222222222222
Step 1154 | loss: 2.873579740524292
Step 1154 | grad_norm: 2.260287046432495
Step 1154 | learning_rate: 1.4592592592592594e-05
Step 1154 | epoch: 2.5644444444444443
Step 1155 | loss: 3.801999807357788
Step 1155 | grad_norm: 2.367851972579956
Step 1155 | learning_rate: 1.4518518518518521e-05
Step 1155 | epoch: 2.5666666666666664
Step 1156 | loss: 3.0827431678771973
Step 1156 | grad_norm: 2.4657623767852783
Step 1156 | learning_rate: 1.4444444444444444e-05
Step 1156 | epoch: 2.568888888888889
Step 1157 | loss: 2.210657835006714
Step 1157 | grad_norm: 2.2209999561309814
Step 1157 | learning_rate: 1.437037037037037e-05
Step 1157 | epoch: 2.571111111111111
Step 1158 | loss: 2.862675189971924
Step 1158 | grad_norm: 2.488640069961548
Step 1158 | learning_rate: 1.4296296296296297e-05
Step 1158 | epoch: 2.5733333333333333
Step 1159 | loss: 3.8244364261627197
Step 1159 | grad_norm: 2.792269468307495
Step 1159 | learning_rate: 1.4222222222222224e-05
Step 1159 | epoch: 2.575555555555556
Step 1160 | loss: 2.494981288909912
Step 1160 | grad_norm: 1.9745985269546509
Step 1160 | learning_rate: 1.4148148148148148e-05
Step 1160 | epoch: 2.5777777777777775
Step 1161 | loss: 3.2336513996124268
Step 1161 | grad_norm: 2.0657100677490234
Step 1161 | learning_rate: 1.4074074074074075e-05
Step 1161 | epoch: 2.58
Step 1162 | loss: 2.937793254852295
Step 1162 | grad_norm: 2.013854742050171
Step 1162 | learning_rate: 1.4000000000000001e-05
Step 1162 | epoch: 2.582222222222222
Step 1163 | loss: 3.5683326721191406
Step 1163 | grad_norm: 3.14341402053833
Step 1163 | learning_rate: 1.3925925925925926e-05
Step 1163 | epoch: 2.5844444444444443
Step 1164 | loss: 3.2590460777282715
Step 1164 | grad_norm: 2.5502936840057373
Step 1164 | learning_rate: 1.3851851851851853e-05
Step 1164 | epoch: 2.586666666666667
Step 1165 | loss: 2.927581310272217
Step 1165 | grad_norm: 2.2098848819732666
Step 1165 | learning_rate: 1.3777777777777778e-05
Step 1165 | epoch: 2.588888888888889
Step 1166 | loss: 2.760199785232544
Step 1166 | grad_norm: 1.919703483581543
Step 1166 | learning_rate: 1.3703703703703704e-05
Step 1166 | epoch: 2.591111111111111
Step 1167 | loss: 3.2789862155914307
Step 1167 | grad_norm: 3.1307272911071777
Step 1167 | learning_rate: 1.362962962962963e-05
Step 1167 | epoch: 2.5933333333333333
Step 1168 | loss: 3.882415771484375
Step 1168 | grad_norm: 2.732118606567383
Step 1168 | learning_rate: 1.3555555555555557e-05
Step 1168 | epoch: 2.5955555555555554
Step 1169 | loss: 4.234589099884033
Step 1169 | grad_norm: 3.2613525390625
Step 1169 | learning_rate: 1.348148148148148e-05
Step 1169 | epoch: 2.597777777777778
Step 1170 | loss: 3.543797731399536
Step 1170 | grad_norm: 2.784809112548828
Step 1170 | learning_rate: 1.3407407407407407e-05
Step 1170 | epoch: 2.6
Step 1171 | loss: 3.6994552612304688
Step 1171 | grad_norm: 2.4333927631378174
Step 1171 | learning_rate: 1.3333333333333333e-05
Step 1171 | epoch: 2.602222222222222
Step 1172 | loss: 2.980090856552124
Step 1172 | grad_norm: 2.746211051940918
Step 1172 | learning_rate: 1.325925925925926e-05
Step 1172 | epoch: 2.6044444444444443
Step 1173 | loss: 3.3147127628326416
Step 1173 | grad_norm: 2.2139792442321777
Step 1173 | learning_rate: 1.3185185185185186e-05
Step 1173 | epoch: 2.6066666666666665
Step 1174 | loss: 2.675524950027466
Step 1174 | grad_norm: 2.6372523307800293
Step 1174 | learning_rate: 1.3111111111111113e-05
Step 1174 | epoch: 2.608888888888889
Step 1175 | loss: 3.2002782821655273
Step 1175 | grad_norm: 3.0229461193084717
Step 1175 | learning_rate: 1.3037037037037036e-05
Step 1175 | epoch: 2.611111111111111
Step 1176 | loss: 3.812161922454834
Step 1176 | grad_norm: 2.2089264392852783
Step 1176 | learning_rate: 1.2962962962962962e-05
Step 1176 | epoch: 2.6133333333333333
Step 1177 | loss: 3.7091660499572754
Step 1177 | grad_norm: 2.8300416469573975
Step 1177 | learning_rate: 1.2888888888888889e-05
Step 1177 | epoch: 2.6155555555555554
Step 1178 | loss: 3.78695011138916
Step 1178 | grad_norm: 2.9692373275756836
Step 1178 | learning_rate: 1.2814814814814815e-05
Step 1178 | epoch: 2.6177777777777775
Step 1179 | loss: 3.950155019760132
Step 1179 | grad_norm: 2.5420637130737305
Step 1179 | learning_rate: 1.2740740740740742e-05
Step 1179 | epoch: 2.62
Step 1180 | loss: 3.0922553539276123
Step 1180 | grad_norm: 2.13384747505188
Step 1180 | learning_rate: 1.2666666666666668e-05
Step 1180 | epoch: 2.6222222222222222
Step 1181 | loss: 2.4831385612487793
Step 1181 | grad_norm: 3.1021671295166016
Step 1181 | learning_rate: 1.2592592592592592e-05
Step 1181 | epoch: 2.6244444444444444
Step 1182 | loss: 2.8259847164154053
Step 1182 | grad_norm: 1.8529033660888672
Step 1182 | learning_rate: 1.2518518518518518e-05
Step 1182 | epoch: 2.626666666666667
Step 1183 | loss: 3.4688196182250977
Step 1183 | grad_norm: 2.342665672302246
Step 1183 | learning_rate: 1.2444444444444445e-05
Step 1183 | epoch: 2.628888888888889
Step 1184 | loss: 3.4505701065063477
Step 1184 | grad_norm: 2.9437754154205322
Step 1184 | learning_rate: 1.2370370370370371e-05
Step 1184 | epoch: 2.631111111111111
Step 1185 | loss: 4.3469038009643555
Step 1185 | grad_norm: 2.9948954582214355
Step 1185 | learning_rate: 1.2296296296296298e-05
Step 1185 | epoch: 2.6333333333333333
Step 1186 | loss: 3.9154839515686035
Step 1186 | grad_norm: 3.127803325653076
Step 1186 | learning_rate: 1.2222222222222222e-05
Step 1186 | epoch: 2.6355555555555554
Step 1187 | loss: 3.0816876888275146
Step 1187 | grad_norm: 3.0864970684051514
Step 1187 | learning_rate: 1.2148148148148149e-05
Step 1187 | epoch: 2.637777777777778
Step 1188 | loss: 3.3935086727142334
Step 1188 | grad_norm: 2.3445210456848145
Step 1188 | learning_rate: 1.2074074074074075e-05
Step 1188 | epoch: 2.64
Step 1189 | loss: 3.821155071258545
Step 1189 | grad_norm: 2.3755407333374023
Step 1189 | learning_rate: 1.2e-05
Step 1189 | epoch: 2.6422222222222222
Step 1190 | loss: 3.2702372074127197
Step 1190 | grad_norm: 2.07519268989563
Step 1190 | learning_rate: 1.1925925925925927e-05
Step 1190 | epoch: 2.6444444444444444
Step 1191 | loss: 4.118494033813477
Step 1191 | grad_norm: 3.4394936561584473
Step 1191 | learning_rate: 1.1851851851851853e-05
Step 1191 | epoch: 2.6466666666666665
Step 1192 | loss: 3.2849526405334473
Step 1192 | grad_norm: 2.1472983360290527
Step 1192 | learning_rate: 1.1777777777777778e-05
Step 1192 | epoch: 2.648888888888889
Step 1193 | loss: 3.7280476093292236
Step 1193 | grad_norm: 3.1565194129943848
Step 1193 | learning_rate: 1.1703703703703705e-05
Step 1193 | epoch: 2.651111111111111
Step 1194 | loss: 3.3850619792938232
Step 1194 | grad_norm: 2.7567644119262695
Step 1194 | learning_rate: 1.1629629629629631e-05
Step 1194 | epoch: 2.6533333333333333
Step 1195 | loss: 4.7008843421936035
Step 1195 | grad_norm: 3.072646379470825
Step 1195 | learning_rate: 1.1555555555555556e-05
Step 1195 | epoch: 2.6555555555555554
Step 1196 | loss: 2.7742931842803955
Step 1196 | grad_norm: 1.9806545972824097
Step 1196 | learning_rate: 1.1481481481481482e-05
Step 1196 | epoch: 2.6577777777777776
Step 1197 | loss: 3.188889741897583
Step 1197 | grad_norm: 2.193512439727783
Step 1197 | learning_rate: 1.1407407407407409e-05
Step 1197 | epoch: 2.66
Step 1198 | loss: 3.9125735759735107
Step 1198 | grad_norm: 3.1204118728637695
Step 1198 | learning_rate: 1.1333333333333334e-05
Step 1198 | epoch: 2.6622222222222223
Step 1199 | loss: 3.8851840496063232
Step 1199 | grad_norm: 2.4843080043792725
Step 1199 | learning_rate: 1.125925925925926e-05
Step 1199 | epoch: 2.6644444444444444
Step 1200 | loss: 3.1143112182617188
Step 1200 | grad_norm: 2.596719980239868
Step 1200 | learning_rate: 1.1185185185185187e-05
Step 1200 | epoch: 2.6666666666666665
Step 1201 | loss: 3.662529468536377
Step 1201 | grad_norm: 2.9524900913238525
Step 1201 | learning_rate: 1.1111111111111112e-05
Step 1201 | epoch: 2.6688888888888886
Step 1202 | loss: 3.730067729949951
Step 1202 | grad_norm: 2.545517921447754
Step 1202 | learning_rate: 1.1037037037037038e-05
Step 1202 | epoch: 2.671111111111111
Step 1203 | loss: 4.064614295959473
Step 1203 | grad_norm: 2.7598068714141846
Step 1203 | learning_rate: 1.0962962962962963e-05
Step 1203 | epoch: 2.6733333333333333
Step 1204 | loss: 3.3676161766052246
Step 1204 | grad_norm: 1.9133747816085815
Step 1204 | learning_rate: 1.088888888888889e-05
Step 1204 | epoch: 2.6755555555555555
Step 1205 | loss: 2.6199724674224854
Step 1205 | grad_norm: 2.0506656169891357
Step 1205 | learning_rate: 1.0814814814814814e-05
Step 1205 | epoch: 2.677777777777778
Step 1206 | loss: 3.2128400802612305
Step 1206 | grad_norm: 2.015702724456787
Step 1206 | learning_rate: 1.074074074074074e-05
Step 1206 | epoch: 2.68
Step 1207 | loss: 3.581449270248413
Step 1207 | grad_norm: 2.1978049278259277
Step 1207 | learning_rate: 1.0666666666666667e-05
Step 1207 | epoch: 2.6822222222222223
Step 1208 | loss: 3.032198190689087
Step 1208 | grad_norm: 2.527876615524292
Step 1208 | learning_rate: 1.0592592592592592e-05
Step 1208 | epoch: 2.6844444444444444
Step 1209 | loss: 3.558204412460327
Step 1209 | grad_norm: 2.67429780960083
Step 1209 | learning_rate: 1.0518518518518519e-05
Step 1209 | epoch: 2.6866666666666665
Step 1210 | loss: 3.9198338985443115
Step 1210 | grad_norm: 2.803274393081665
Step 1210 | learning_rate: 1.0444444444444445e-05
Step 1210 | epoch: 2.688888888888889
Step 1211 | loss: 2.77573561668396
Step 1211 | grad_norm: 1.9255845546722412
Step 1211 | learning_rate: 1.037037037037037e-05
Step 1211 | epoch: 2.6911111111111112
Step 1212 | loss: 3.311343193054199
Step 1212 | grad_norm: 2.4814867973327637
Step 1212 | learning_rate: 1.0296296296296296e-05
Step 1212 | epoch: 2.6933333333333334
Step 1213 | loss: 3.187209129333496
Step 1213 | grad_norm: 2.2410237789154053
Step 1213 | learning_rate: 1.0222222222222223e-05
Step 1213 | epoch: 2.6955555555555555
Step 1214 | loss: 3.809926748275757
Step 1214 | grad_norm: 3.151516914367676
Step 1214 | learning_rate: 1.0148148148148148e-05
Step 1214 | epoch: 2.6977777777777776
Step 1215 | loss: 3.081437826156616
Step 1215 | grad_norm: 2.5216314792633057
Step 1215 | learning_rate: 1.0074074074074074e-05
Step 1215 | epoch: 2.7
Step 1216 | loss: 3.4202287197113037
Step 1216 | grad_norm: 2.2760815620422363
Step 1216 | learning_rate: 1e-05
Step 1216 | epoch: 2.7022222222222223
Step 1217 | loss: 3.31622314453125
Step 1217 | grad_norm: 2.9386420249938965
Step 1217 | learning_rate: 9.925925925925926e-06
Step 1217 | epoch: 2.7044444444444444
Step 1218 | loss: 3.624044895172119
Step 1218 | grad_norm: 3.058366537094116
Step 1218 | learning_rate: 9.851851851851852e-06
Step 1218 | epoch: 2.7066666666666666
Step 1219 | loss: 3.577085018157959
Step 1219 | grad_norm: 3.0164661407470703
Step 1219 | learning_rate: 9.777777777777779e-06
Step 1219 | epoch: 2.7088888888888887
Step 1220 | loss: 3.4035606384277344
Step 1220 | grad_norm: 2.4865002632141113
Step 1220 | learning_rate: 9.703703703703703e-06
Step 1220 | epoch: 2.7111111111111112
Step 1221 | loss: 3.044354200363159
Step 1221 | grad_norm: 2.361623764038086
Step 1221 | learning_rate: 9.62962962962963e-06
Step 1221 | epoch: 2.7133333333333334
Step 1222 | loss: 4.20228385925293
Step 1222 | grad_norm: 2.806419849395752
Step 1222 | learning_rate: 9.555555555555556e-06
Step 1222 | epoch: 2.7155555555555555
Step 1223 | loss: 3.0049359798431396
Step 1223 | grad_norm: 2.3570148944854736
Step 1223 | learning_rate: 9.481481481481481e-06
Step 1223 | epoch: 2.7177777777777776
Step 1224 | loss: 4.428682804107666
Step 1224 | grad_norm: 2.9339795112609863
Step 1224 | learning_rate: 9.407407407407408e-06
Step 1224 | epoch: 2.7199999999999998
Step 1225 | loss: 3.46523118019104
Step 1225 | grad_norm: 2.0933315753936768
Step 1225 | learning_rate: 9.333333333333334e-06
Step 1225 | epoch: 2.7222222222222223
Step 1226 | loss: 2.9181482791900635
Step 1226 | grad_norm: 2.073997974395752
Step 1226 | learning_rate: 9.259259259259259e-06
Step 1226 | epoch: 2.7244444444444444
Step 1227 | loss: 3.0472171306610107
Step 1227 | grad_norm: 2.5574774742126465
Step 1227 | learning_rate: 9.185185185185186e-06
Step 1227 | epoch: 2.7266666666666666
Step 1228 | loss: 3.81418776512146
Step 1228 | grad_norm: 2.8021676540374756
Step 1228 | learning_rate: 9.111111111111112e-06
Step 1228 | epoch: 2.728888888888889
Step 1229 | loss: 3.8244411945343018
Step 1229 | grad_norm: 2.703047037124634
Step 1229 | learning_rate: 9.037037037037037e-06
Step 1229 | epoch: 2.7311111111111113
Step 1230 | loss: 3.0804648399353027
Step 1230 | grad_norm: 3.229978561401367
Step 1230 | learning_rate: 8.962962962962963e-06
Step 1230 | epoch: 2.7333333333333334
Step 1231 | loss: 2.795819044113159
Step 1231 | grad_norm: 2.2629706859588623
Step 1231 | learning_rate: 8.88888888888889e-06
Step 1231 | epoch: 2.7355555555555555
Step 1232 | loss: 3.4860026836395264
Step 1232 | grad_norm: 3.5878207683563232
Step 1232 | learning_rate: 8.814814814814815e-06
Step 1232 | epoch: 2.7377777777777776
Step 1233 | loss: 3.5658884048461914
Step 1233 | grad_norm: 2.346026659011841
Step 1233 | learning_rate: 8.740740740740741e-06
Step 1233 | epoch: 2.74
Step 1234 | loss: 3.6990456581115723
Step 1234 | grad_norm: 3.5395126342773438
Step 1234 | learning_rate: 8.666666666666668e-06
Step 1234 | epoch: 2.7422222222222223
Step 1235 | loss: 3.2623465061187744
Step 1235 | grad_norm: 2.083258628845215
Step 1235 | learning_rate: 8.592592592592593e-06
Step 1235 | epoch: 2.7444444444444445
Step 1236 | loss: 3.049640655517578
Step 1236 | grad_norm: 1.9158926010131836
Step 1236 | learning_rate: 8.518518518518519e-06
Step 1236 | epoch: 2.7466666666666666
Step 1237 | loss: 2.6752257347106934
Step 1237 | grad_norm: 2.269296169281006
Step 1237 | learning_rate: 8.444444444444446e-06
Step 1237 | epoch: 2.7488888888888887
Step 1238 | loss: 3.7270846366882324
Step 1238 | grad_norm: 2.8484513759613037
Step 1238 | learning_rate: 8.37037037037037e-06
Step 1238 | epoch: 2.7511111111111113
Step 1239 | loss: 2.70794939994812
Step 1239 | grad_norm: 1.767863154411316
Step 1239 | learning_rate: 8.296296296296297e-06
Step 1239 | epoch: 2.7533333333333334
Step 1240 | loss: 2.850442886352539
Step 1240 | grad_norm: 3.069737195968628
Step 1240 | learning_rate: 8.222222222222223e-06
Step 1240 | epoch: 2.7555555555555555
Step 1241 | loss: 3.2778306007385254
Step 1241 | grad_norm: 3.167776346206665
Step 1241 | learning_rate: 8.14814814814815e-06
Step 1241 | epoch: 2.7577777777777777
Step 1242 | loss: 3.158747434616089
Step 1242 | grad_norm: 2.806899070739746
Step 1242 | learning_rate: 8.074074074074075e-06
Step 1242 | epoch: 2.76
Step 1243 | loss: 2.8387110233306885
Step 1243 | grad_norm: 2.6561598777770996
Step 1243 | learning_rate: 8.000000000000001e-06
Step 1243 | epoch: 2.7622222222222224
Step 1244 | loss: 3.144794464111328
Step 1244 | grad_norm: 2.501255512237549
Step 1244 | learning_rate: 7.925925925925926e-06
Step 1244 | epoch: 2.7644444444444445
Step 1245 | loss: 3.4322259426116943
Step 1245 | grad_norm: 2.8489603996276855
Step 1245 | learning_rate: 7.851851851851853e-06
Step 1245 | epoch: 2.7666666666666666
Step 1246 | loss: 4.062248229980469
Step 1246 | grad_norm: 2.840942859649658
Step 1246 | learning_rate: 7.777777777777777e-06
Step 1246 | epoch: 2.7688888888888887
Step 1247 | loss: 3.586562395095825
Step 1247 | grad_norm: 3.0372462272644043
Step 1247 | learning_rate: 7.703703703703704e-06
Step 1247 | epoch: 2.771111111111111
Step 1248 | loss: 3.073765516281128
Step 1248 | grad_norm: 3.381342649459839
Step 1248 | learning_rate: 7.629629629629629e-06
Step 1248 | epoch: 2.7733333333333334
Step 1249 | loss: 3.2634847164154053
Step 1249 | grad_norm: 3.6651697158813477
Step 1249 | learning_rate: 7.555555555555556e-06
Step 1249 | epoch: 2.7755555555555556
Step 1250 | loss: 3.662529468536377
Step 1250 | grad_norm: 2.6721906661987305
Step 1250 | learning_rate: 7.481481481481483e-06
Step 1250 | epoch: 2.7777777777777777
Step 1251 | loss: 3.6912662982940674
Step 1251 | grad_norm: 2.667747974395752
Step 1251 | learning_rate: 7.4074074074074075e-06
Step 1251 | epoch: 2.7800000000000002
Step 1252 | loss: 3.059312343597412
Step 1252 | grad_norm: 2.356184482574463
Step 1252 | learning_rate: 7.333333333333334e-06
Step 1252 | epoch: 2.7822222222222224
Step 1253 | loss: 3.638845443725586
Step 1253 | grad_norm: 2.401132583618164
Step 1253 | learning_rate: 7.2592592592592605e-06
Step 1253 | epoch: 2.7844444444444445
Step 1254 | loss: 3.4359898567199707
Step 1254 | grad_norm: 2.62019681930542
Step 1254 | learning_rate: 7.185185185185185e-06
Step 1254 | epoch: 2.7866666666666666
Step 1255 | loss: 2.9195003509521484
Step 1255 | grad_norm: 2.0583457946777344
Step 1255 | learning_rate: 7.111111111111112e-06
Step 1255 | epoch: 2.7888888888888888
Step 1256 | loss: 2.9580886363983154
Step 1256 | grad_norm: 2.5637407302856445
Step 1256 | learning_rate: 7.0370370370370375e-06
Step 1256 | epoch: 2.7911111111111113
Step 1257 | loss: 3.5506908893585205
Step 1257 | grad_norm: 3.0605087280273438
Step 1257 | learning_rate: 6.962962962962963e-06
Step 1257 | epoch: 2.7933333333333334
Step 1258 | loss: 3.8072025775909424
Step 1258 | grad_norm: 2.8758368492126465
Step 1258 | learning_rate: 6.888888888888889e-06
Step 1258 | epoch: 2.7955555555555556
Step 1259 | loss: 3.1298463344573975
Step 1259 | grad_norm: 3.047178030014038
Step 1259 | learning_rate: 6.814814814814815e-06
Step 1259 | epoch: 2.7977777777777777
Step 1260 | loss: 3.5422043800354004
Step 1260 | grad_norm: 2.6310923099517822
Step 1260 | learning_rate: 6.74074074074074e-06
Step 1260 | epoch: 2.8
Step 1261 | loss: 3.168686628341675
Step 1261 | grad_norm: 2.8685784339904785
Step 1261 | learning_rate: 6.666666666666667e-06
Step 1261 | epoch: 2.8022222222222224
Step 1262 | loss: 3.639024019241333
Step 1262 | grad_norm: 3.682851552963257
Step 1262 | learning_rate: 6.592592592592593e-06
Step 1262 | epoch: 2.8044444444444445
Step 1263 | loss: 2.772562026977539
Step 1263 | grad_norm: 2.616899251937866
Step 1263 | learning_rate: 6.518518518518518e-06
Step 1263 | epoch: 2.8066666666666666
Step 1264 | loss: 2.5762579441070557
Step 1264 | grad_norm: 1.9884870052337646
Step 1264 | learning_rate: 6.4444444444444445e-06
Step 1264 | epoch: 2.8088888888888888
Step 1265 | loss: 3.3577494621276855
Step 1265 | grad_norm: 2.9654898643493652
Step 1265 | learning_rate: 6.370370370370371e-06
Step 1265 | epoch: 2.811111111111111
Step 1266 | loss: 3.354670763015747
Step 1266 | grad_norm: 2.0943853855133057
Step 1266 | learning_rate: 6.296296296296296e-06
Step 1266 | epoch: 2.8133333333333335
Step 1267 | loss: 3.9356040954589844
Step 1267 | grad_norm: 2.8481452465057373
Step 1267 | learning_rate: 6.222222222222222e-06
Step 1267 | epoch: 2.8155555555555556
Step 1268 | loss: 3.918668508529663
Step 1268 | grad_norm: 2.795560121536255
Step 1268 | learning_rate: 6.148148148148149e-06
Step 1268 | epoch: 2.8177777777777777
Step 1269 | loss: 3.6386146545410156
Step 1269 | grad_norm: 2.981614589691162
Step 1269 | learning_rate: 6.0740740740740745e-06
Step 1269 | epoch: 2.82
Step 1270 | loss: 3.0078396797180176
Step 1270 | grad_norm: 3.3155341148376465
Step 1270 | learning_rate: 6e-06
Step 1270 | epoch: 2.822222222222222
Step 1271 | loss: 3.6637299060821533
Step 1271 | grad_norm: 2.899991512298584
Step 1271 | learning_rate: 5.925925925925927e-06
Step 1271 | epoch: 2.8244444444444445
Step 1272 | loss: 3.92167592048645
Step 1272 | grad_norm: 3.735163927078247
Step 1272 | learning_rate: 5.851851851851852e-06
Step 1272 | epoch: 2.8266666666666667
Step 1273 | loss: 2.5829412937164307
Step 1273 | grad_norm: 2.7508180141448975
Step 1273 | learning_rate: 5.777777777777778e-06
Step 1273 | epoch: 2.828888888888889
Step 1274 | loss: 3.527679920196533
Step 1274 | grad_norm: 3.004605531692505
Step 1274 | learning_rate: 5.7037037037037045e-06
Step 1274 | epoch: 2.8311111111111114
Step 1275 | loss: 3.5298385620117188
Step 1275 | grad_norm: 2.4643166065216064
Step 1275 | learning_rate: 5.62962962962963e-06
Step 1275 | epoch: 2.8333333333333335
Step 1276 | loss: 3.3596689701080322
Step 1276 | grad_norm: 2.5510685443878174
Step 1276 | learning_rate: 5.555555555555556e-06
Step 1276 | epoch: 2.8355555555555556
Step 1277 | loss: 3.112422466278076
Step 1277 | grad_norm: 2.9617388248443604
Step 1277 | learning_rate: 5.4814814814814815e-06
Step 1277 | epoch: 2.8377777777777777
Step 1278 | loss: 3.4022738933563232
Step 1278 | grad_norm: 2.665950059890747
Step 1278 | learning_rate: 5.407407407407407e-06
Step 1278 | epoch: 2.84
Step 1279 | loss: 3.4402685165405273
Step 1279 | grad_norm: 2.917358160018921
Step 1279 | learning_rate: 5.333333333333334e-06
Step 1279 | epoch: 2.8422222222222224
Step 1280 | loss: 3.220165491104126
Step 1280 | grad_norm: 2.3667495250701904
Step 1280 | learning_rate: 5.259259259259259e-06
Step 1280 | epoch: 2.8444444444444446
Step 1281 | loss: 3.3446123600006104
Step 1281 | grad_norm: 2.133024215698242
Step 1281 | learning_rate: 5.185185185185185e-06
Step 1281 | epoch: 2.8466666666666667
Step 1282 | loss: 4.127958297729492
Step 1282 | grad_norm: 5.046187400817871
Step 1282 | learning_rate: 5.1111111111111115e-06
Step 1282 | epoch: 2.848888888888889
Step 1283 | loss: 3.9104740619659424
Step 1283 | grad_norm: 3.710664749145508
Step 1283 | learning_rate: 5.037037037037037e-06
Step 1283 | epoch: 2.851111111111111
Step 1284 | loss: 3.886570930480957
Step 1284 | grad_norm: 2.9060518741607666
Step 1284 | learning_rate: 4.962962962962963e-06
Step 1284 | epoch: 2.8533333333333335
Step 1285 | loss: 3.3809919357299805
Step 1285 | grad_norm: 2.722999095916748
Step 1285 | learning_rate: 4.888888888888889e-06
Step 1285 | epoch: 2.8555555555555556
Step 1286 | loss: 2.771294593811035
Step 1286 | grad_norm: 3.093087673187256
Step 1286 | learning_rate: 4.814814814814815e-06
Step 1286 | epoch: 2.8577777777777778
Step 1287 | loss: 3.591801166534424
Step 1287 | grad_norm: 2.727210760116577
Step 1287 | learning_rate: 4.740740740740741e-06
Step 1287 | epoch: 2.86
Step 1288 | loss: 3.8414254188537598
Step 1288 | grad_norm: 2.4697203636169434
Step 1288 | learning_rate: 4.666666666666667e-06
Step 1288 | epoch: 2.862222222222222
Step 1289 | loss: 3.0484395027160645
Step 1289 | grad_norm: 2.3626677989959717
Step 1289 | learning_rate: 4.592592592592593e-06
Step 1289 | epoch: 2.8644444444444446
Step 1290 | loss: 3.4326605796813965
Step 1290 | grad_norm: 2.408665418624878
Step 1290 | learning_rate: 4.5185185185185185e-06
Step 1290 | epoch: 2.8666666666666667
Step 1291 | loss: 3.6606574058532715
Step 1291 | grad_norm: 2.6827809810638428
Step 1291 | learning_rate: 4.444444444444445e-06
Step 1291 | epoch: 2.868888888888889
Step 1292 | loss: 3.2598321437835693
Step 1292 | grad_norm: 4.013312339782715
Step 1292 | learning_rate: 4.370370370370371e-06
Step 1292 | epoch: 2.871111111111111
Step 1293 | loss: 3.0444424152374268
Step 1293 | grad_norm: 2.1151950359344482
Step 1293 | learning_rate: 4.296296296296296e-06
Step 1293 | epoch: 2.873333333333333
Step 1294 | loss: 3.653738260269165
Step 1294 | grad_norm: 3.646028995513916
Step 1294 | learning_rate: 4.222222222222223e-06
Step 1294 | epoch: 2.8755555555555556
Step 1295 | loss: 3.3065974712371826
Step 1295 | grad_norm: 2.6282248497009277
Step 1295 | learning_rate: 4.1481481481481485e-06
Step 1295 | epoch: 2.8777777777777778
Step 1296 | loss: 4.216118812561035
Step 1296 | grad_norm: 2.864680051803589
Step 1296 | learning_rate: 4.074074074074075e-06
Step 1296 | epoch: 2.88
Step 1297 | loss: 2.4838194847106934
Step 1297 | grad_norm: 2.317898750305176
Step 1297 | learning_rate: 4.000000000000001e-06
Step 1297 | epoch: 2.8822222222222225
Step 1298 | loss: 2.911144256591797
Step 1298 | grad_norm: 2.201385021209717
Step 1298 | learning_rate: 3.925925925925926e-06
Step 1298 | epoch: 2.8844444444444446
Step 1299 | loss: 2.920236825942993
Step 1299 | grad_norm: 2.3714406490325928
Step 1299 | learning_rate: 3.851851851851852e-06
Step 1299 | epoch: 2.8866666666666667
Step 1300 | loss: 3.718935489654541
Step 1300 | grad_norm: 2.4937829971313477
Step 1300 | learning_rate: 3.777777777777778e-06
Step 1300 | epoch: 2.888888888888889
Step 1301 | loss: 3.0148062705993652
Step 1301 | grad_norm: 2.2763285636901855
Step 1301 | learning_rate: 3.7037037037037037e-06
Step 1301 | epoch: 2.891111111111111
Step 1302 | loss: 3.5908665657043457
Step 1302 | grad_norm: 4.0664381980896
Step 1302 | learning_rate: 3.6296296296296302e-06
Step 1302 | epoch: 2.8933333333333335
Step 1303 | loss: 3.211524486541748
Step 1303 | grad_norm: 2.886244535446167
Step 1303 | learning_rate: 3.555555555555556e-06
Step 1303 | epoch: 2.8955555555555557
Step 1304 | loss: 3.5463039875030518
Step 1304 | grad_norm: 2.730160713195801
Step 1304 | learning_rate: 3.4814814814814816e-06
Step 1304 | epoch: 2.897777777777778
Step 1305 | loss: 3.6456542015075684
Step 1305 | grad_norm: 2.5909740924835205
Step 1305 | learning_rate: 3.4074074074074077e-06
Step 1305 | epoch: 2.9
Step 1306 | loss: 3.0755720138549805
Step 1306 | grad_norm: 2.303400993347168
Step 1306 | learning_rate: 3.3333333333333333e-06
Step 1306 | epoch: 2.902222222222222
Step 1307 | loss: 4.139537811279297
Step 1307 | grad_norm: 2.6599390506744385
Step 1307 | learning_rate: 3.259259259259259e-06
Step 1307 | epoch: 2.9044444444444446
Step 1308 | loss: 3.2085819244384766
Step 1308 | grad_norm: 3.016690969467163
Step 1308 | learning_rate: 3.1851851851851855e-06
Step 1308 | epoch: 2.9066666666666667
Step 1309 | loss: 3.320582628250122
Step 1309 | grad_norm: 3.219053268432617
Step 1309 | learning_rate: 3.111111111111111e-06
Step 1309 | epoch: 2.908888888888889
Step 1310 | loss: 3.6496732234954834
Step 1310 | grad_norm: 2.498945951461792
Step 1310 | learning_rate: 3.0370370370370372e-06
Step 1310 | epoch: 2.911111111111111
Step 1311 | loss: 3.0580577850341797
Step 1311 | grad_norm: 2.4235076904296875
Step 1311 | learning_rate: 2.9629629629629633e-06
Step 1311 | epoch: 2.913333333333333
Step 1312 | loss: 3.201075315475464
Step 1312 | grad_norm: 3.4154019355773926
Step 1312 | learning_rate: 2.888888888888889e-06
Step 1312 | epoch: 2.9155555555555557
Step 1313 | loss: 2.972416400909424
Step 1313 | grad_norm: 2.139092206954956
Step 1313 | learning_rate: 2.814814814814815e-06
Step 1313 | epoch: 2.917777777777778
Step 1314 | loss: 3.2804856300354004
Step 1314 | grad_norm: 2.283259630203247
Step 1314 | learning_rate: 2.7407407407407407e-06
Step 1314 | epoch: 2.92
Step 1315 | loss: 3.1587531566619873
Step 1315 | grad_norm: 2.9365546703338623
Step 1315 | learning_rate: 2.666666666666667e-06
Step 1315 | epoch: 2.9222222222222225
Step 1316 | loss: 3.7576146125793457
Step 1316 | grad_norm: 2.5264909267425537
Step 1316 | learning_rate: 2.5925925925925925e-06
Step 1316 | epoch: 2.924444444444444
Step 1317 | loss: 4.152032852172852
Step 1317 | grad_norm: 3.3023202419281006
Step 1317 | learning_rate: 2.5185185185185186e-06
Step 1317 | epoch: 2.9266666666666667
Step 1318 | loss: 3.9205145835876465
Step 1318 | grad_norm: 3.1355135440826416
Step 1318 | learning_rate: 2.4444444444444447e-06
Step 1318 | epoch: 2.928888888888889
Step 1319 | loss: 2.680448055267334
Step 1319 | grad_norm: 2.1677300930023193
Step 1319 | learning_rate: 2.3703703703703703e-06
Step 1319 | epoch: 2.931111111111111
Step 1320 | loss: 2.9179844856262207
Step 1320 | grad_norm: 2.7010512351989746
Step 1320 | learning_rate: 2.2962962962962964e-06
Step 1320 | epoch: 2.9333333333333336
Step 1321 | loss: 3.9785315990448
Step 1321 | grad_norm: 3.1576995849609375
Step 1321 | learning_rate: 2.2222222222222225e-06
Step 1321 | epoch: 2.9355555555555557
Step 1322 | loss: 3.5418789386749268
Step 1322 | grad_norm: 2.5122973918914795
Step 1322 | learning_rate: 2.148148148148148e-06
Step 1322 | epoch: 2.937777777777778
Step 1323 | loss: 3.292184829711914
Step 1323 | grad_norm: 3.094003200531006
Step 1323 | learning_rate: 2.0740740740740742e-06
Step 1323 | epoch: 2.94
Step 1324 | loss: 3.81740665435791
Step 1324 | grad_norm: 2.908055305480957
Step 1324 | learning_rate: 2.0000000000000003e-06
Step 1324 | epoch: 2.942222222222222
Step 1325 | loss: 3.313063621520996
Step 1325 | grad_norm: 2.05222749710083
Step 1325 | learning_rate: 1.925925925925926e-06
Step 1325 | epoch: 2.9444444444444446
Step 1326 | loss: 3.1664421558380127
Step 1326 | grad_norm: 2.161068916320801
Step 1326 | learning_rate: 1.8518518518518519e-06
Step 1326 | epoch: 2.9466666666666668
Step 1327 | loss: 3.5031111240386963
Step 1327 | grad_norm: 2.837129592895508
Step 1327 | learning_rate: 1.777777777777778e-06
Step 1327 | epoch: 2.948888888888889
Step 1328 | loss: 3.714038133621216
Step 1328 | grad_norm: 2.9992427825927734
Step 1328 | learning_rate: 1.7037037037037038e-06
Step 1328 | epoch: 2.951111111111111
Step 1329 | loss: 3.126864433288574
Step 1329 | grad_norm: 2.733675479888916
Step 1329 | learning_rate: 1.6296296296296295e-06
Step 1329 | epoch: 2.953333333333333
Step 1330 | loss: 4.3540120124816895
Step 1330 | grad_norm: 2.9092605113983154
Step 1330 | learning_rate: 1.5555555555555556e-06
Step 1330 | epoch: 2.9555555555555557
Step 1331 | loss: 3.311990737915039
Step 1331 | grad_norm: 3.815133810043335
Step 1331 | learning_rate: 1.4814814814814817e-06
Step 1331 | epoch: 2.957777777777778
Step 1332 | loss: 2.785590887069702
Step 1332 | grad_norm: 2.6638872623443604
Step 1332 | learning_rate: 1.4074074074074075e-06
Step 1332 | epoch: 2.96
Step 1333 | loss: 3.9361732006073
Step 1333 | grad_norm: 3.0294322967529297
Step 1333 | learning_rate: 1.3333333333333334e-06
Step 1333 | epoch: 2.962222222222222
Step 1334 | loss: 2.96098256111145
Step 1334 | grad_norm: 2.5858190059661865
Step 1334 | learning_rate: 1.2592592592592593e-06
Step 1334 | epoch: 2.964444444444444
Step 1335 | loss: 3.5563437938690186
Step 1335 | grad_norm: 2.816342353820801
Step 1335 | learning_rate: 1.1851851851851852e-06
Step 1335 | epoch: 2.966666666666667
Step 1336 | loss: 2.9389986991882324
Step 1336 | grad_norm: 2.0544021129608154
Step 1336 | learning_rate: 1.1111111111111112e-06
Step 1336 | epoch: 2.968888888888889
Step 1337 | loss: 3.549516439437866
Step 1337 | grad_norm: 2.827728509902954
Step 1337 | learning_rate: 1.0370370370370371e-06
Step 1337 | epoch: 2.971111111111111
Step 1338 | loss: 3.00911021232605
Step 1338 | grad_norm: 2.313354015350342
Step 1338 | learning_rate: 9.62962962962963e-07
Step 1338 | epoch: 2.9733333333333336
Step 1339 | loss: 2.7333455085754395
Step 1339 | grad_norm: 2.719463348388672
Step 1339 | learning_rate: 8.88888888888889e-07
Step 1339 | epoch: 2.9755555555555553
Step 1340 | loss: 3.1250858306884766
Step 1340 | grad_norm: 2.7301647663116455
Step 1340 | learning_rate: 8.148148148148147e-07
Step 1340 | epoch: 2.977777777777778
Step 1341 | loss: 3.3260338306427
Step 1341 | grad_norm: 2.6824305057525635
Step 1341 | learning_rate: 7.407407407407408e-07
Step 1341 | epoch: 2.98
Step 1342 | loss: 3.6967101097106934
Step 1342 | grad_norm: 2.9669337272644043
Step 1342 | learning_rate: 6.666666666666667e-07
Step 1342 | epoch: 2.982222222222222
Step 1343 | loss: 3.9624106884002686
Step 1343 | grad_norm: 3.209395170211792
Step 1343 | learning_rate: 5.925925925925926e-07
Step 1343 | epoch: 2.9844444444444447
Step 1344 | loss: 4.40482759475708
Step 1344 | grad_norm: 3.252636432647705
Step 1344 | learning_rate: 5.185185185185186e-07
Step 1344 | epoch: 2.986666666666667
Step 1345 | loss: 3.022459030151367
Step 1345 | grad_norm: 2.7256062030792236
Step 1345 | learning_rate: 4.444444444444445e-07
Step 1345 | epoch: 2.988888888888889
Step 1346 | loss: 3.2362546920776367
Step 1346 | grad_norm: 2.943183660507202
Step 1346 | learning_rate: 3.703703703703704e-07
Step 1346 | epoch: 2.991111111111111
Step 1347 | loss: 3.7944233417510986
Step 1347 | grad_norm: 2.778451681137085
Step 1347 | learning_rate: 2.962962962962963e-07
Step 1347 | epoch: 2.993333333333333
Step 1348 | loss: 3.347296714782715
Step 1348 | grad_norm: 3.027120590209961
Step 1348 | learning_rate: 2.2222222222222224e-07
Step 1348 | epoch: 2.9955555555555557
Step 1349 | loss: 2.9553582668304443
Step 1349 | grad_norm: 2.865182399749756
Step 1349 | learning_rate: 1.4814814814814815e-07
Step 1349 | epoch: 2.997777777777778
Step 1350 | loss: 2.4737749099731445
Step 1350 | grad_norm: 2.6313743591308594
Step 1350 | learning_rate: 7.407407407407407e-08
Step 1350 | epoch: 3.0
Step 1350 | train_runtime: 53.0954
Step 1350 | train_samples_per_second: 50.852
Step 1350 | train_steps_per_second: 25.426
Step 1350 | total_flos: 57870990968832.0
Step 1350 | train_loss: 3.471693611498232
Step 1350 | epoch: 3.0