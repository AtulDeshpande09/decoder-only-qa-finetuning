Step 10 | loss: 2.4239349365234375
Step 10 | grad_norm: 0.9019245505332947
Step 10 | learning_rate: 9.734513274336283e-05
Step 10 | epoch: 0.08888888888888889
Step 20 | loss: 2.0611236572265623
Step 20 | grad_norm: 1.121282935142517
Step 20 | learning_rate: 9.43952802359882e-05
Step 20 | epoch: 0.17777777777777778
Step 30 | loss: 2.001886177062988
Step 30 | grad_norm: 1.53823721408844
Step 30 | learning_rate: 9.144542772861357e-05
Step 30 | epoch: 0.26666666666666666
Step 40 | loss: 1.8893497467041016
Step 40 | grad_norm: 1.4831281900405884
Step 40 | learning_rate: 8.849557522123895e-05
Step 40 | epoch: 0.35555555555555557
Step 50 | loss: 1.8010990142822265
Step 50 | grad_norm: 1.8822264671325684
Step 50 | learning_rate: 8.554572271386431e-05
Step 50 | epoch: 0.4444444444444444
Step 60 | loss: 1.7293317794799805
Step 60 | grad_norm: 1.1960279941558838
Step 60 | learning_rate: 8.259587020648968e-05
Step 60 | epoch: 0.5333333333333333
Step 70 | loss: 1.639145278930664
Step 70 | grad_norm: 1.6334761381149292
Step 70 | learning_rate: 7.964601769911504e-05
Step 70 | epoch: 0.6222222222222222
Step 80 | loss: 1.6558385848999024
Step 80 | grad_norm: 1.3762482404708862
Step 80 | learning_rate: 7.669616519174043e-05
Step 80 | epoch: 0.7111111111111111
Step 90 | loss: 1.7157558441162108
Step 90 | grad_norm: 1.6297342777252197
Step 90 | learning_rate: 7.374631268436578e-05
Step 90 | epoch: 0.8
Step 100 | loss: 1.7265335083007813
Step 100 | grad_norm: 1.3107000589370728
Step 100 | learning_rate: 7.079646017699115e-05
Step 100 | epoch: 0.8888888888888888
Step 110 | loss: 1.6326244354248047
Step 110 | grad_norm: 1.570945143699646
Step 110 | learning_rate: 6.784660766961653e-05
Step 110 | epoch: 0.9777777777777777
Step 120 | loss: 1.608256721496582
Step 120 | grad_norm: 1.184063196182251
Step 120 | learning_rate: 6.48967551622419e-05
Step 120 | epoch: 1.0622222222222222
Step 130 | loss: 1.5070938110351562
Step 130 | grad_norm: 1.4472403526306152
Step 130 | learning_rate: 6.194690265486725e-05
Step 130 | epoch: 1.1511111111111112
Step 140 | loss: 1.617960548400879
Step 140 | grad_norm: 1.4832816123962402
Step 140 | learning_rate: 5.899705014749263e-05
Step 140 | epoch: 1.24
Step 150 | loss: 1.535500431060791
Step 150 | grad_norm: 1.192920207977295
Step 150 | learning_rate: 5.6047197640118e-05
Step 150 | epoch: 1.3288888888888888
Step 160 | loss: 1.5671370506286622
Step 160 | grad_norm: 1.5446281433105469
Step 160 | learning_rate: 5.309734513274337e-05
Step 160 | epoch: 1.4177777777777778
Step 170 | loss: 1.5485933303833008
Step 170 | grad_norm: 1.406619668006897
Step 170 | learning_rate: 5.014749262536873e-05
Step 170 | epoch: 1.5066666666666668
Step 180 | loss: 1.6255035400390625
Step 180 | grad_norm: 1.8343878984451294
Step 180 | learning_rate: 4.71976401179941e-05
Step 180 | epoch: 1.5955555555555554
Step 190 | loss: 1.6242404937744142
Step 190 | grad_norm: 1.4340215921401978
Step 190 | learning_rate: 4.4247787610619477e-05
Step 190 | epoch: 1.6844444444444444
Step 200 | loss: 1.669744873046875
Step 200 | grad_norm: 1.7748016119003296
Step 200 | learning_rate: 4.129793510324484e-05
Step 200 | epoch: 1.7733333333333334
Step 210 | loss: 1.590914821624756
Step 210 | grad_norm: 1.7036384344100952
Step 210 | learning_rate: 3.834808259587021e-05
Step 210 | epoch: 1.8622222222222222
Step 220 | loss: 1.5886670112609864
Step 220 | grad_norm: 1.700918436050415
Step 220 | learning_rate: 3.5398230088495574e-05
Step 220 | epoch: 1.951111111111111
Step 230 | loss: 1.599790382385254
Step 230 | grad_norm: 1.6613085269927979
Step 230 | learning_rate: 3.244837758112095e-05
Step 230 | epoch: 2.0355555555555553
Step 240 | loss: 1.5886404037475585
Step 240 | grad_norm: 1.5643587112426758
Step 240 | learning_rate: 2.9498525073746314e-05
Step 240 | epoch: 2.1244444444444444
Step 250 | loss: 1.5508305549621582
Step 250 | grad_norm: 1.6169902086257935
Step 250 | learning_rate: 2.6548672566371686e-05
Step 250 | epoch: 2.2133333333333334
Step 260 | loss: 1.4609228134155274
Step 260 | grad_norm: 1.8486607074737549
Step 260 | learning_rate: 2.359882005899705e-05
Step 260 | epoch: 2.3022222222222224
Step 270 | loss: 1.5282536506652833
Step 270 | grad_norm: 1.6459141969680786
Step 270 | learning_rate: 2.064896755162242e-05
Step 270 | epoch: 2.391111111111111
Step 280 | loss: 1.5835105895996093
Step 280 | grad_norm: 1.6243653297424316
Step 280 | learning_rate: 1.7699115044247787e-05
Step 280 | epoch: 2.48
Step 290 | loss: 1.4304046630859375
Step 290 | grad_norm: 1.6301909685134888
Step 290 | learning_rate: 1.4749262536873157e-05
Step 290 | epoch: 2.568888888888889
Step 300 | loss: 1.4831942558288573
Step 300 | grad_norm: 1.64620840549469
Step 300 | learning_rate: 1.1799410029498525e-05
Step 300 | epoch: 2.6577777777777776
Step 310 | loss: 1.6185827255249023
Step 310 | grad_norm: 1.6697567701339722
Step 310 | learning_rate: 8.849557522123894e-06
Step 310 | epoch: 2.7466666666666666
Step 320 | loss: 1.3808584213256836
Step 320 | grad_norm: 1.626280665397644
Step 320 | learning_rate: 5.899705014749263e-06
Step 320 | epoch: 2.8355555555555556
Step 330 | loss: 1.5853233337402344
Step 330 | grad_norm: 1.4787206649780273
Step 330 | learning_rate: 2.9498525073746313e-06
Step 330 | epoch: 2.924444444444444
Step 339 | train_runtime: 923.4229
Step 339 | train_samples_per_second: 2.924
Step 339 | train_steps_per_second: 0.367
Step 339 | total_flos: 1799409038929920.0
Step 339 | train_loss: 1.650361485889176
Step 339 | epoch: 3.0
