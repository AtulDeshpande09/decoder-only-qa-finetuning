Step 1 | loss: 1.7420353889465332
Step 1 | grad_norm: 3.3338358402252197
Step 1 | learning_rate: 0.0001
Step 1 | epoch: 0.008888888888888889
Step 2 | loss: 1.7559771537780762
Step 2 | grad_norm: 2.2733678817749023
Step 2 | learning_rate: 9.970501474926254e-05
Step 2 | epoch: 0.017777777777777778
Step 3 | loss: 1.7065458297729492
Step 3 | grad_norm: 2.9404096603393555
Step 3 | learning_rate: 9.941002949852508e-05
Step 3 | epoch: 0.02666666666666667
Step 4 | loss: 1.605485200881958
Step 4 | grad_norm: 2.7470483779907227
Step 4 | learning_rate: 9.911504424778762e-05
Step 4 | epoch: 0.035555555555555556
Step 5 | loss: 1.5056989192962646
Step 5 | grad_norm: 2.8767378330230713
Step 5 | learning_rate: 9.882005899705014e-05
Step 5 | epoch: 0.044444444444444446
Step 6 | loss: 1.4369817972183228
Step 6 | grad_norm: 3.216629981994629
Step 6 | learning_rate: 9.85250737463127e-05
Step 6 | epoch: 0.05333333333333334
Step 7 | loss: 1.2668681144714355
Step 7 | grad_norm: 3.4129693508148193
Step 7 | learning_rate: 9.823008849557522e-05
Step 7 | epoch: 0.06222222222222222
Step 8 | loss: 1.3485735654830933
Step 8 | grad_norm: 4.275066375732422
Step 8 | learning_rate: 9.793510324483777e-05
Step 8 | epoch: 0.07111111111111111
Step 9 | loss: 1.3999534845352173
Step 9 | grad_norm: 4.233871936798096
Step 9 | learning_rate: 9.76401179941003e-05
Step 9 | epoch: 0.08
Step 10 | loss: 1.171308159828186
Step 10 | grad_norm: 3.935659646987915
Step 10 | learning_rate: 9.734513274336283e-05
Step 10 | epoch: 0.08888888888888889
Step 11 | loss: 1.437175989151001
Step 11 | grad_norm: 6.153777599334717
Step 11 | learning_rate: 9.705014749262537e-05
Step 11 | epoch: 0.09777777777777778
Step 12 | loss: 1.1567401885986328
Step 12 | grad_norm: 4.505079746246338
Step 12 | learning_rate: 9.675516224188791e-05
Step 12 | epoch: 0.10666666666666667
Step 13 | loss: 1.245755672454834
Step 13 | grad_norm: 5.309363842010498
Step 13 | learning_rate: 9.646017699115044e-05
Step 13 | epoch: 0.11555555555555555
Step 14 | loss: 1.258330225944519
Step 14 | grad_norm: 5.3352766036987305
Step 14 | learning_rate: 9.616519174041299e-05
Step 14 | epoch: 0.12444444444444444
Step 15 | loss: 1.2971742153167725
Step 15 | grad_norm: 4.055401802062988
Step 15 | learning_rate: 9.587020648967551e-05
Step 15 | epoch: 0.13333333333333333
Step 16 | loss: 1.034523606300354
Step 16 | grad_norm: 3.4597976207733154
Step 16 | learning_rate: 9.557522123893806e-05
Step 16 | epoch: 0.14222222222222222
Step 17 | loss: 1.2448726892471313
Step 17 | grad_norm: 4.12960958480835
Step 17 | learning_rate: 9.528023598820059e-05
Step 17 | epoch: 0.1511111111111111
Step 18 | loss: 1.2311034202575684
Step 18 | grad_norm: 3.644775390625
Step 18 | learning_rate: 9.498525073746313e-05
Step 18 | epoch: 0.16
Step 19 | loss: 0.9000757932662964
Step 19 | grad_norm: 2.6259312629699707
Step 19 | learning_rate: 9.469026548672566e-05
Step 19 | epoch: 0.1688888888888889
Step 20 | loss: 1.253853678703308
Step 20 | grad_norm: 4.078970432281494
Step 20 | learning_rate: 9.43952802359882e-05
Step 20 | epoch: 0.17777777777777778
Step 21 | loss: 1.4366346597671509
Step 21 | grad_norm: 4.216625213623047
Step 21 | learning_rate: 9.410029498525074e-05
Step 21 | epoch: 0.18666666666666668
Step 22 | loss: 1.0358352661132812
Step 22 | grad_norm: 3.289872407913208
Step 22 | learning_rate: 9.380530973451328e-05
Step 22 | epoch: 0.19555555555555557
Step 23 | loss: 1.0822008848190308
Step 23 | grad_norm: 3.242924928665161
Step 23 | learning_rate: 9.351032448377582e-05
Step 23 | epoch: 0.20444444444444446
Step 24 | loss: 1.1972934007644653
Step 24 | grad_norm: 3.381570339202881
Step 24 | learning_rate: 9.321533923303836e-05
Step 24 | epoch: 0.21333333333333335
Step 25 | loss: 1.153757095336914
Step 25 | grad_norm: 3.9830524921417236
Step 25 | learning_rate: 9.29203539823009e-05
Step 25 | epoch: 0.2222222222222222
Step 26 | loss: 1.2084475755691528
Step 26 | grad_norm: 3.756699800491333
Step 26 | learning_rate: 9.262536873156342e-05
Step 26 | epoch: 0.2311111111111111
Step 27 | loss: 1.1431564092636108
Step 27 | grad_norm: 3.6001546382904053
Step 27 | learning_rate: 9.233038348082597e-05
Step 27 | epoch: 0.24
Step 28 | loss: 1.0241649150848389
Step 28 | grad_norm: 3.563141107559204
Step 28 | learning_rate: 9.20353982300885e-05
Step 28 | epoch: 0.24888888888888888
Step 29 | loss: 1.2388148307800293
Step 29 | grad_norm: 4.894481182098389
Step 29 | learning_rate: 9.174041297935103e-05
Step 29 | epoch: 0.2577777777777778
Step 30 | loss: 1.411401391029358
Step 30 | grad_norm: 7.092138290405273
Step 30 | learning_rate: 9.144542772861357e-05
Step 30 | epoch: 0.26666666666666666
Step 31 | loss: 1.2497366666793823
Step 31 | grad_norm: 4.214211463928223
Step 31 | learning_rate: 9.115044247787611e-05
Step 31 | epoch: 0.27555555555555555
Step 32 | loss: 1.2979227304458618
Step 32 | grad_norm: 4.018912315368652
Step 32 | learning_rate: 9.085545722713865e-05
Step 32 | epoch: 0.28444444444444444
Step 33 | loss: 1.3484013080596924
Step 33 | grad_norm: 4.211479187011719
Step 33 | learning_rate: 9.056047197640119e-05
Step 33 | epoch: 0.29333333333333333
Step 34 | loss: 1.258545994758606
Step 34 | grad_norm: 4.13679313659668
Step 34 | learning_rate: 9.026548672566371e-05
Step 34 | epoch: 0.3022222222222222
Step 35 | loss: 1.2555392980575562
Step 35 | grad_norm: 3.890781879425049
Step 35 | learning_rate: 8.997050147492626e-05
Step 35 | epoch: 0.3111111111111111
Step 36 | loss: 1.1052358150482178
Step 36 | grad_norm: 4.503800868988037
Step 36 | learning_rate: 8.96755162241888e-05
Step 36 | epoch: 0.32
Step 37 | loss: 1.3560733795166016
Step 37 | grad_norm: 4.3952317237854
Step 37 | learning_rate: 8.938053097345133e-05
Step 37 | epoch: 0.3288888888888889
Step 38 | loss: 1.3048114776611328
Step 38 | grad_norm: 4.503504753112793
Step 38 | learning_rate: 8.908554572271388e-05
Step 38 | epoch: 0.3377777777777778
Step 39 | loss: 0.9575071334838867
Step 39 | grad_norm: 3.5347354412078857
Step 39 | learning_rate: 8.87905604719764e-05
Step 39 | epoch: 0.3466666666666667
Step 40 | loss: 1.176257848739624
Step 40 | grad_norm: 4.063342571258545
Step 40 | learning_rate: 8.849557522123895e-05
Step 40 | epoch: 0.35555555555555557
Step 41 | loss: 0.9948554039001465
Step 41 | grad_norm: 4.105808734893799
Step 41 | learning_rate: 8.820058997050148e-05
Step 41 | epoch: 0.36444444444444446
Step 42 | loss: 0.945576012134552
Step 42 | grad_norm: 4.411929607391357
Step 42 | learning_rate: 8.790560471976402e-05
Step 42 | epoch: 0.37333333333333335
Step 43 | loss: 1.1920257806777954
Step 43 | grad_norm: 3.5663206577301025
Step 43 | learning_rate: 8.761061946902655e-05
Step 43 | epoch: 0.38222222222222224
Step 44 | loss: 1.0226911306381226
Step 44 | grad_norm: 3.7314977645874023
Step 44 | learning_rate: 8.731563421828909e-05
Step 44 | epoch: 0.39111111111111113
Step 45 | loss: 1.2915189266204834
Step 45 | grad_norm: 4.097662925720215
Step 45 | learning_rate: 8.702064896755162e-05
Step 45 | epoch: 0.4
Step 46 | loss: 1.352358102798462
Step 46 | grad_norm: 3.963235855102539
Step 46 | learning_rate: 8.672566371681417e-05
Step 46 | epoch: 0.4088888888888889
Step 47 | loss: 1.2254174947738647
Step 47 | grad_norm: 4.999230861663818
Step 47 | learning_rate: 8.64306784660767e-05
Step 47 | epoch: 0.4177777777777778
Step 48 | loss: 1.4119033813476562
Step 48 | grad_norm: 4.062812328338623
Step 48 | learning_rate: 8.613569321533924e-05
Step 48 | epoch: 0.4266666666666667
Step 49 | loss: 1.1291192770004272
Step 49 | grad_norm: 4.003264904022217
Step 49 | learning_rate: 8.584070796460177e-05
Step 49 | epoch: 0.43555555555555553
Step 50 | loss: 0.9308962225914001
Step 50 | grad_norm: 4.4854350090026855
Step 50 | learning_rate: 8.554572271386431e-05
Step 50 | epoch: 0.4444444444444444
Step 51 | loss: 1.0592546463012695
Step 51 | grad_norm: 4.313470363616943
Step 51 | learning_rate: 8.525073746312685e-05
Step 51 | epoch: 0.4533333333333333
Step 52 | loss: 0.9500022530555725
Step 52 | grad_norm: 3.6390841007232666
Step 52 | learning_rate: 8.495575221238938e-05
Step 52 | epoch: 0.4622222222222222
Step 53 | loss: 1.1172109842300415
Step 53 | grad_norm: 3.9551167488098145
Step 53 | learning_rate: 8.466076696165192e-05
Step 53 | epoch: 0.4711111111111111
Step 54 | loss: 0.9690203070640564
Step 54 | grad_norm: 5.120077610015869
Step 54 | learning_rate: 8.436578171091446e-05
Step 54 | epoch: 0.48
Step 55 | loss: 1.137641191482544
Step 55 | grad_norm: 4.1196184158325195
Step 55 | learning_rate: 8.4070796460177e-05
Step 55 | epoch: 0.4888888888888889
Step 56 | loss: 1.0479755401611328
Step 56 | grad_norm: 3.457765579223633
Step 56 | learning_rate: 8.377581120943954e-05
Step 56 | epoch: 0.49777777777777776
Step 57 | loss: 1.1260813474655151
Step 57 | grad_norm: 4.776216506958008
Step 57 | learning_rate: 8.348082595870208e-05
Step 57 | epoch: 0.5066666666666667
Step 58 | loss: 1.1364892721176147
Step 58 | grad_norm: 3.594287157058716
Step 58 | learning_rate: 8.31858407079646e-05
Step 58 | epoch: 0.5155555555555555
Step 59 | loss: 1.026124358177185
Step 59 | grad_norm: 3.7575902938842773
Step 59 | learning_rate: 8.289085545722715e-05
Step 59 | epoch: 0.5244444444444445
Step 60 | loss: 1.034901738166809
Step 60 | grad_norm: 3.929811954498291
Step 60 | learning_rate: 8.259587020648968e-05
Step 60 | epoch: 0.5333333333333333
Step 61 | loss: 1.1193890571594238
Step 61 | grad_norm: 3.6838390827178955
Step 61 | learning_rate: 8.230088495575221e-05
Step 61 | epoch: 0.5422222222222223
Step 62 | loss: 0.9318765997886658
Step 62 | grad_norm: 4.1795830726623535
Step 62 | learning_rate: 8.200589970501475e-05
Step 62 | epoch: 0.5511111111111111
Step 63 | loss: 1.2222411632537842
Step 63 | grad_norm: 4.147827625274658
Step 63 | learning_rate: 8.171091445427729e-05
Step 63 | epoch: 0.56
Step 64 | loss: 1.3954253196716309
Step 64 | grad_norm: 4.541697025299072
Step 64 | learning_rate: 8.141592920353983e-05
Step 64 | epoch: 0.5688888888888889
Step 65 | loss: 1.158734679222107
Step 65 | grad_norm: 4.068440914154053
Step 65 | learning_rate: 8.112094395280237e-05
Step 65 | epoch: 0.5777777777777777
Step 66 | loss: 1.1771010160446167
Step 66 | grad_norm: 4.179208755493164
Step 66 | learning_rate: 8.082595870206489e-05
Step 66 | epoch: 0.5866666666666667
Step 67 | loss: 1.2105810642242432
Step 67 | grad_norm: 4.229593753814697
Step 67 | learning_rate: 8.053097345132744e-05
Step 67 | epoch: 0.5955555555555555
Step 68 | loss: 0.8985664248466492
Step 68 | grad_norm: 3.8395562171936035
Step 68 | learning_rate: 8.023598820058997e-05
Step 68 | epoch: 0.6044444444444445
Step 69 | loss: 1.2153284549713135
Step 69 | grad_norm: 3.6123831272125244
Step 69 | learning_rate: 7.99410029498525e-05
Step 69 | epoch: 0.6133333333333333
Step 70 | loss: 1.0225391387939453
Step 70 | grad_norm: 4.186034202575684
Step 70 | learning_rate: 7.964601769911504e-05
Step 70 | epoch: 0.6222222222222222
Step 71 | loss: 1.031262993812561
Step 71 | grad_norm: 3.1706676483154297
Step 71 | learning_rate: 7.935103244837758e-05
Step 71 | epoch: 0.6311111111111111
Step 72 | loss: 1.0865459442138672
Step 72 | grad_norm: 3.5804238319396973
Step 72 | learning_rate: 7.905604719764012e-05
Step 72 | epoch: 0.64
Step 73 | loss: 0.8803452849388123
Step 73 | grad_norm: 3.51705002784729
Step 73 | learning_rate: 7.876106194690266e-05
Step 73 | epoch: 0.6488888888888888
Step 74 | loss: 0.9137510061264038
Step 74 | grad_norm: 3.4271011352539062
Step 74 | learning_rate: 7.84660766961652e-05
Step 74 | epoch: 0.6577777777777778
Step 75 | loss: 1.0847684144973755
Step 75 | grad_norm: 4.234294414520264
Step 75 | learning_rate: 7.817109144542774e-05
Step 75 | epoch: 0.6666666666666666
Step 76 | loss: 1.1234203577041626
Step 76 | grad_norm: 5.077967643737793
Step 76 | learning_rate: 7.787610619469027e-05
Step 76 | epoch: 0.6755555555555556
Step 77 | loss: 1.1219401359558105
Step 77 | grad_norm: 4.151572227478027
Step 77 | learning_rate: 7.75811209439528e-05
Step 77 | epoch: 0.6844444444444444
Step 78 | loss: 1.3931118249893188
Step 78 | grad_norm: 4.692355632781982
Step 78 | learning_rate: 7.728613569321535e-05
Step 78 | epoch: 0.6933333333333334
Step 79 | loss: 1.3520427942276
Step 79 | grad_norm: 4.097238540649414
Step 79 | learning_rate: 7.699115044247787e-05
Step 79 | epoch: 0.7022222222222222
Step 80 | loss: 1.2337536811828613
Step 80 | grad_norm: 4.516443729400635
Step 80 | learning_rate: 7.669616519174043e-05
Step 80 | epoch: 0.7111111111111111
Step 81 | loss: 0.9902271032333374
Step 81 | grad_norm: 4.05944299697876
Step 81 | learning_rate: 7.640117994100295e-05
Step 81 | epoch: 0.72
Step 82 | loss: 1.128161907196045
Step 82 | grad_norm: 4.657468318939209
Step 82 | learning_rate: 7.610619469026549e-05
Step 82 | epoch: 0.7288888888888889
Step 83 | loss: 1.1298739910125732
Step 83 | grad_norm: 4.194515228271484
Step 83 | learning_rate: 7.581120943952803e-05
Step 83 | epoch: 0.7377777777777778
Step 84 | loss: 0.9840070605278015
Step 84 | grad_norm: 3.6793782711029053
Step 84 | learning_rate: 7.551622418879057e-05
Step 84 | epoch: 0.7466666666666667
Step 85 | loss: 0.9531606435775757
Step 85 | grad_norm: 4.888102054595947
Step 85 | learning_rate: 7.522123893805309e-05
Step 85 | epoch: 0.7555555555555555
Step 86 | loss: 0.921936571598053
Step 86 | grad_norm: 3.9160592555999756
Step 86 | learning_rate: 7.492625368731564e-05
Step 86 | epoch: 0.7644444444444445
Step 87 | loss: 1.0527052879333496
Step 87 | grad_norm: 4.065443992614746
Step 87 | learning_rate: 7.463126843657817e-05
Step 87 | epoch: 0.7733333333333333
Step 88 | loss: 1.0425504446029663
Step 88 | grad_norm: 3.968543291091919
Step 88 | learning_rate: 7.433628318584072e-05
Step 88 | epoch: 0.7822222222222223
Step 89 | loss: 0.9191468954086304
Step 89 | grad_norm: 4.9171061515808105
Step 89 | learning_rate: 7.404129793510324e-05
Step 89 | epoch: 0.7911111111111111
Step 90 | loss: 1.0266735553741455
Step 90 | grad_norm: 4.996938705444336
Step 90 | learning_rate: 7.374631268436578e-05
Step 90 | epoch: 0.8
Step 91 | loss: 1.0418426990509033
Step 91 | grad_norm: 3.5637810230255127
Step 91 | learning_rate: 7.345132743362832e-05
Step 91 | epoch: 0.8088888888888889
Step 92 | loss: 1.24305260181427
Step 92 | grad_norm: 4.974267959594727
Step 92 | learning_rate: 7.315634218289086e-05
Step 92 | epoch: 0.8177777777777778
Step 93 | loss: 1.5599298477172852
Step 93 | grad_norm: 4.820266246795654
Step 93 | learning_rate: 7.28613569321534e-05
Step 93 | epoch: 0.8266666666666667
Step 94 | loss: 1.1066004037857056
Step 94 | grad_norm: 3.9769439697265625
Step 94 | learning_rate: 7.256637168141593e-05
Step 94 | epoch: 0.8355555555555556
Step 95 | loss: 1.0935511589050293
Step 95 | grad_norm: 3.8092589378356934
Step 95 | learning_rate: 7.227138643067847e-05
Step 95 | epoch: 0.8444444444444444
Step 96 | loss: 1.1558403968811035
Step 96 | grad_norm: 4.049916744232178
Step 96 | learning_rate: 7.197640117994101e-05
Step 96 | epoch: 0.8533333333333334
Step 97 | loss: 1.0977603197097778
Step 97 | grad_norm: 3.9339699745178223
Step 97 | learning_rate: 7.168141592920355e-05
Step 97 | epoch: 0.8622222222222222
Step 98 | loss: 1.0697017908096313
Step 98 | grad_norm: 4.029179096221924
Step 98 | learning_rate: 7.138643067846607e-05
Step 98 | epoch: 0.8711111111111111
Step 99 | loss: 1.1611449718475342
Step 99 | grad_norm: 4.4027628898620605
Step 99 | learning_rate: 7.109144542772862e-05
Step 99 | epoch: 0.88
Step 100 | loss: 1.1494346857070923
Step 100 | grad_norm: 4.697266578674316
Step 100 | learning_rate: 7.079646017699115e-05
Step 100 | epoch: 0.8888888888888888
Step 101 | loss: 0.9136340022087097
Step 101 | grad_norm: 3.364335060119629
Step 101 | learning_rate: 7.050147492625369e-05
Step 101 | epoch: 0.8977777777777778
Step 102 | loss: 0.9525058269500732
Step 102 | grad_norm: 3.6234347820281982
Step 102 | learning_rate: 7.020648967551623e-05
Step 102 | epoch: 0.9066666666666666
Step 103 | loss: 0.8413904905319214
Step 103 | grad_norm: 3.369638442993164
Step 103 | learning_rate: 6.991150442477876e-05
Step 103 | epoch: 0.9155555555555556
Step 104 | loss: 1.1383721828460693
Step 104 | grad_norm: 3.545168399810791
Step 104 | learning_rate: 6.96165191740413e-05
Step 104 | epoch: 0.9244444444444444
Step 105 | loss: 1.1334059238433838
Step 105 | grad_norm: 4.0735979080200195
Step 105 | learning_rate: 6.932153392330384e-05
Step 105 | epoch: 0.9333333333333333
Step 106 | loss: 1.0008082389831543
Step 106 | grad_norm: 4.177277088165283
Step 106 | learning_rate: 6.902654867256638e-05
Step 106 | epoch: 0.9422222222222222
Step 107 | loss: 0.984049916267395
Step 107 | grad_norm: 3.822232961654663
Step 107 | learning_rate: 6.873156342182892e-05
Step 107 | epoch: 0.9511111111111111
Step 108 | loss: 1.159053921699524
Step 108 | grad_norm: 4.67564058303833
Step 108 | learning_rate: 6.843657817109145e-05
Step 108 | epoch: 0.96
Step 109 | loss: 0.9474616050720215
Step 109 | grad_norm: 4.314421653747559
Step 109 | learning_rate: 6.814159292035398e-05
Step 109 | epoch: 0.9688888888888889
Step 110 | loss: 0.9186239242553711
Step 110 | grad_norm: 4.180852890014648
Step 110 | learning_rate: 6.784660766961653e-05
Step 110 | epoch: 0.9777777777777777
Step 111 | loss: 1.2185159921646118
Step 111 | grad_norm: 4.069605827331543
Step 111 | learning_rate: 6.755162241887906e-05
Step 111 | epoch: 0.9866666666666667
Step 112 | loss: 1.1154744625091553
Step 112 | grad_norm: 3.8809871673583984
Step 112 | learning_rate: 6.725663716814161e-05
Step 112 | epoch: 0.9955555555555555
Step 113 | loss: 0.6469885110855103
Step 113 | grad_norm: 4.983860492706299
Step 113 | learning_rate: 6.696165191740413e-05
Step 113 | epoch: 1.0
Step 114 | loss: 1.1077507734298706
Step 114 | grad_norm: 5.14516544342041
Step 114 | learning_rate: 6.666666666666667e-05
Step 114 | epoch: 1.008888888888889
Step 115 | loss: 0.9790040850639343
Step 115 | grad_norm: 3.9632792472839355
Step 115 | learning_rate: 6.637168141592921e-05
Step 115 | epoch: 1.0177777777777777
Step 116 | loss: 0.9112403988838196
Step 116 | grad_norm: 3.4079253673553467
Step 116 | learning_rate: 6.607669616519175e-05
Step 116 | epoch: 1.0266666666666666
Step 117 | loss: 0.8182623386383057
Step 117 | grad_norm: 3.2708168029785156
Step 117 | learning_rate: 6.578171091445427e-05
Step 117 | epoch: 1.0355555555555556
Step 118 | loss: 0.9221792817115784
Step 118 | grad_norm: 4.179415702819824
Step 118 | learning_rate: 6.548672566371682e-05
Step 118 | epoch: 1.0444444444444445
Step 119 | loss: 0.7512006759643555
Step 119 | grad_norm: 4.083488464355469
Step 119 | learning_rate: 6.519174041297935e-05
Step 119 | epoch: 1.0533333333333332
Step 120 | loss: 0.7823841571807861
Step 120 | grad_norm: 3.668118953704834
Step 120 | learning_rate: 6.48967551622419e-05
Step 120 | epoch: 1.0622222222222222
Step 121 | loss: 0.7724936604499817
Step 121 | grad_norm: 3.1872878074645996
Step 121 | learning_rate: 6.460176991150442e-05
Step 121 | epoch: 1.0711111111111111
Step 122 | loss: 0.7169017791748047
Step 122 | grad_norm: 3.7699778079986572
Step 122 | learning_rate: 6.430678466076696e-05
Step 122 | epoch: 1.08
Step 123 | loss: 0.8238706588745117
Step 123 | grad_norm: 4.229085922241211
Step 123 | learning_rate: 6.40117994100295e-05
Step 123 | epoch: 1.0888888888888888
Step 124 | loss: 0.9681369662284851
Step 124 | grad_norm: 4.344576358795166
Step 124 | learning_rate: 6.371681415929204e-05
Step 124 | epoch: 1.0977777777777777
Step 125 | loss: 0.7047964930534363
Step 125 | grad_norm: 3.428241491317749
Step 125 | learning_rate: 6.342182890855458e-05
Step 125 | epoch: 1.1066666666666667
Step 126 | loss: 0.8753160834312439
Step 126 | grad_norm: 3.694786548614502
Step 126 | learning_rate: 6.312684365781711e-05
Step 126 | epoch: 1.1155555555555556
Step 127 | loss: 0.9123793244361877
Step 127 | grad_norm: 4.090634822845459
Step 127 | learning_rate: 6.283185840707965e-05
Step 127 | epoch: 1.1244444444444444
Step 128 | loss: 0.9298102855682373
Step 128 | grad_norm: 4.002630710601807
Step 128 | learning_rate: 6.253687315634219e-05
Step 128 | epoch: 1.1333333333333333
Step 129 | loss: 0.6832253932952881
Step 129 | grad_norm: 3.5211291313171387
Step 129 | learning_rate: 6.224188790560473e-05
Step 129 | epoch: 1.1422222222222222
Step 130 | loss: 0.8625417351722717
Step 130 | grad_norm: 5.024784564971924
Step 130 | learning_rate: 6.194690265486725e-05
Step 130 | epoch: 1.1511111111111112
Step 131 | loss: 0.7559471726417542
Step 131 | grad_norm: 4.027288913726807
Step 131 | learning_rate: 6.16519174041298e-05
Step 131 | epoch: 1.16
Step 132 | loss: 0.9180426597595215
Step 132 | grad_norm: 4.558779716491699
Step 132 | learning_rate: 6.135693215339233e-05
Step 132 | epoch: 1.1688888888888889
Step 133 | loss: 0.823638916015625
Step 133 | grad_norm: 4.267069339752197
Step 133 | learning_rate: 6.106194690265487e-05
Step 133 | epoch: 1.1777777777777778
Step 134 | loss: 1.0082728862762451
Step 134 | grad_norm: 4.468804359436035
Step 134 | learning_rate: 6.0766961651917406e-05
Step 134 | epoch: 1.1866666666666668
Step 135 | loss: 0.9924509525299072
Step 135 | grad_norm: 6.089796543121338
Step 135 | learning_rate: 6.0471976401179945e-05
Step 135 | epoch: 1.1955555555555555
Step 136 | loss: 0.7729296684265137
Step 136 | grad_norm: 4.045688152313232
Step 136 | learning_rate: 6.017699115044248e-05
Step 136 | epoch: 1.2044444444444444
Step 137 | loss: 0.8677297830581665
Step 137 | grad_norm: 4.41200590133667
Step 137 | learning_rate: 5.988200589970502e-05
Step 137 | epoch: 1.2133333333333334
Step 138 | loss: 0.861175000667572
Step 138 | grad_norm: 4.7520833015441895
Step 138 | learning_rate: 5.958702064896755e-05
Step 138 | epoch: 1.2222222222222223
Step 139 | loss: 0.9364213347434998
Step 139 | grad_norm: 5.095918655395508
Step 139 | learning_rate: 5.92920353982301e-05
Step 139 | epoch: 1.231111111111111
Step 140 | loss: 0.9592275023460388
Step 140 | grad_norm: 5.184606075286865
Step 140 | learning_rate: 5.899705014749263e-05
Step 140 | epoch: 1.24
Step 141 | loss: 0.745677649974823
Step 141 | grad_norm: 4.443906784057617
Step 141 | learning_rate: 5.870206489675516e-05
Step 141 | epoch: 1.248888888888889
Step 142 | loss: 0.8877981901168823
Step 142 | grad_norm: 4.762393474578857
Step 142 | learning_rate: 5.8407079646017705e-05
Step 142 | epoch: 1.2577777777777777
Step 143 | loss: 1.0318026542663574
Step 143 | grad_norm: 5.089329242706299
Step 143 | learning_rate: 5.8112094395280236e-05
Step 143 | epoch: 1.2666666666666666
Step 144 | loss: 0.8389211297035217
Step 144 | grad_norm: 4.533627510070801
Step 144 | learning_rate: 5.781710914454278e-05
Step 144 | epoch: 1.2755555555555556
Step 145 | loss: 0.7741897702217102
Step 145 | grad_norm: 4.061990737915039
Step 145 | learning_rate: 5.752212389380531e-05
Step 145 | epoch: 1.2844444444444445
Step 146 | loss: 0.8251665830612183
Step 146 | grad_norm: 6.388865947723389
Step 146 | learning_rate: 5.7227138643067844e-05
Step 146 | epoch: 1.2933333333333334
Step 147 | loss: 0.7085758447647095
Step 147 | grad_norm: 4.97199010848999
Step 147 | learning_rate: 5.693215339233039e-05
Step 147 | epoch: 1.3022222222222222
Step 148 | loss: 0.7113863825798035
Step 148 | grad_norm: 4.355283260345459
Step 148 | learning_rate: 5.663716814159292e-05
Step 148 | epoch: 1.3111111111111111
Step 149 | loss: 0.9363450407981873
Step 149 | grad_norm: 5.149911880493164
Step 149 | learning_rate: 5.634218289085545e-05
Step 149 | epoch: 1.32
Step 150 | loss: 0.6387327313423157
Step 150 | grad_norm: 3.9889233112335205
Step 150 | learning_rate: 5.6047197640118e-05
Step 150 | epoch: 1.3288888888888888
Step 151 | loss: 0.7220273613929749
Step 151 | grad_norm: 4.025754928588867
Step 151 | learning_rate: 5.575221238938053e-05
Step 151 | epoch: 1.3377777777777777
Step 152 | loss: 0.6866630911827087
Step 152 | grad_norm: 4.551762580871582
Step 152 | learning_rate: 5.545722713864307e-05
Step 152 | epoch: 1.3466666666666667
Step 153 | loss: 0.793350875377655
Step 153 | grad_norm: 5.725704669952393
Step 153 | learning_rate: 5.5162241887905605e-05
Step 153 | epoch: 1.3555555555555556
Step 154 | loss: 0.6746803522109985
Step 154 | grad_norm: 4.995482921600342
Step 154 | learning_rate: 5.486725663716814e-05
Step 154 | epoch: 1.3644444444444446
Step 155 | loss: 0.8673689961433411
Step 155 | grad_norm: 4.352887153625488
Step 155 | learning_rate: 5.457227138643069e-05
Step 155 | epoch: 1.3733333333333333
Step 156 | loss: 0.8997471332550049
Step 156 | grad_norm: 4.99804162979126
Step 156 | learning_rate: 5.427728613569322e-05
Step 156 | epoch: 1.3822222222222222
Step 157 | loss: 0.9677724838256836
Step 157 | grad_norm: 4.14137077331543
Step 157 | learning_rate: 5.398230088495575e-05
Step 157 | epoch: 1.3911111111111112
Step 158 | loss: 0.8905572891235352
Step 158 | grad_norm: 5.436502456665039
Step 158 | learning_rate: 5.3687315634218295e-05
Step 158 | epoch: 1.4
Step 159 | loss: 0.8827969431877136
Step 159 | grad_norm: 4.876256942749023
Step 159 | learning_rate: 5.339233038348083e-05
Step 159 | epoch: 1.4088888888888889
Step 160 | loss: 0.9653514623641968
Step 160 | grad_norm: 5.61561393737793
Step 160 | learning_rate: 5.309734513274337e-05
Step 160 | epoch: 1.4177777777777778
Step 161 | loss: 0.7246577739715576
Step 161 | grad_norm: 4.948924541473389
Step 161 | learning_rate: 5.28023598820059e-05
Step 161 | epoch: 1.4266666666666667
Step 162 | loss: 0.8781888484954834
Step 162 | grad_norm: 4.30485200881958
Step 162 | learning_rate: 5.2507374631268435e-05
Step 162 | epoch: 1.4355555555555555
Step 163 | loss: 0.8061007857322693
Step 163 | grad_norm: 4.201789855957031
Step 163 | learning_rate: 5.221238938053098e-05
Step 163 | epoch: 1.4444444444444444
Step 164 | loss: 0.7446263432502747
Step 164 | grad_norm: 4.218804359436035
Step 164 | learning_rate: 5.191740412979351e-05
Step 164 | epoch: 1.4533333333333334
Step 165 | loss: 0.8227376341819763
Step 165 | grad_norm: 5.064818382263184
Step 165 | learning_rate: 5.162241887905604e-05
Step 165 | epoch: 1.462222222222222
Step 166 | loss: 0.8167884945869446
Step 166 | grad_norm: 5.373565196990967
Step 166 | learning_rate: 5.132743362831859e-05
Step 166 | epoch: 1.471111111111111
Step 167 | loss: 0.719927966594696
Step 167 | grad_norm: 4.948611736297607
Step 167 | learning_rate: 5.103244837758112e-05
Step 167 | epoch: 1.48
Step 168 | loss: 0.7565000057220459
Step 168 | grad_norm: 4.8008527755737305
Step 168 | learning_rate: 5.0737463126843664e-05
Step 168 | epoch: 1.488888888888889
Step 169 | loss: 0.835614800453186
Step 169 | grad_norm: 5.436436653137207
Step 169 | learning_rate: 5.0442477876106195e-05
Step 169 | epoch: 1.4977777777777779
Step 170 | loss: 0.6522485613822937
Step 170 | grad_norm: 4.176706314086914
Step 170 | learning_rate: 5.014749262536873e-05
Step 170 | epoch: 1.5066666666666668
Step 171 | loss: 0.8037072420120239
Step 171 | grad_norm: 5.412860870361328
Step 171 | learning_rate: 4.985250737463127e-05
Step 171 | epoch: 1.5155555555555555
Step 172 | loss: 0.9814816117286682
Step 172 | grad_norm: 5.4522480964660645
Step 172 | learning_rate: 4.955752212389381e-05
Step 172 | epoch: 1.5244444444444445
Step 173 | loss: 0.9132600426673889
Step 173 | grad_norm: 6.708873271942139
Step 173 | learning_rate: 4.926253687315635e-05
Step 173 | epoch: 1.5333333333333332
Step 174 | loss: 0.5791953802108765
Step 174 | grad_norm: 5.58281946182251
Step 174 | learning_rate: 4.8967551622418886e-05
Step 174 | epoch: 1.5422222222222222
Step 175 | loss: 0.6715361475944519
Step 175 | grad_norm: 5.004402160644531
Step 175 | learning_rate: 4.867256637168142e-05
Step 175 | epoch: 1.551111111111111
Step 176 | loss: 1.1230717897415161
Step 176 | grad_norm: 6.965766906738281
Step 176 | learning_rate: 4.8377581120943956e-05
Step 176 | epoch: 1.56
Step 177 | loss: 0.6997664570808411
Step 177 | grad_norm: 4.916704177856445
Step 177 | learning_rate: 4.8082595870206494e-05
Step 177 | epoch: 1.568888888888889
Step 178 | loss: 0.9014516472816467
Step 178 | grad_norm: 5.288059234619141
Step 178 | learning_rate: 4.778761061946903e-05
Step 178 | epoch: 1.5777777777777777
Step 179 | loss: 0.8120046854019165
Step 179 | grad_norm: 6.010931015014648
Step 179 | learning_rate: 4.749262536873156e-05
Step 179 | epoch: 1.5866666666666667
Step 180 | loss: 0.8823382258415222
Step 180 | grad_norm: 5.3597846031188965
Step 180 | learning_rate: 4.71976401179941e-05
Step 180 | epoch: 1.5955555555555554
Step 181 | loss: 1.0666043758392334
Step 181 | grad_norm: 5.408235549926758
Step 181 | learning_rate: 4.690265486725664e-05
Step 181 | epoch: 1.6044444444444443
Step 182 | loss: 0.7017137408256531
Step 182 | grad_norm: 5.248713493347168
Step 182 | learning_rate: 4.660766961651918e-05
Step 182 | epoch: 1.6133333333333333
Step 183 | loss: 1.0144681930541992
Step 183 | grad_norm: 5.934606075286865
Step 183 | learning_rate: 4.631268436578171e-05
Step 183 | epoch: 1.6222222222222222
Step 184 | loss: 0.7786692380905151
Step 184 | grad_norm: 4.580370903015137
Step 184 | learning_rate: 4.601769911504425e-05
Step 184 | epoch: 1.6311111111111112
Step 185 | loss: 0.9775762557983398
Step 185 | grad_norm: 5.243830680847168
Step 185 | learning_rate: 4.5722713864306786e-05
Step 185 | epoch: 1.6400000000000001
Step 186 | loss: 0.9335139393806458
Step 186 | grad_norm: 8.273067474365234
Step 186 | learning_rate: 4.5427728613569324e-05
Step 186 | epoch: 1.6488888888888888
Step 187 | loss: 0.7653258442878723
Step 187 | grad_norm: 5.381140232086182
Step 187 | learning_rate: 4.5132743362831855e-05
Step 187 | epoch: 1.6577777777777778
Step 188 | loss: 0.7711794376373291
Step 188 | grad_norm: 5.1748270988464355
Step 188 | learning_rate: 4.48377581120944e-05
Step 188 | epoch: 1.6666666666666665
Step 189 | loss: 0.701546311378479
Step 189 | grad_norm: 4.993161201477051
Step 189 | learning_rate: 4.454277286135694e-05
Step 189 | epoch: 1.6755555555555555
Step 190 | loss: 0.777847945690155
Step 190 | grad_norm: 4.582861423492432
Step 190 | learning_rate: 4.4247787610619477e-05
Step 190 | epoch: 1.6844444444444444
Step 191 | loss: 1.0589733123779297
Step 191 | grad_norm: 5.8026628494262695
Step 191 | learning_rate: 4.395280235988201e-05
Step 191 | epoch: 1.6933333333333334
Step 192 | loss: 0.722801923751831
Step 192 | grad_norm: 5.009365081787109
Step 192 | learning_rate: 4.3657817109144546e-05
Step 192 | epoch: 1.7022222222222223
Step 193 | loss: 0.7552410364151001
Step 193 | grad_norm: 5.275051593780518
Step 193 | learning_rate: 4.3362831858407084e-05
Step 193 | epoch: 1.7111111111111112
Step 194 | loss: 0.6660550236701965
Step 194 | grad_norm: 4.5719895362854
Step 194 | learning_rate: 4.306784660766962e-05
Step 194 | epoch: 1.72
Step 195 | loss: 0.8076562881469727
Step 195 | grad_norm: 5.070939540863037
Step 195 | learning_rate: 4.2772861356932154e-05
Step 195 | epoch: 1.728888888888889
Step 196 | loss: 0.9314118027687073
Step 196 | grad_norm: 5.884499549865723
Step 196 | learning_rate: 4.247787610619469e-05
Step 196 | epoch: 1.7377777777777776
Step 197 | loss: 1.038475513458252
Step 197 | grad_norm: 5.71091365814209
Step 197 | learning_rate: 4.218289085545723e-05
Step 197 | epoch: 1.7466666666666666
Step 198 | loss: 0.5775535106658936
Step 198 | grad_norm: 4.204078674316406
Step 198 | learning_rate: 4.188790560471977e-05
Step 198 | epoch: 1.7555555555555555
Step 199 | loss: 0.7438396215438843
Step 199 | grad_norm: 5.381398677825928
Step 199 | learning_rate: 4.15929203539823e-05
Step 199 | epoch: 1.7644444444444445
Step 200 | loss: 1.0021921396255493
Step 200 | grad_norm: 6.582272052764893
Step 200 | learning_rate: 4.129793510324484e-05
Step 200 | epoch: 1.7733333333333334
Step 201 | loss: 1.041493535041809
Step 201 | grad_norm: 7.088857650756836
Step 201 | learning_rate: 4.1002949852507376e-05
Step 201 | epoch: 1.7822222222222224
Step 202 | loss: 0.7721502184867859
Step 202 | grad_norm: 4.884614944458008
Step 202 | learning_rate: 4.0707964601769914e-05
Step 202 | epoch: 1.791111111111111
Step 203 | loss: 0.7252294421195984
Step 203 | grad_norm: 5.224587440490723
Step 203 | learning_rate: 4.0412979351032446e-05
Step 203 | epoch: 1.8
Step 204 | loss: 0.9992951154708862
Step 204 | grad_norm: 5.204029083251953
Step 204 | learning_rate: 4.0117994100294984e-05
Step 204 | epoch: 1.8088888888888888
Step 205 | loss: 0.81535804271698
Step 205 | grad_norm: 6.835456371307373
Step 205 | learning_rate: 3.982300884955752e-05
Step 205 | epoch: 1.8177777777777777
Step 206 | loss: 0.8229919075965881
Step 206 | grad_norm: 5.733746528625488
Step 206 | learning_rate: 3.952802359882006e-05
Step 206 | epoch: 1.8266666666666667
Step 207 | loss: 0.802575409412384
Step 207 | grad_norm: 6.616541862487793
Step 207 | learning_rate: 3.92330383480826e-05
Step 207 | epoch: 1.8355555555555556
Step 208 | loss: 0.6525135040283203
Step 208 | grad_norm: 4.551557540893555
Step 208 | learning_rate: 3.893805309734514e-05
Step 208 | epoch: 1.8444444444444446
Step 209 | loss: 0.9893503189086914
Step 209 | grad_norm: 5.307546615600586
Step 209 | learning_rate: 3.8643067846607675e-05
Step 209 | epoch: 1.8533333333333335
Step 210 | loss: 0.9772375226020813
Step 210 | grad_norm: 5.684010982513428
Step 210 | learning_rate: 3.834808259587021e-05
Step 210 | epoch: 1.8622222222222222
Step 211 | loss: 0.9216883182525635
Step 211 | grad_norm: 5.739295959472656
Step 211 | learning_rate: 3.8053097345132744e-05
Step 211 | epoch: 1.871111111111111
Step 212 | loss: 0.7591532468795776
Step 212 | grad_norm: 5.026391983032227
Step 212 | learning_rate: 3.775811209439528e-05
Step 212 | epoch: 1.88
Step 213 | loss: 0.8874602317810059
Step 213 | grad_norm: 7.317058563232422
Step 213 | learning_rate: 3.746312684365782e-05
Step 213 | epoch: 1.8888888888888888
Step 214 | loss: 0.7364134192466736
Step 214 | grad_norm: 4.33488655090332
Step 214 | learning_rate: 3.716814159292036e-05
Step 214 | epoch: 1.8977777777777778
Step 215 | loss: 0.9192534685134888
Step 215 | grad_norm: 5.872885704040527
Step 215 | learning_rate: 3.687315634218289e-05
Step 215 | epoch: 1.9066666666666667
Step 216 | loss: 0.8022522926330566
Step 216 | grad_norm: 5.248341083526611
Step 216 | learning_rate: 3.657817109144543e-05
Step 216 | epoch: 1.9155555555555557
Step 217 | loss: 0.7711683511734009
Step 217 | grad_norm: 5.8132100105285645
Step 217 | learning_rate: 3.628318584070797e-05
Step 217 | epoch: 1.9244444444444444
Step 218 | loss: 0.6922206282615662
Step 218 | grad_norm: 4.7275710105896
Step 218 | learning_rate: 3.5988200589970505e-05
Step 218 | epoch: 1.9333333333333333
Step 219 | loss: 0.8855599761009216
Step 219 | grad_norm: 6.846333026885986
Step 219 | learning_rate: 3.5693215339233036e-05
Step 219 | epoch: 1.942222222222222
Step 220 | loss: 0.6375575065612793
Step 220 | grad_norm: 5.7992262840271
Step 220 | learning_rate: 3.5398230088495574e-05
Step 220 | epoch: 1.951111111111111
Step 221 | loss: 0.871606171131134
Step 221 | grad_norm: 5.980714797973633
Step 221 | learning_rate: 3.510324483775811e-05
Step 221 | epoch: 1.96
Step 222 | loss: 0.7802296280860901
Step 222 | grad_norm: 5.561983585357666
Step 222 | learning_rate: 3.480825958702065e-05
Step 222 | epoch: 1.968888888888889
Step 223 | loss: 0.6375651359558105
Step 223 | grad_norm: 4.097321033477783
Step 223 | learning_rate: 3.451327433628319e-05
Step 223 | epoch: 1.9777777777777779
Step 224 | loss: 0.7769365310668945
Step 224 | grad_norm: 4.66118860244751
Step 224 | learning_rate: 3.421828908554573e-05
Step 224 | epoch: 1.9866666666666668
Step 225 | loss: 0.6321507096290588
Step 225 | grad_norm: 4.624169826507568
Step 225 | learning_rate: 3.3923303834808265e-05
Step 225 | epoch: 1.9955555555555555
Step 226 | loss: 0.8343429565429688
Step 226 | grad_norm: 7.3293538093566895
Step 226 | learning_rate: 3.3628318584070804e-05
Step 226 | epoch: 2.0
Step 227 | loss: 0.6603801846504211
Step 227 | grad_norm: 4.902756690979004
Step 227 | learning_rate: 3.3333333333333335e-05
Step 227 | epoch: 2.008888888888889
Step 228 | loss: 0.5886722207069397
Step 228 | grad_norm: 4.30156946182251
Step 228 | learning_rate: 3.303834808259587e-05
Step 228 | epoch: 2.017777777777778
Step 229 | loss: 0.44482001662254333
Step 229 | grad_norm: 4.39084005355835
Step 229 | learning_rate: 3.274336283185841e-05
Step 229 | epoch: 2.026666666666667
Step 230 | loss: 0.5517650246620178
Step 230 | grad_norm: 4.489272117614746
Step 230 | learning_rate: 3.244837758112095e-05
Step 230 | epoch: 2.0355555555555553
Step 231 | loss: 0.8056892156600952
Step 231 | grad_norm: 4.891422748565674
Step 231 | learning_rate: 3.215339233038348e-05
Step 231 | epoch: 2.0444444444444443
Step 232 | loss: 0.7160212397575378
Step 232 | grad_norm: 5.212916374206543
Step 232 | learning_rate: 3.185840707964602e-05
Step 232 | epoch: 2.0533333333333332
Step 233 | loss: 0.5029615163803101
Step 233 | grad_norm: 4.309323310852051
Step 233 | learning_rate: 3.156342182890856e-05
Step 233 | epoch: 2.062222222222222
Step 234 | loss: 0.6094775199890137
Step 234 | grad_norm: 5.427579402923584
Step 234 | learning_rate: 3.1268436578171095e-05
Step 234 | epoch: 2.071111111111111
Step 235 | loss: 0.6753891706466675
Step 235 | grad_norm: 5.093523025512695
Step 235 | learning_rate: 3.097345132743363e-05
Step 235 | epoch: 2.08
Step 236 | loss: 0.6900237202644348
Step 236 | grad_norm: 5.812115669250488
Step 236 | learning_rate: 3.0678466076696165e-05
Step 236 | epoch: 2.088888888888889
Step 237 | loss: 0.5371295809745789
Step 237 | grad_norm: 5.517641544342041
Step 237 | learning_rate: 3.0383480825958703e-05
Step 237 | epoch: 2.097777777777778
Step 238 | loss: 0.48497673869132996
Step 238 | grad_norm: 5.156633377075195
Step 238 | learning_rate: 3.008849557522124e-05
Step 238 | epoch: 2.1066666666666665
Step 239 | loss: 0.5173084139823914
Step 239 | grad_norm: 5.367783546447754
Step 239 | learning_rate: 2.9793510324483776e-05
Step 239 | epoch: 2.1155555555555554
Step 240 | loss: 0.5388160943984985
Step 240 | grad_norm: 6.8070149421691895
Step 240 | learning_rate: 2.9498525073746314e-05
Step 240 | epoch: 2.1244444444444444
Step 241 | loss: 0.6873117089271545
Step 241 | grad_norm: 5.324512004852295
Step 241 | learning_rate: 2.9203539823008852e-05
Step 241 | epoch: 2.1333333333333333
Step 242 | loss: 0.548331618309021
Step 242 | grad_norm: 6.1811628341674805
Step 242 | learning_rate: 2.890855457227139e-05
Step 242 | epoch: 2.1422222222222222
Step 243 | loss: 0.4826176166534424
Step 243 | grad_norm: 5.91814661026001
Step 243 | learning_rate: 2.8613569321533922e-05
Step 243 | epoch: 2.151111111111111
Step 244 | loss: 0.7814361453056335
Step 244 | grad_norm: 7.1241841316223145
Step 244 | learning_rate: 2.831858407079646e-05
Step 244 | epoch: 2.16
Step 245 | loss: 0.5293041467666626
Step 245 | grad_norm: 5.662058353424072
Step 245 | learning_rate: 2.8023598820059e-05
Step 245 | epoch: 2.168888888888889
Step 246 | loss: 0.6544119715690613
Step 246 | grad_norm: 7.53287410736084
Step 246 | learning_rate: 2.7728613569321537e-05
Step 246 | epoch: 2.1777777777777776
Step 247 | loss: 0.6688807010650635
Step 247 | grad_norm: 7.148896217346191
Step 247 | learning_rate: 2.743362831858407e-05
Step 247 | epoch: 2.1866666666666665
Step 248 | loss: 0.6219838857650757
Step 248 | grad_norm: 6.286671161651611
Step 248 | learning_rate: 2.713864306784661e-05
Step 248 | epoch: 2.1955555555555555
Step 249 | loss: 0.6024092435836792
Step 249 | grad_norm: 6.352601528167725
Step 249 | learning_rate: 2.6843657817109148e-05
Step 249 | epoch: 2.2044444444444444
Step 250 | loss: 0.663449764251709
Step 250 | grad_norm: 6.495377063751221
Step 250 | learning_rate: 2.6548672566371686e-05
Step 250 | epoch: 2.2133333333333334
Step 251 | loss: 0.3958335518836975
Step 251 | grad_norm: 6.466401100158691
Step 251 | learning_rate: 2.6253687315634217e-05
Step 251 | epoch: 2.2222222222222223
Step 252 | loss: 0.6536906361579895
Step 252 | grad_norm: 7.542074680328369
Step 252 | learning_rate: 2.5958702064896756e-05
Step 252 | epoch: 2.2311111111111113
Step 253 | loss: 0.2750342786312103
Step 253 | grad_norm: 4.414941787719727
Step 253 | learning_rate: 2.5663716814159294e-05
Step 253 | epoch: 2.24
Step 254 | loss: 0.605162501335144
Step 254 | grad_norm: 5.376485347747803
Step 254 | learning_rate: 2.5368731563421832e-05
Step 254 | epoch: 2.2488888888888887
Step 255 | loss: 0.5360974669456482
Step 255 | grad_norm: 5.753400802612305
Step 255 | learning_rate: 2.5073746312684367e-05
Step 255 | epoch: 2.2577777777777777
Step 256 | loss: 0.42398491501808167
Step 256 | grad_norm: 5.001084804534912
Step 256 | learning_rate: 2.4778761061946905e-05
Step 256 | epoch: 2.2666666666666666
Step 257 | loss: 0.6410257816314697
Step 257 | grad_norm: 6.4991326332092285
Step 257 | learning_rate: 2.4483775811209443e-05
Step 257 | epoch: 2.2755555555555556
Step 258 | loss: 0.5252240300178528
Step 258 | grad_norm: 7.204753875732422
Step 258 | learning_rate: 2.4188790560471978e-05
Step 258 | epoch: 2.2844444444444445
Step 259 | loss: 0.6199726462364197
Step 259 | grad_norm: 7.820274353027344
Step 259 | learning_rate: 2.3893805309734516e-05
Step 259 | epoch: 2.2933333333333334
Step 260 | loss: 0.5748510956764221
Step 260 | grad_norm: 5.650464057922363
Step 260 | learning_rate: 2.359882005899705e-05
Step 260 | epoch: 2.3022222222222224
Step 261 | loss: 0.3995080590248108
Step 261 | grad_norm: 5.268932342529297
Step 261 | learning_rate: 2.330383480825959e-05
Step 261 | epoch: 2.311111111111111
Step 262 | loss: 0.8798525929450989
Step 262 | grad_norm: 7.241148948669434
Step 262 | learning_rate: 2.3008849557522124e-05
Step 262 | epoch: 2.32
Step 263 | loss: 0.5112025141716003
Step 263 | grad_norm: 5.062538146972656
Step 263 | learning_rate: 2.2713864306784662e-05
Step 263 | epoch: 2.328888888888889
Step 264 | loss: 0.6349602341651917
Step 264 | grad_norm: 6.4634809494018555
Step 264 | learning_rate: 2.24188790560472e-05
Step 264 | epoch: 2.3377777777777777
Step 265 | loss: 0.6079506278038025
Step 265 | grad_norm: 6.892983913421631
Step 265 | learning_rate: 2.2123893805309738e-05
Step 265 | epoch: 2.3466666666666667
Step 266 | loss: 0.6619051694869995
Step 266 | grad_norm: 6.141165256500244
Step 266 | learning_rate: 2.1828908554572273e-05
Step 266 | epoch: 2.3555555555555556
Step 267 | loss: 0.8471581935882568
Step 267 | grad_norm: 7.090826511383057
Step 267 | learning_rate: 2.153392330383481e-05
Step 267 | epoch: 2.3644444444444446
Step 268 | loss: 0.4962629973888397
Step 268 | grad_norm: 6.3098626136779785
Step 268 | learning_rate: 2.1238938053097346e-05
Step 268 | epoch: 2.3733333333333335
Step 269 | loss: 0.5994328260421753
Step 269 | grad_norm: 5.860748291015625
Step 269 | learning_rate: 2.0943952802359884e-05
Step 269 | epoch: 2.3822222222222225
Step 270 | loss: 0.4828560948371887
Step 270 | grad_norm: 5.816458225250244
Step 270 | learning_rate: 2.064896755162242e-05
Step 270 | epoch: 2.391111111111111
Step 271 | loss: 0.7106615900993347
Step 271 | grad_norm: 7.949878215789795
Step 271 | learning_rate: 2.0353982300884957e-05
Step 271 | epoch: 2.4
Step 272 | loss: 0.6167834997177124
Step 272 | grad_norm: 6.69644832611084
Step 272 | learning_rate: 2.0058997050147492e-05
Step 272 | epoch: 2.408888888888889
Step 273 | loss: 0.5396332144737244
Step 273 | grad_norm: 5.747426509857178
Step 273 | learning_rate: 1.976401179941003e-05
Step 273 | epoch: 2.417777777777778
Step 274 | loss: 0.566823422908783
Step 274 | grad_norm: 6.745016098022461
Step 274 | learning_rate: 1.946902654867257e-05
Step 274 | epoch: 2.4266666666666667
Step 275 | loss: 0.521293580532074
Step 275 | grad_norm: 6.344274520874023
Step 275 | learning_rate: 1.9174041297935107e-05
Step 275 | epoch: 2.4355555555555557
Step 276 | loss: 0.48472392559051514
Step 276 | grad_norm: 6.555459499359131
Step 276 | learning_rate: 1.887905604719764e-05
Step 276 | epoch: 2.4444444444444446
Step 277 | loss: 0.5275029540061951
Step 277 | grad_norm: 7.403828144073486
Step 277 | learning_rate: 1.858407079646018e-05
Step 277 | epoch: 2.453333333333333
Step 278 | loss: 0.7934353351593018
Step 278 | grad_norm: 6.690319061279297
Step 278 | learning_rate: 1.8289085545722714e-05
Step 278 | epoch: 2.462222222222222
Step 279 | loss: 0.6184310913085938
Step 279 | grad_norm: 6.918939590454102
Step 279 | learning_rate: 1.7994100294985252e-05
Step 279 | epoch: 2.471111111111111
Step 280 | loss: 0.6458969712257385
Step 280 | grad_norm: 6.730095863342285
Step 280 | learning_rate: 1.7699115044247787e-05
Step 280 | epoch: 2.48
Step 281 | loss: 0.5373237133026123
Step 281 | grad_norm: 7.484817981719971
Step 281 | learning_rate: 1.7404129793510325e-05
Step 281 | epoch: 2.488888888888889
Step 282 | loss: 0.5359360575675964
Step 282 | grad_norm: 5.55005407333374
Step 282 | learning_rate: 1.7109144542772864e-05
Step 282 | epoch: 2.497777777777778
Step 283 | loss: 0.6243545413017273
Step 283 | grad_norm: 6.658310890197754
Step 283 | learning_rate: 1.6814159292035402e-05
Step 283 | epoch: 2.506666666666667
Step 284 | loss: 0.5664772391319275
Step 284 | grad_norm: 7.627093315124512
Step 284 | learning_rate: 1.6519174041297937e-05
Step 284 | epoch: 2.5155555555555553
Step 285 | loss: 0.7564082145690918
Step 285 | grad_norm: 6.975790023803711
Step 285 | learning_rate: 1.6224188790560475e-05
Step 285 | epoch: 2.5244444444444447
Step 286 | loss: 0.5378230214118958
Step 286 | grad_norm: 6.956887245178223
Step 286 | learning_rate: 1.592920353982301e-05
Step 286 | epoch: 2.533333333333333
Step 287 | loss: 0.5936757326126099
Step 287 | grad_norm: 6.5904927253723145
Step 287 | learning_rate: 1.5634218289085548e-05
Step 287 | epoch: 2.542222222222222
Step 288 | loss: 0.5961534380912781
Step 288 | grad_norm: 7.280108451843262
Step 288 | learning_rate: 1.5339233038348082e-05
Step 288 | epoch: 2.551111111111111
Step 289 | loss: 0.5958335399627686
Step 289 | grad_norm: 7.025186538696289
Step 289 | learning_rate: 1.504424778761062e-05
Step 289 | epoch: 2.56
Step 290 | loss: 0.437766432762146
Step 290 | grad_norm: 5.643430709838867
Step 290 | learning_rate: 1.4749262536873157e-05
Step 290 | epoch: 2.568888888888889
Step 291 | loss: 0.5233342051506042
Step 291 | grad_norm: 6.189855098724365
Step 291 | learning_rate: 1.4454277286135695e-05
Step 291 | epoch: 2.5777777777777775
Step 292 | loss: 0.5685540437698364
Step 292 | grad_norm: 7.3064069747924805
Step 292 | learning_rate: 1.415929203539823e-05
Step 292 | epoch: 2.586666666666667
Step 293 | loss: 0.5017629861831665
Step 293 | grad_norm: 5.959670543670654
Step 293 | learning_rate: 1.3864306784660768e-05
Step 293 | epoch: 2.5955555555555554
Step 294 | loss: 0.557346522808075
Step 294 | grad_norm: 7.267475605010986
Step 294 | learning_rate: 1.3569321533923305e-05
Step 294 | epoch: 2.6044444444444443
Step 295 | loss: 0.6212794184684753
Step 295 | grad_norm: 6.2281975746154785
Step 295 | learning_rate: 1.3274336283185843e-05
Step 295 | epoch: 2.6133333333333333
Step 296 | loss: 0.5794931650161743
Step 296 | grad_norm: 6.021389484405518
Step 296 | learning_rate: 1.2979351032448378e-05
Step 296 | epoch: 2.6222222222222222
Step 297 | loss: 0.48895537853240967
Step 297 | grad_norm: 5.095693588256836
Step 297 | learning_rate: 1.2684365781710916e-05
Step 297 | epoch: 2.631111111111111
Step 298 | loss: 0.6768502593040466
Step 298 | grad_norm: 6.14889669418335
Step 298 | learning_rate: 1.2389380530973452e-05
Step 298 | epoch: 2.64
Step 299 | loss: 0.7608550786972046
Step 299 | grad_norm: 7.089272975921631
Step 299 | learning_rate: 1.2094395280235989e-05
Step 299 | epoch: 2.648888888888889
Step 300 | loss: 0.764225959777832
Step 300 | grad_norm: 7.063418388366699
Step 300 | learning_rate: 1.1799410029498525e-05
Step 300 | epoch: 2.6577777777777776
Step 301 | loss: 0.5499376654624939
Step 301 | grad_norm: 7.957964897155762
Step 301 | learning_rate: 1.1504424778761062e-05
Step 301 | epoch: 2.6666666666666665
Step 302 | loss: 0.4768988788127899
Step 302 | grad_norm: 5.599592208862305
Step 302 | learning_rate: 1.12094395280236e-05
Step 302 | epoch: 2.6755555555555555
Step 303 | loss: 0.6984423995018005
Step 303 | grad_norm: 7.188364028930664
Step 303 | learning_rate: 1.0914454277286137e-05
Step 303 | epoch: 2.6844444444444444
Step 304 | loss: 0.659572184085846
Step 304 | grad_norm: 6.332640171051025
Step 304 | learning_rate: 1.0619469026548673e-05
Step 304 | epoch: 2.6933333333333334
Step 305 | loss: 0.5378457903862
Step 305 | grad_norm: 5.263694763183594
Step 305 | learning_rate: 1.032448377581121e-05
Step 305 | epoch: 2.7022222222222223
Step 306 | loss: 0.6475489139556885
Step 306 | grad_norm: 6.865248203277588
Step 306 | learning_rate: 1.0029498525073746e-05
Step 306 | epoch: 2.7111111111111112
Step 307 | loss: 0.5955469608306885
Step 307 | grad_norm: 7.27032995223999
Step 307 | learning_rate: 9.734513274336284e-06
Step 307 | epoch: 2.7199999999999998
Step 308 | loss: 0.4039888083934784
Step 308 | grad_norm: 5.550525665283203
Step 308 | learning_rate: 9.43952802359882e-06
Step 308 | epoch: 2.728888888888889
Step 309 | loss: 0.5899297595024109
Step 309 | grad_norm: 6.4748854637146
Step 309 | learning_rate: 9.144542772861357e-06
Step 309 | epoch: 2.7377777777777776
Step 310 | loss: 0.5053325295448303
Step 310 | grad_norm: 5.646251678466797
Step 310 | learning_rate: 8.849557522123894e-06
Step 310 | epoch: 2.7466666666666666
Step 311 | loss: 0.41239434480667114
Step 311 | grad_norm: 4.973318576812744
Step 311 | learning_rate: 8.554572271386432e-06
Step 311 | epoch: 2.7555555555555555
Step 312 | loss: 0.637859046459198
Step 312 | grad_norm: 8.099498748779297
Step 312 | learning_rate: 8.259587020648968e-06
Step 312 | epoch: 2.7644444444444445
Step 313 | loss: 0.6062397956848145
Step 313 | grad_norm: 6.524500846862793
Step 313 | learning_rate: 7.964601769911505e-06
Step 313 | epoch: 2.7733333333333334
Step 314 | loss: 0.5912420749664307
Step 314 | grad_norm: 5.44658088684082
Step 314 | learning_rate: 7.669616519174041e-06
Step 314 | epoch: 2.7822222222222224
Step 315 | loss: 0.5863788723945618
Step 315 | grad_norm: 6.681256294250488
Step 315 | learning_rate: 7.374631268436579e-06
Step 315 | epoch: 2.7911111111111113
Step 316 | loss: 0.7662203907966614
Step 316 | grad_norm: 8.870214462280273
Step 316 | learning_rate: 7.079646017699115e-06
Step 316 | epoch: 2.8
Step 317 | loss: 0.5230894088745117
Step 317 | grad_norm: 6.250392913818359
Step 317 | learning_rate: 6.784660766961652e-06
Step 317 | epoch: 2.8088888888888888
Step 318 | loss: 0.5357432961463928
Step 318 | grad_norm: 5.678427219390869
Step 318 | learning_rate: 6.489675516224189e-06
Step 318 | epoch: 2.8177777777777777
Step 319 | loss: 0.4522515833377838
Step 319 | grad_norm: 5.170536041259766
Step 319 | learning_rate: 6.194690265486726e-06
Step 319 | epoch: 2.8266666666666667
Step 320 | loss: 0.5552689433097839
Step 320 | grad_norm: 5.240983486175537
Step 320 | learning_rate: 5.899705014749263e-06
Step 320 | epoch: 2.8355555555555556
Step 321 | loss: 0.6026131510734558
Step 321 | grad_norm: 6.937589645385742
Step 321 | learning_rate: 5.6047197640118e-06
Step 321 | epoch: 2.8444444444444446
Step 322 | loss: 0.747238039970398
Step 322 | grad_norm: 8.425260543823242
Step 322 | learning_rate: 5.3097345132743365e-06
Step 322 | epoch: 2.8533333333333335
Step 323 | loss: 0.54716956615448
Step 323 | grad_norm: 5.779758453369141
Step 323 | learning_rate: 5.014749262536873e-06
Step 323 | epoch: 2.862222222222222
Step 324 | loss: 0.5320668816566467
Step 324 | grad_norm: 6.342469692230225
Step 324 | learning_rate: 4.71976401179941e-06
Step 324 | epoch: 2.871111111111111
Step 325 | loss: 0.5508782267570496
Step 325 | grad_norm: 7.187057018280029
Step 325 | learning_rate: 4.424778761061947e-06
Step 325 | epoch: 2.88
Step 326 | loss: 0.5160535573959351
Step 326 | grad_norm: 7.125871181488037
Step 326 | learning_rate: 4.129793510324484e-06
Step 326 | epoch: 2.888888888888889
Step 327 | loss: 0.7747188210487366
Step 327 | grad_norm: 7.802507400512695
Step 327 | learning_rate: 3.834808259587021e-06
Step 327 | epoch: 2.897777777777778
Step 328 | loss: 0.7538015842437744
Step 328 | grad_norm: 7.264074802398682
Step 328 | learning_rate: 3.5398230088495575e-06
Step 328 | epoch: 2.9066666666666667
Step 329 | loss: 0.7246187329292297
Step 329 | grad_norm: 6.716663360595703
Step 329 | learning_rate: 3.2448377581120944e-06
Step 329 | epoch: 2.9155555555555557
Step 330 | loss: 0.5694733262062073
Step 330 | grad_norm: 6.064235687255859
Step 330 | learning_rate: 2.9498525073746313e-06
Step 330 | epoch: 2.924444444444444
Step 331 | loss: 0.6399763226509094
Step 331 | grad_norm: 6.360866546630859
Step 331 | learning_rate: 2.6548672566371683e-06
Step 331 | epoch: 2.9333333333333336
Step 332 | loss: 0.6355582475662231
Step 332 | grad_norm: 7.316195964813232
Step 332 | learning_rate: 2.359882005899705e-06
Step 332 | epoch: 2.942222222222222
Step 333 | loss: 0.5208853483200073
Step 333 | grad_norm: 6.748278617858887
Step 333 | learning_rate: 2.064896755162242e-06
Step 333 | epoch: 2.951111111111111
Step 334 | loss: 0.5747878551483154
Step 334 | grad_norm: 6.353729248046875
Step 334 | learning_rate: 1.7699115044247788e-06
Step 334 | epoch: 2.96
Step 335 | loss: 0.5174808502197266
Step 335 | grad_norm: 7.3678975105285645
Step 335 | learning_rate: 1.4749262536873157e-06
Step 335 | epoch: 2.968888888888889
Step 336 | loss: 0.5437156558036804
Step 336 | grad_norm: 6.156686305999756
Step 336 | learning_rate: 1.1799410029498526e-06
Step 336 | epoch: 2.977777777777778
Step 337 | loss: 0.7410184144973755
Step 337 | grad_norm: 7.386032581329346
Step 337 | learning_rate: 8.849557522123894e-07
Step 337 | epoch: 2.986666666666667
Step 338 | loss: 0.48132628202438354
Step 338 | grad_norm: 6.7748284339904785
Step 338 | learning_rate: 5.899705014749263e-07
Step 338 | epoch: 2.9955555555555557
Step 339 | loss: 0.5984735488891602
Step 339 | grad_norm: 8.344001770019531
Step 339 | learning_rate: 2.9498525073746315e-07
Step 339 | epoch: 3.0
Step 339 | train_runtime: 2675.1884
Step 339 | train_samples_per_second: 1.009
Step 339 | train_steps_per_second: 0.127
Step 339 | total_flos: 4757790604861440.0
Step 339 | train_loss: 0.8593767165786046
Step 339 | epoch: 3.0
